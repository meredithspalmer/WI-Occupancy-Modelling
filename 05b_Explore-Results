##########################
##### Model Results ######
##########################

## Set workspace --------------------------------------------------------------
setwd("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights")
rm(list=ls()); gc()
set.seed(123)

# Load libraries
library(tidyterra)
library(tidyverse)
library(terra)
library(spOccupancy)
library(coda)
library(stringr)
library(ggplot2)
library(rasterVis)
library(rnaturalearth)
library(rnaturalearthdata)
library(sf)


## Load data ------------------------------------------------------------------

filenames <- list.files("Model outputs", full.names = TRUE) 
species <- "Acinonyx jubatus"

# Model outputs 
file_path <- filenames[grep(species, filenames)]
file_name_only <- basename(file_path)
filename <- paste0(file_path, "/", file_name_only, "_svcTPGOcc.RData")
load(filename)

# MOL range maps 
species_file <- gsub(" ", "_", species)
mmd_map_path <- paste("/gpfs/gibbs/pi/jetz/data/species_datasets/rangemaps/mammals/mdd_mammals/rasters_cea/", species_file, ".tif", sep="")
mmd_map <- rast(mmd_map_path)


## Results --------------------------------------------------------------------

## Basic summary
summary(out)

## Parameter plots: Occurrence 
occ.samps <- out$beta.samples 
occ_quantiles <- as.data.frame(matrix(NA, ncol(occ.samps), 6)) 
colnames(occ_quantiles) <- c("covariate", "mean", "q2.5", "q25", "q75", "q97.5")
occ_quantiles$covariate <- colnames(occ.samps)
occ_quantiles$covariate <- c("Intercept", "Crop", "GPW", "Precip", "WDPA", "EVI", "GFC", "Road Dist", "TRI", "River Dist", "CV", "City Dist", "Trend") #update as appropriate 
desired_order <- c("TRI", "Road Dist", "River Dist", "City Dist", "CV",
                   "Crop", "GPW", "Precip", "WDPA", "EVI", "GFC", "Trend", "Intercept")
occ_quantiles$covariate <- factor(occ_quantiles$covariate, levels = desired_order)
occ_quantiles$mean <- colMeans(occ.samps)
occ_quantiles[,3:6] <- t(apply(occ.samps, 2, quantile, probs = c(0.025, 0.25, 0.75, 0.975)))

(occParams <- ggplot(occ_quantiles, aes(x = covariate, y = mean)) +
    geom_errorbar(aes(ymin = q2.5, ymax = q97.5), width = 0, linewidth = 0.6) +
    geom_errorbar(aes(ymin = q25, ymax = q75, color = covariate), width = 0,
                  linewidth = 1.5, show.legend = FALSE) +
    geom_point() +
    theme_bw() +
    geom_hline(yintercept = 0, lty = 2, linewidth = 0.3) +
    xlab('Covariates (scaled)') +
    ylab('Posterior Distribution') +
    ggtitle('Occurrence coefficients', 'Multi-season, single-species model') + 
    coord_flip())

file_name <- paste(file_path, "/", target_species, "_occurrenceParams.jpg", sep="")
ggsave(filename = file_name, plot = occParams, width = 6, height = 5, units = "in")

## Parameter plots: Detection 
det.samps <- out$alpha.samples
det_quantiles <- as.data.frame(matrix(NA, ncol(det.samps), 6))
colnames(det_quantiles) <- c("covariate", "mean", "q2.5", "q25", "q75", "q97.5")
det_quantiles$covariate <- colnames(det.samps)
det_quantiles$covariate <- c("Intercept", "Effort", "Julian Day", "Julian Day Sq", "N. Depl") #update as appropriate
desired_order <- c("Effort", "N. Depl", "Julian Day", "Julian Day Sq", "Intercept")
det_quantiles$covariate <- factor(det_quantiles$covariate, levels = desired_order)
det_quantiles$mean <- colMeans(det.samps)
det_quantiles[,3:6] <- t(apply(det.samps ,2,quantile,probs=c(0.025, 0.25, 0.75, 0.975)))

(detParams <- ggplot(det_quantiles, aes(x = covariate, y = mean)) +
    geom_errorbar(aes(ymin = q2.5, ymax = q97.5), width = 0, linewidth = 0.6) +
    geom_errorbar(aes(ymin = q25, ymax = q75, color = covariate), width = 0,
                  linewidth = 1.5, show.legend = FALSE) +
    geom_point() +
    theme_bw() +
    geom_hline(yintercept = 0, lty = 2, linewidth = 0.3) +
    xlab('Covariates (scaled)') +
    ylab('Posterior Distribution') +
    ggtitle('Detection coefficients', 'Multi-season, single-species model') + 
    coord_flip())

file_name <- paste(file_path, "/", target_species, "_detectionParams.jpg", sep="")
ggsave(filename = file_name, plot = detParams, width = 6, height = 3, units = "in")


# -> Interpretations:
#      - Coefficients are estimated for the logit(ψ) = βX model and thus must be 
#        interpreted on the logit scale
#      - Positive beta coefficents imply that an increase in the value of that 
#        covariate results in an increase in the probability of occurrence, when
#        all other covariate values are held constant.
#      - Negative beta coefficents imply that an increase in the value of that 
#        covariate results in a decrease in the probability of occurrence, when 
#        all other covariate values are held constant.


## Occupancy probabilities across periods -------------------------------------

# This plot shows the average occupancy probability across all 373 sites within
# each period. The black points/lines are the medians and the shaded regions are 
# the 95% Bayesian CIs. 

# Parametners 
unique.periods <- as.numeric(gsub("p", "", dimnames(out$y)[[2]])) #unique periods
n.periods <- length(unique.periods) #number of periods 

# Average occupancy prob in each period across all sites
psi.mean.samples <- apply(out$psi.samples, c(1, 3), mean) 
psi.medians <- apply(psi.mean.samples, 2, median) #medians per period 
psi.quants <- apply(psi.mean.samples, 2, quantile, c(0.025, 0.975)) #95% Bayesian CIs

# Data frame for plotting in ggplot
plot.df <- data.frame(periods = unique.periods,
                      med = psi.medians,
                      low = c(psi.quants[1, ]),
                      high = c(psi.quants[2, ]))

# Plotting 
(occPeriods <- ggplot() +
    geom_ribbon(data = plot.df, aes(x = periods, ymin = low, ymax = high),
                fill = 'darkorchid4', alpha = 0.3) +
    geom_line(data = plot.df, aes(x = periods, y = med), lineend = 'butt') +
    geom_point(data = plot.df, aes(x = periods, y = med)) +
    theme_bw(base_size = 14) +
    theme(legend.position = c(0.75, 0.125)) +
    scale_y_continuous(limits = c(0, 1)) +
    labs(x = "Periods", y = 'Average Occupancy Probability'))

file_name <- paste(file_path, "/", target_species, "_occAcrossPeriods.jpg", sep="")
ggsave(filename = file_name, plot = occPeriods, width = 7, height = 4, units = "in")


## Predictions ----------------------------------------------------------------

# Load and format reference  
ref <- rast("10x10 km spatial layers/raster_100km2_with_cellID.tif")
world <- ne_countries(scale = "medium", returnclass = "sf")

# get bounding box for species detection sites
plot_coords <- as.data.frame(coords) 
crs <- crs(ref)
plot_coords_sf <- st_as_sf(plot_coords, coords = c("x", "y"), crs = crs)  
bbox_sf <- st_as_sfc(st_bbox(plot_coords_sf))

# get bounding box for expert range maps 
bbox_mmd <- ext(mmd_map)
bbox_sf_mmd <- st_as_sfc(st_bbox(mmd_map))

# Plot detection sites 
world <- ne_countries(scale = "medium", returnclass = "sf")
world <- st_transform(world, crs = crs)

ggplot() + 
  geom_sf(data = world, fill = "gray90", color = "gray60", linewidth = 0.3) +
  geom_spatraster(data = mmd_map) + 
  geom_sf(data = plot_coords_sf, color = "violet", size = 2) +
  scale_fill_viridis_c(na.value = NA) +
  theme_minimal() + theme(legend.position = "none") + 
  labs(title = "Species Detection Sites")

# Plot zoomed to bbox  
world_cropped_mmd <- st_crop(world, bbox_mmd)
(speciesDet <- ggplot() +
    geom_sf(data = world_cropped_mmd, fill = "gray90", color = "gray60") +
    geom_spatraster(data = mmd_map) + 
    geom_sf(data = plot_coords_sf, color = "violet", size = 2) +
    #geom_sf(data = bbox_sf_mmd, fill = NA, color = "red", size = 1) +
    scale_fill_viridis_c(na.value = NA) +
    theme_minimal() + theme(legend.position = "none") + 
    labs(title = "Species Detection Sites [Zoomed]"))

file_name <- paste(file_path, "/", target_species, "_speciesDetections.jpg", sep="")
ggsave(filename = file_name, plot = speciesDet, width = 6, height = 5, units = "in")


# Crop to get cell_IDs within MMD ---------------------------------------------
mmd_repro <- project(mmd_map, ref)
masked_ref <- mask(ref, mmd_repro)
plot(masked_ref)
mmd_cellIDs <- unique(masked_ref$cell_ID) #for some reason, one that is not in covs 


# Format static covs ----------------------------------------------------------

static_covs <- read.csv("Covariates for modelling/site_level_covariates_full.csv") %>% 
  dplyr::select(-c(cities_md, cities_sm)) %>% 
  filter(cell_ID %in% mmd_cellIDs$cell_ID)

road_dist.0 <- static_covs$road_dist
tri.0 <- static_covs$tri
river_dist.0 <- static_covs$river_dist 
coeff_var.0 <- static_covs$coeff_var 
cities_lg.0 <- static_covs$cities_lg 


# Format dynamic covariates ---------------------------------------------------
primary.periods <- seq(min(covs$period_counter), max(covs$period_counter), by = 1)

# Extraction function
cov_subset <- function(cov_dataframe_name, primary.periods, cell_IDs){
  covariate <- read.csv(cov_dataframe_name) %>% 
    dplyr::select(-any_of("X")) %>% 
    filter(cell_ID %in% cell_IDs) %>% 
    arrange(cell_ID)
  cols <- grep(
    paste0("^X(", paste(primary.periods, collapse = "|"), ")$"),
    names(covariate),
    value = TRUE
  )
  cols <- cols[order(as.numeric(sub("X", "", cols)))]
  covariate <- covariate %>% 
    select(all_of(cols)) %>% 
    as.matrix() 
  return(covariate)
}

# Processed values 
crop.0 <- cov_subset("Covariates for modelling/primary_occ_covariates_crop_full.csv", primary.periods, mmd_cellIDs$cell_ID) 
evi.0 <- cov_subset("Covariates for modelling/primary_occ_covariates_EVI_full.csv", primary.periods, mmd_cellIDs$cell_ID)
gpw.0 <- cov_subset("Covariates for modelling/primary_occ_covariates_GPW_full.csv", primary.periods, mmd_cellIDs$cell_ID)
precseas.0 <- cov_subset("Covariates for modelling/primary_occ_covariates_precseas_full.csv", primary.periods, mmd_cellIDs$cell_ID)
wdpa.0 <- cov_subset("Covariates for modelling/primary_occ_covariates_WDPA_full.csv", primary.periods, mmd_cellIDs$cell_ID) 
gfc.0 <- cov_subset("Covariates for modelling/primary_occ_covariates_GFC_full.csv", primary.periods, mmd_cellIDs$cell_ID)
trend.0 <- matrix(rep(primary.periods, each = nrow(static_covs)),
                  nrow = nrow(static_covs),
                  ncol = length(primary.periods),
                  byrow = FALSE)
colnames(trend.0) <- as.character(primary.periods)


## Create prediction design matrix (1 = intercept) ----------------------------

# Parameters
n.sites <- nrow(static_covs)            # number of sites 
n.periods <- length(primary.periods)    # number ofyears 
n.beta <- ncol(out$beta.samples)        # number of occupancy parameters 

coords.0 <- static_covs[,c("x","y")] %>% distinct() %>% as.matrix()

# Create and fill an array with the proper dimensions
X.0 <- array(NA, dim = c(n.sites, n.periods, n.beta))
X.0[, , 1] <-  1
X.0[, , 2] <-  (crop.0 - mean(dat_ls$occ.covs$crop))/sd(dat_ls$occ.covs$crop)
X.0[, , 3] <-  (gpw.0 - mean(dat_ls$occ.covs$gpw))/sd(dat_ls$occ.covs$gpw)
X.0[, , 4] <-  (precseas.0 - mean(dat_ls$occ.covs$precseas))/sd(dat_ls$occ.covs$precseas)
X.0[, , 5] <-  (wdpa.0 - mean(dat_ls$occ.covs$wdpa))/sd(dat_ls$occ.covs$wdpa)
X.0[, , 6] <-  (evi.0 - mean(dat_ls$occ.covs$evi))/sd(dat_ls$occ.covs$evi)
X.0[, , 7] <-  (gfc.0 - mean(dat_ls$occ.covs$gfc))/sd(dat_ls$occ.covs$gfc)
X.0[, , 8] <-  (road_dist.0 - mean(dat_ls$occ.covs$road_dist))/sd(dat_ls$occ.covs$road_dist)   
X.0[, , 9] <-  (tri.0 - mean(dat_ls$occ.covs$tri))/sd(dat_ls$occ.covs$tri)            
X.0[, , 10] <- (river_dist.0 - mean(dat_ls$occ.covs$river_dist))/sd(dat_ls$occ.covs$river_dist)          
X.0[, , 11] <- (coeff_var.0 - mean(dat_ls$occ.covs$coeff_var))/sd(dat_ls$occ.covs$coeff_var)         
X.0[, , 12] <- (cities_lg.0 - mean(dat_ls$occ.covs$cities_lg))/sd(dat_ls$occ.covs$cities_lg)      
X.0[, , 13] <- (trend.0 - mean(dat_ls$occ.covs$trend))/sd(dat_ls$occ.covs$trend)

# Give names to X.0 just to make things clear
dimnames(X.0)[[3]] <- c('(Intercept)', 'Crop', 'GPW', 'Precip', 'WDPA', 'EVI', 'GFC', 'Road Dist', 'TRI', 'River Dist', 'Coef Var', 'Cities Dist', 'Periods') 

## Predict occupancy for all years --------------------------------------------

## Regular predict ----
start_time <- Sys.time()
out.pred <- predict(out, X.0, coords.0, t.cols = 1:n.periods, ignore.RE = TRUE, type = 'occupancy')
end_time <- Sys.time()
end_time - start_time 
str(out.pred)

# Calculate mean occupancy probability for each site/year: 
psi.means <- apply(out.pred$psi.0.samples, c(2, 3), mean)

## For extremely large datasets, chunk the predictions ---- 
chunked_predict_from_betas <- function(model, X.0, coords.0, t.cols,
                                       chunk_size = 2000,
                                       n_post_draws = NULL,     
                                       n_q_draws = NULL,        
                                       probs = c(0.025, 0.5, 0.975),
                                       keep_quantiles = TRUE,
                                       seed = 123,
                                       verbose = TRUE,
                                       save_chunks_dir = NULL) {
  
  set.seed(seed)
  
  n_sites   <- nrow(coords.0)
  n_periods <- length(t.cols)
  n_beta    <- dim(X.0)[3]
  n_chunks  <- ceiling(n_sites / chunk_size)
  
  # Get beta samples and orient as matrix: rows = draws, cols = parameters
  if (is.null(model$beta.samples)) stop("model$beta.samples not found in model object.")
  bsm <- model$beta.samples
  
  # Try to get matrix rows = draws, cols = n_beta
  if (is.matrix(bsm)) {
    if (ncol(bsm) == n_beta) {
      beta_mat <- bsm
    } else if (nrow(bsm) == n_beta) {
      beta_mat <- t(bsm)
      warning("Transposing model$beta.samples to get rows = draws.")
    } else {
      # best guess: rows are draws if rows > cols
      if (nrow(bsm) >= ncol(bsm)) {
        beta_mat <- bsm
        warning("Couldn't perfectly match beta.samples dims to X.0. Assuming rows = draws.")
      } else {
        beta_mat <- t(bsm)
        warning("Couldn't perfectly match beta.samples dims to X.0. Assuming columns were draws; transposing.")
      }
    }
  } else if (is.array(bsm) && length(dim(bsm)) == 2) {
    beta_mat <- as.matrix(bsm)
  } else {
    stop("Unsupported format for model$beta.samples.")
  }
  
  n_total_draws <- nrow(beta_mat)
  if (is.null(n_post_draws)) n_post_draws <- min(1000, n_total_draws)
  n_post_draws <- min(n_post_draws, n_total_draws)
  if (is.null(n_q_draws)) n_q_draws <- min(n_post_draws, 200)
  n_q_draws <- min(n_q_draws, n_post_draws)
  
  # Pick draw indices (random subsample of posterior draws)
  draw_idx_all <- sort(sample.int(n_total_draws, size = n_post_draws, replace = FALSE))
  # Which positions in that stream will be used for quantiles
  q_draw_positions <- sort(sample.int(n_post_draws, size = n_q_draws, replace = FALSE))
  
  # Prepare storage
  mean_mat <- matrix(NA_real_, nrow = n_sites, ncol = n_periods)
  sd_mat   <- matrix(NA_real_, nrow = n_sites, ncol = n_periods)
  quantiles_array <- if (keep_quantiles) array(NA_real_, dim = c(n_sites, n_periods, length(probs))) else NULL
  
  # Per-chunk loop
  for (i in seq_len(n_chunks)) {
    idx_start <- (i - 1) * chunk_size + 1
    idx_end   <- min(i * chunk_size, n_sites)
    idx       <- idx_start:idx_end
    nchunk    <- length(idx)
    
    if (verbose) message(sprintf("Chunk %d/%d: sites %d:%d (%d sites)", i, n_chunks, idx_start, idx_end, nchunk))
    
    # Subset X.0 and build design matrix: rows = site*period, cols = n_beta
    Xsub <- X.0[idx, , , drop = FALSE]   # dim: [nchunk, n_periods, n_beta]
    Ncells <- nchunk * n_periods
    
    # Estimate memory for storing quantile draws for this chunk if requested
    if (keep_quantiles) {
      est_bytes <- as.numeric(Ncells) * as.numeric(n_q_draws) * 8
      # if too big (> ~1 GB), reduce n_q_draws for this chunk
      max_bytes <- 1e9
      if (est_bytes > max_bytes) {
        new_n_q <- max(1, floor(max_bytes / (Ncells * 8)))
        if (new_n_q < n_q_draws) {
          warning(sprintf("Chunk %d would need ~%.1f MB to store quantile draws; reducing n_q_draws from %d -> %d for this chunk.",
                          i, est_bytes/1e6, n_q_draws, new_n_q))
          q_draw_positions <- sort(sample.int(n_post_draws, size = new_n_q, replace = FALSE))
        }
      }
    }
    
    # Create X matrix (rows: site-period pairs, column: covariates)
    # Ordering: as.vector(Xsub[,,k]) -> site varies fastest within each period, then next period
    Xmat <- matrix(NA_real_, nrow = Ncells, ncol = n_beta)
    for (k in seq_len(n_beta)) {
      Xmat[, k] <- as.vector(Xsub[,, k])
    }
    
    # Streaming accumulators (Welford)
    mean_vec <- numeric(Ncells)
    M2_vec   <- numeric(Ncells)
    
    # Storage for quantile draws for this chunk (only if requested)
    if (keep_quantiles && length(q_draw_positions) > 0) {
      psi_q_mat <- matrix(NA_real_, nrow = Ncells, ncol = length(q_draw_positions))
      q_counter <- 1L
    } else {
      psi_q_mat <- NULL
      q_counter <- NULL
    }
    
    # Sstream through selected draws
    iter <- 0L
    for (jj in seq_along(draw_idx_all)) {
      sidx <- draw_idx_all[jj]
      beta_s <- beta_mat[sidx, ]
      eta <- as.numeric(Xmat %*% beta_s)      # length Ncells
      psi  <- 1 / (1 + exp(-eta))             # plogis
      
      iter <- iter + 1L
      if (iter == 1L) {
        mean_vec <- psi
        M2_vec   <- numeric(Ncells)
      } else {
        delta <- psi - mean_vec
        mean_vec <- mean_vec + delta / iter
        M2_vec   <- M2_vec + delta * (psi - mean_vec)
      }
      
      # If this draw position was chosen for quantiles, store
      if (!is.null(psi_q_mat) && (jj %in% q_draw_positions)) {
        psi_q_mat[, q_counter] <- psi
        q_counter <- q_counter + 1L
      }
      
      # Occasional garbage collection
      if ((jj %% 200) == 0L) gc(verbose = FALSE)
    } # draws loop
    
    # Finalize chunk summaries
    mean_chunk <- matrix(mean_vec, nrow = nchunk, ncol = n_periods)
    mean_mat[idx, ] <- mean_chunk
    
    if (iter > 1L) {
      sd_vec <- sqrt(M2_vec / (iter - 1L))
      sd_chunk <- matrix(sd_vec, nrow = nchunk, ncol = n_periods)
      sd_mat[idx, ] <- sd_chunk
    } else {
      sd_mat[idx, ] <- NA_real_
    }
    
    # Quantiles (if requested)
    if (!is.null(psi_q_mat)) {
      # apply quantile over rows -> returns matrix (length(probs) x Ncells); transpose to Ncells x length(probs)
      q_mat <- t(apply(psi_q_mat, 1, quantile, probs = probs, na.rm = TRUE))
      # q_mat dims: Ncells x length(probs); reshape into nchunk x n_periods x length(probs)
      q_chunk_array <- array(q_mat, dim = c(nchunk, n_periods, length(probs)))
      quantiles_array[idx, , ] <- q_chunk_array
      rm(psi_q_mat, q_mat, q_chunk_array)
    }
    
    # Optionally save chunk to disk to free memory
    if (!is.null(save_chunks_dir)) {
      dir.create(save_chunks_dir, showWarnings = FALSE, recursive = TRUE)
      saveRDS(list(mean = mean_chunk,
                   sd = matrix(sd_vec, nrow = nchunk, ncol = n_periods),
                   quantiles = if (!is.null(quantiles_array)) quantiles_array[idx, , , drop = FALSE] else NULL),
              file = file.path(save_chunks_dir, sprintf("pred_chunk_%03d.rds", i)))
      # free chunk-level items
      if (verbose) message("Saved chunk ", i, " to disk.")
      if (!is.null(quantiles_array)) quantiles_array[idx, , ] <- NA_real_
      mean_mat[idx, ] <- NA_real_
      sd_mat[idx, ] <- NA_real_
    }
    
    # Free memory and continue
    rm(Xsub, Xmat, mean_vec, M2_vec, mean_chunk); gc(verbose = FALSE)
  } # chunk loop
  
  # Return
  out <- list(mean = mean_mat,
              sd = sd_mat,
              quantiles = quantiles_array,
              probs = probs,
              n_post_draws = n_post_draws,
              n_q_draws = length(q_draw_positions))
  return(out)
}

res <- chunked_predict_from_betas(out, X.0, coords.0, t.cols = 1:n.periods,
                                  chunk_size = 2000,
                                  n_post_draws = 1000,   
                                  n_q_draws = 200,       
                                  keep_quantiles = TRUE,
                                  verbose = TRUE)

# Posterior mean occupancy (sites x periods)
occ_mean <- res$mean

# Posterior sd
occ_sd <- res$sd

# Posterior quantiles (if computed)
occ_lower <- res$quantiles[, , 1]   
occ_median <- res$quantiles[, , 2]
occ_upper <- res$quantiles[, , 3]


## Plotting predicted occupancy -----------------------------------------------

# Formatting data -------------------------------------------------------------

# Convert to long dataframe
df_occ <- occ_mean %>%
  as.data.frame() %>%
  mutate(site = 1:nrow(.),
         x = coords.0[,1],
         y = coords.0[,2]) %>%
  pivot_longer(cols = starts_with("V"), 
               names_to = "period",
               values_to = "mean_occ") %>%
  mutate(period = primary.periods[as.integer(str_remove(period, "V"))])

# Add CIs
df_lower <- as.data.frame(occ_lower) %>%
  mutate(site = 1:nrow(.)) %>%
  pivot_longer(cols = starts_with("V"),
               names_to = "period",
               values_to = "lower") %>%
  mutate(period = primary.periods[as.integer(str_remove(period, "V"))])

df_upper <- as.data.frame(occ_upper) %>%
  mutate(site = 1:nrow(.)) %>%
  pivot_longer(cols = starts_with("V"),
               names_to = "period",
               values_to = "upper") %>%
  mutate(period = primary.periods[as.integer(str_remove(period, "V"))])

df_occ <- df_occ %>%
  left_join(df_lower, by = c("site", "period")) %>%
  left_join(df_upper, by = c("site", "period"))


# Time series with uncertainty ------------------------------------------------

df_occ %>%
  group_by(period) %>%
  summarise(mean = mean(mean_occ, na.rm=TRUE),
            lower = mean(lower, na.rm=TRUE),
            upper = mean(upper, na.rm=TRUE)) %>%
  ggplot(aes(x = period, y = mean)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "grey80") +
  geom_line(size = 1.2, color = "darkblue") +
  labs(y = "Occupancy probability", x = "Period",
       title = "Average occupancy over time") +
  theme_minimal()


# Spatial map for one period --------------------------------------------------

period <- "60"

df_occ_sf <- st_as_sf(df_occ, coords = c("x", "y"), crs = crs(ref), remove = FALSE)
world <- ne_countries(scale = "medium", returnclass = "sf")
world_proj <- st_transform(world, st_crs(df_occ_sf))
world_crop <- st_crop(world_proj, st_bbox(df_occ_sf))
plot_df <- df_occ %>% filter(period == period_sel & !is.na(mean_occ))

(occPeriod <- ggplot() +
  geom_sf(data = world_crop, fill = "gray95", color = "grey30", size = 0.3) +
  geom_tile(data = plot_df, aes(x = x, y = y, fill = mean_occ), alpha = 0.9) +
  scale_fill_viridis_c(option = "C", na.value = NA) +
  coord_sf(xlim = c(min(plot_df$x), max(plot_df$x)),
           ylim = c(min(plot_df$y), max(plot_df$y)),
           expand = FALSE) +
  labs(x = NULL, y = NULL,
       fill = "Occupancy", 
       title = bquote(italic(.(target_species))),
    subtitle = paste0("Predicted occupancy (Period ", period_sel, ")")
  ) +
  theme_minimal())

file_name <- paste(file_path, "/", target_species, "_occPeriod.jpg", sep="")
ggsave(filename = file_name, plot = occPeriod, width = 6, height = 5, units = "in")


# Difference map (change between two periods) ---------------------------------

df_diff <- df_occ %>%
  filter(period %in% c("44","99")) %>%
  select(site, x, y, period, mean_occ) %>%
  pivot_wider(names_from = period, values_from = mean_occ) %>%
  mutate(diff = `99` - `44`)

ggplot(df_diff, aes(x = x, y = y, fill = diff)) +
  geom_tile() +
  scale_fill_gradient2(low = "red", mid = "white", high = "blue") +
  coord_equal() +
  labs(fill = "Δ occupancy (2020-2000)",
       title = "Change in occupancy between 2000 and 2020") +
  theme_minimal()

## Heatmap (site x period)
df_occ %>%
  ggplot(aes(x = period, y = site, fill = mean_occ)) +
  geom_tile() +
  scale_fill_viridis_c(option = "C") +
  labs(x = "Year/Period", y = "Site",
       fill = "Occupancy",
       title = "Occupancy across sites and periods") +
  theme_minimal() +
  theme(axis.text.y = element_blank())

## Animate map over time 
library("gganimate")

p <- df_occ %>%
  ggplot(aes(x = x, y = y, fill = mean_occ)) +
  geom_tile() +
  scale_fill_viridis_c(option = "C", limits = c(0,1)) +
  coord_equal() +
  labs(title = "Predicted occupancy: {closest_state}",
       fill = "Occupancy") +
  theme_minimal() +
  transition_states(period, state_length = 1, transition_length = 1)

animate(p, nframes = length(primary.periods)*5, fps = 5)


# ------------------------
# 3. Collapse to average occupancy per year
# ------------------------

# out.pred$psi.samples has dimension: [iterations × sites × years]
psi.samples <- out.pred$psi.samples

# Average across sites for each iteration and year
psi.mean.by.year <- apply(psi.samples, c(1, 3), mean)  # [iterations × years]

# Summarize posterior mean & CI per year
occ.summary <- data.frame(
  year = years,
  mean = apply(psi.mean.by.year, 2, mean),
  lower = apply(psi.mean.by.year, 2, quantile, 0.025),
  upper = apply(psi.mean.by.year, 2, quantile, 0.975)
)

# ------------------------
# 4. Plot occupancy vs year
# ------------------------

ggplot(occ.summary, aes(x = year, y = mean)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), fill = "gray80", alpha = 0.6) +
  geom_line(color = "blue", size = 1) +
  labs(x = "Year", y = "Occupancy probability (ψ)", 
       title = "Predicted occupancy across sites") +
  theme_minimal()






# Number of prediction sites
J.pred <- nrow(static_covs)

# Number of prediction years 
n.years.pred <- 25 

# Number of occupancy predictors (including intercept) 
p.occ <- ncol(out$beta.samples)

# Get covariates and standardize them using values used to fit the model
tri.pred <- (static_covs$tri - mean(dat_ls$occ.covs$tri)) / sd(dat_ls$occ.covs$tri) #shouldn't need?
year_1999_scaled <- (1999 - mean(dat_ls$occ.covs$years)) / sd(dat_ls$occ.covs$years)
year.pred <- matrix(year_1999_scaled, nrow = J.pred, ncol = 1)

# Create three-dimensional array
X.0 <- array(1, dim = c(J.pred, n.years.pred, p.occ))

# Fill in the array
X.0[, , 2] <- year.pred #years
X.0[, , 3] <- tri.pred #elevation
str(X.0)

# Indicate which primary time periods (years) we are predicting for
t.cols <- c(1, 9)

# Predict 
out.pred <- predict(out, X.0, t.cols = t.cols, ignore.RE = TRUE, type = 'occupancy')
str(out.pred)


#we plot the mean of REVI occurrence probability in 2009 and 2018 across the forest.
plot.dat <- data.frame(x = hbefElev$Easting, y = hbefElev$Northing, 
                       mean.2009.psi = apply(out.pred$psi.0.samples[, , 1], 2, mean), 
                       mean.2018.psi = apply(out.pred$psi.0.samples[, , 2], 2, mean), 
                       sd.2009.psi = apply(out.pred$psi.0.samples[, , 1], 2, sd), 
                       sd.2018.psi = apply(out.pred$psi.0.samples[, , 2], 2, sd), 
                       stringsAsFactors = FALSE)

# Make a species distribution map showing the point estimates, or predictions (posterior means)
dat.stars <- st_as_stars(plot.dat, dims = c('x', 'y'))

# 2009
ggplot() + geom_stars(data = dat.stars, aes(x = x, y = y, fill = mean.2009.psi)) +
  scale_fill_viridis_c(na.value = 'transparent') +
  labs(x = 'Easting', y = 'Northing', fill = '', 
       title = 'Mean REVI occurrence probability 2009') +
  theme_bw()  




# We now use our fitted model to predict occurrence across all cells in our study area. We use the predict function within the spOcc package, which intakes a design matrix of the covariate values to predict over. Note that the design matrix must match the order of covariates used to fit the model. The covariate values that we predict over must be standardized (scaled) in the same way that the variables were standardized to fit the model. This means they must be scaled against all covariate values.

TO DO: Here somehow have to get bb on area surveyed, extract covariates for this area at 10x10 km2 scale. That means need to ensure have global covarage on covs, not just cropped coverage

# Scale covariates 
fc.0 <- scale(occ_covs_all$Forest_PercentCover)
Cliffs.0 <- scale(occ_covs_all$CliffsCanyons_PercentCover)
Precip.0 <- scale(occ_covs_all$MeanAnnualPrecipitation_mm)

# Create prediction design matrix (1 = intercept)
X.0 <- cbind(1, fc.0, Cliffs.0, Precip.0)

out.pred <- predict(out, X.0)
psi.0.samples <- out.pred$psi.0.samples

# MSP: assuming it is here that I can combine across years and plot?? 


# Assuming you have a fitted svcTPGOcc model object named 'svc_model'
# and a data frame 'new_data' with covariates for prediction locations and years.

# Predict occupancy probability at new locations and/or future years
predicted_occupancy <- predict(svc_model, X.0 = new_data)

# 'predicted_occupancy' will contain posterior predictive samples for occupancy
# at the specified locations and years, allowing you to assess uncertainty.


# ## Predict -------

# load and format reference ---------------------------------------------------
ref <- rast("10x10 km spatial layers/raster_100km2_with_cellID.tif")
world <- ne_countries(scale = "medium", returnclass = "sf")

# get bounding box for species detection sites
plot_coords <- as.data.frame(coords) 
crs <- crs(ref)
plot_coords_sf <- st_as_sf(plot_coords, coords = c("x", "y"), crs = crs)  
bbox_sf <- st_as_sfc(st_bbox(plot_coords_sf))

# Plot detection sites --------------------------------------------------------
world <- ne_countries(scale = "medium", returnclass = "sf")
world <- st_transform(world, crs = crs)

ggplot() +
  geom_sf(data = world, fill = "gray90", color = "gray60") +
  geom_sf(data = plot_coords_sf, color = "violet", size = 2) +
  theme_minimal() +
  labs(title = "Species Detection Sites")

# Plot zoomed to bbox ---------------------------------------------------------
world_cropped <- st_crop(world, bbox_sf) 

ggplot() +
  geom_sf(data = world_cropped, fill = "gray90", color = "gray60") +
  geom_sf(data = plot_coords_sf, color = "violet", size = 2) +
  geom_sf(data = bbox_sf, fill = NA, color = "red", size = 1) +
  theme_minimal() +
  labs(title = "Species Detection Sites [Zoomed]")

# Crop raster to bbox to get cell_IDs within bounding box ---------------------
r_cropped <- crop(ref, bbox_sf)
r_df <- as.data.frame(r_cropped, xy = TRUE)

# Format static covs ----------------------------------------------------------
static_covs <- read.csv("Covariates for modelling/site_level_covariates_full.csv") %>% 
  filter(cell_ID %in% r_df$cell_ID)






# We create our final predictive maps from the full posterior predictive distributions of ψ across the 4500 cells.

# Occurrence probability means:
psi.0.mean <- apply(psi.0.samples, 2, mean)
# -> each column of psi.0.samples is an MCMC sample of the psi for one site
# -> these are the means ordered in that vector based on their cell

length(psi.0.mean) #this is mean values across all the cells, so we need a map to interpret it

# Find 95% credible intervals and widths of 95% credible interval for occurrence probability estimates:
quantiles.ordered.samples <- as.data.frame(
  t(apply(psi.0.samples ,2,quantile,probs=c(0.025,0.975))))
quantiles.ordered.samples$width <- quantiles.ordered.samples[,2] -
  quantiles.ordered.samples[,1]

# Predictive plots require the shapefile for the 10x10 grids that can be linked to the unique cell IDs. 

# Create a dataframe for plotting:Plot names in the covariates and in the prediction outputs line up because that is the order it was read into the prediction formula

predicted.df <- data.frame(Cell= occ_covs_all$Cell,
                           psi.mean = psi.0.mean,
                           psi.width = quantiles.ordered.samples$width)

# Join the predicted results to the spatial grid, using the plot name
plottable <- left_join(select(grid_shp,Cell, geometry),
                       predicted.df,
                       by = 'Cell')
# There are some NA grid cells that didn't have covariate values and don't have predictions; remove them: 
plottable <- na.omit(plottable)

ggplot(data = plottable) +
  geom_sf(aes(fill = psi.mean), lwd = 0.05) +
  theme_bw() +
  viridis::scale_fill_viridis(limits = c(0, 1)) +
  ggtitle('LANO posterior mean predicted occurrence',
          'Single-season, single-species model')

ggplot(plottable) +
  geom_sf(aes(fill = psi.width), lwd = 0.05) +
  theme_bw() +
  viridis::scale_fill_viridis(option = 'plasma', limits = c(0, 1)) +
  ggtitle('LANO predicted occurrence 95% interval width',
          'Single-season, single-species model')



# Below we predict across the 400 “new” locations and plot them in comparison to the true values we used to simulate the data.

# Predict occupancy at the 400 new sites
out.pred <- predict(out.svc.trend, X.pred, coords.pred, t.cols = 1:n.time.max)

# get SVC values at the prediction locations
svc.pred.samples <- getSVCSamples(out.svc.trend, pred.object = out.pred)

# Get mean values of the SVC for the covariate
svc.cov.pred.mean <- apply(svc.pred.samples$occ.cov.1, 2, mean)

# Plot
plot(coords.pred, type = "n", xlab = "", ylab = "", asp = TRUE, 
     main = "Estimated values", bty = 'n')
points(coords.pred, pch=15, cex = 2.1, 
       col = rgb(0,0,0,(svc.cov.pred.mean-min(svc.cov.pred.mean))/diff(range(svc.cov.pred.mean))))

## adapt 
# First standardize elevation using mean and sd from fitted model
elev.pred <- (hbefElev$val - mean(btbwHBEF$occ.covs[, 1])) / sd(btbwHBEF$occ.covs[, 1])
coords.0 <- as.matrix(hbefElev[, c('Easting', 'Northing')])
X.0 <- cbind(1, elev.pred, elev.pred^2)
out.pred <- predict(out, X.0, coords.0, verbose = FALSE)

# Plot spatially varying trend across study region  
**FIX THIS**
  
  y <- yarray            #detection-nondetection data
X <- dat$X             #occurrence design matrix for fixed effects
X.p <- dat$X.p         #detection design matrix for fixed effets  
psi <- dat$psi         #occurrence values
coords <- coords       #spatial coordinates
w <- dat$w             #spatially varying intercept and covariate effects
cov.effect <- beta[2] + w[, 2]
plot.dat <- data.frame(x = coords[, 1], 
                       y = coords[, 2], 
                       cov.effect = cov.effect)

ggplot(plot.dat, aes(x = x, y = y, fill = cov.effect)) + 
  geom_raster() + 
  scale_fill_gradient2(midpoint = 0, low = '#B2182B', mid = 'white', 
                       high = '#2166AC', na.value = NA) + 
  theme_bw() 
# (decrease in occurrence = red, increase in occurrence = blue)





## Occurrence data across study area ------------------------------------------

# We may be interested in average occurrence across the whole study area, which can be computed as a derived quantity of the cell-level occurrences. We include naive average occurrence across surveyed cells as a solid line.

psi <- as.data.frame(out$psi.samples) # samples of latent psi across all sites
#this is psi sampled across all 169 sites
avg_psi <- rowMeans(psi) #take averages across all columns (sites)
avg_psi_naive <- mean(as.numeric(as.character(naive$Naive_occupancy))) FIGURE THIS OUT 
hist(avg_psi, breaks = 20, freq=F, main = "", xlab = "Mean occurrence across all cells")
abline(v = avg_psi_naive, lwd = 2) FIGURE THIS OUT 




## add in the following 

## Estimates of SVCs 

# To extract the estimates of the spatially varying coefficients at each of the spatial locations in the data set used to fit the model, we need to combine the non-spatial component of the coefficient (contained in out.svc$beta.samples) and the spatial component of the coefficient (contained in out.svc$w.samples). Recall that in an SVC occupancy model, the total effect of a covariate at any given location is the sum of the non-spatial effect and the adjustment of the effect at that specific location. We provide the function getSVCSamples() to extract the SVCs at each location.

# The resulting object, here called svc.samples, is a list with each component corresponding to a matrix of the MCMC samples of each spatially varying coefficient estimated in the model, with rows corresponding to MCMC sample and column corresponding to site.

svc.samples <- getSVCSamples(out) 
str(svc.samples)

# Intercept 
svc.samples <- getSVCSamples(out.svc.trend)
int.quants <- apply(svc.samples[["(Intercept)"]], 2, quantile, 
                    probs = c(0.025, 0.5, 0.975))
svc.true.fit <- beta + w.fit
plot(svc.true.fit[, 1], int.quants[2, ], pch = 19, 
     ylim = c(min(int.quants[1, ]), max(int.quants[3, ])),
     xlab = "True", ylab = "Fit", main = 'Intercept')
abline(0, 1)
arrows(svc.true.fit[, 1], int.quants[2, ], svc.true.fit[, 1], col = 'gray', 
       int.quants[1, ], length = 0.02, angle = 90)
arrows(svc.true.fit[, 1], int.quants[1, ], svc.true.fit[, 1], col = 'gray', 
       int.quants[3, ], length = 0.02, angle = 90)
points(svc.true.fit[, 1], int.quants[2, ], pch = 19)

# Trend -------------------------------------------------------------------
trend.quants <- apply(svc.samples[["trend"]], 2, quantile, 
                      probs = c(0.025, 0.5, 0.975))
plot(svc.true.fit[, 2], trend.quants[2, ], pch = 19, 
     ylim = c(min(trend.quants[1, ]), max(trend.quants[3, ])),
     xlab = "True", ylab = "Fit", main = 'Spatially-Varying Trends')
abline(0, 1)
arrows(svc.true.fit[, 2], trend.quants[2, ], svc.true.fit[, 2], col = 'gray', 
       trend.quants[1, ], length = 0.02, angle = 90)
arrows(svc.true.fit[, 2], trend.quants[1, ], svc.true.fit[, 2], col = 'gray', 
       trend.quants[3, ], length = 0.02, angle = 90)
points(svc.true.fit[, 2], trend.quants[2, ], pch = 19)


#https://code.usgs.gov/usgs/norock/irvine_k/vignette-bayesian-site-occupancy-model-bat-acoustic-data/-/blob/main/LANO-OR-WA-vignette.pdf?ref_type=heads
#Plotting covariates
ggplot(data = plottable_covs) +
  geom_sf(aes(fill = Forest_PercentCover), lwd = 0.05 ) +
  theme_bw() +
  viridis::scale_fill_viridis(limits = c(0, 1))




# Load full reference grid ---------------------------------------------------------------
grid_full <- rast("/gpfs/gibbs/pi/jetz/wildlife_insights/RWT/data/data_for_annotation/Global_1km_CEA_reference_raster_with_cellID.tif")

# Load range map ---------------------------------------------------------------
file_rangemap <- list.files("/gpfs/gibbs/pi/jetz/data/species_datasets/rangemaps/mammals/mdd_mammals/rasters_cea/", full.names = TRUE) %>% 
  str_subset(pattern = species_full)
#rangemap <- rast("/gpfs/gibbs/pi/jetz/data/species_datasets/rangemaps/mammals/mdd_mammals/rasters_cea/Suncus_etruscus.tif")
rangemap <- rast(file_rangemap)
rangemap_bf <- buffer(rangemap, width = 100000) #500 km
rangemap_bf <- as.numeric(rangemap_bf) # from true/false to 1/0
rangemap_bf <- ifel(rangemap_bf == 0, NA, 1)



# Subset grid based on range map -----------------------------------------------
grid_sub <- terra::crop(grid_full, rangemap_bf) # to make extent match
grid_sub <- terra::mask(grid_sub, rangemap_bf) # to mask 

# plot(grid_sub, y = 2)
# plot(rangemap_bf)
# plot(rangemap, add = TRUE, col = "red")
# plot(rangemap_bf, add = TRUE, col = "blue")
# plot(world,  col=NULL, "l", add = TRUE)
# points(coords, col = "blue")


# Remove no-land ---------------------------------------------------------------
land <- rast("/gpfs/gibbs/pi/jetz/data/regions_datasets/MOL_land_layer/v2/MOL_Land_Layer_Countries_Fixed_1km.tiff") %>% 
  terra::project(crs(grid_sub))

land_sub <- terra::crop(land, grid_sub) # to make extent match
land_sub <- ifel(land_sub == 0, NA, 1)
land_sub_rs <- resample(land_sub, grid_sub) # resample to align
grid_land <- terra::mask(grid_sub, land_sub_rs) # to remove no-land 

# Extract covariates -----------------------------------------------------------
