##########################
##### Running models ##### 
##########################

# Set working directory
setwd("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights")
rm(list=ls()); gc()
set.seed(123)

# Load libraries 
library(dplyr)
library(forcats)
library(lubridate)
library(purrr)
library(reshape2)
library(sf)
library(spOccupancy)
library(stringr)
library(terra)
library(tibble)
library(tidyr)
library(tidyverse) 
library(coda)
library(stars)
library(ggplot2)
library(gtools)

## User-defined functions -----------------------------------------------------

# Scaling functions
scale2 <- function(x, na.rm = FALSE) ((x - mean(1:7))/sd(1:7))
scale3 <- function(x, na.rm = FALSE) ((x - mean(1:365))/sd(1:365))

# Covariate formatting function 
cov_subset <- function(cov_dataframe_name, period_data){
  covariate <- read.csv(cov_dataframe_name) %>% 
    dplyr::select(-any_of("X")) %>% 
    filter(cell_ID %in% cell_IDs) %>% 
    arrange(cell_ID)
  keep_cols <- intersect(paste0("X", unique(period_data$period_counter)), names(covariate))
  df_subset <- covariate[, keep_cols] %>% 
    as.matrix()
  return(df_subset)
}


## Load prepped data ----------------------------------------------------------

(filenames <- list.files("Data for modelling", full.names = TRUE))

#CHANGE
filename <- filenames[1]

load(filename)
print(target_species)
print(nrow(ystack))
print(nrow(covs))

# Create new directory to store outputs 
file_species <- gsub(" ", "_", target_species)
new_dir_path <- paste("Model outputs/", file_species, sep="")
if (!dir.exists(new_dir_path)) {
  dir.create(new_dir_path)
} else {
  print("Directory already exists")
}

## Organize data and covariates -----------------------------------------------

# Remove cases without cell_ID
ystack <- cbind(ystack, covs %>% dplyr::select(cell_ID)) %>% drop_na(cell_ID)
covs <- covs %>% drop_na(cell_ID)

# Set dimensions
(ncell <- length(unique(covs$cell_ID)))
(nperiod <- length(seq(min(covs$period_counter), max(covs$period_counter), by = 1)))
nweek <- 13
(min_year <- min(covs$year))
(max_year <- max(covs$year))
cell_IDs <- unique(ystack$cell_ID)

# Create periods table 
# NOTE: if outliers are identified, remove here as well 
periods <- data.frame(period_counter = seq(min(covs$period_counter), 
                                           max(covs$period_counter), by = 1)) %>% 
  left_join(covs %>% dplyr::select(period_counter, season, year) %>% distinct()) %>% 
  mutate(year = as.numeric(year))

# Fill in missing seasons and years
season_order <- c("Spring", "Summer", "Fall", "Winter")    # order of seasons 

# If first row is NA, initialize: 
head(periods)
if(is.na(periods$season[1])) periods$season[1] <- "Fall"   # chose start season
if(is.na(periods$year[1])) periods$year[1] <- 2000         # chose start year 

# Fill remaining rows
for(i in 2:nrow(periods)) {
  prev_season <- periods$season[i-1]
  prev_year   <- periods$year[i-1]
  
  if(is.na(periods$season[i])) {
    # Compute next season
    next_season <- season_order[(match(prev_season, season_order) %% 4) + 1]
    periods$season[i] <- next_season
    
    # Increment year if season wraps from Winter -> Spring
    if(prev_season == "Winter") {
      periods$year[i] <- prev_year + 1
    } else {
      periods$year[i] <- prev_year
    }
  }
}
head(periods)

## Observations ---------------------------------------------------------------
ystack2 <- cbind(ystack, covs %>% dplyr::select(cell_ID, period_counter)) %>% 
  pivot_longer(V1:V13, names_to = "week", values_to = "det", values_drop_na = TRUE) %>% 
  mutate(week = as.numeric(str_remove(week, pattern = "V")),
         period_counter = paste0("p", period_counter)) 
ystack2 <- ystack2[order(ystack2$cell_ID),]

yarray <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(yarray) <- unique(covs$cell_ID)
colnames(yarray) <- paste0("p", seq(min(covs$period_counter), 
                                    max(covs$period_counter), by = 1))

for(i in 1:nrow(ystack2)){
  yarray[as.character(ystack2$cell_ID[i]),
         as.character(ystack2$period_counter[i]),
         ystack2$week[i]] <- ystack2$det[i]
}


## Coordinates ----------------------------------------------------------------
coords <- as.data.frame(rast("10x10 km spatial layers/raster_100km2_with_cellID.tif"), xy=T) %>% 
  filter(cell_ID %in% cell_IDs) %>% 
  arrange(cell_ID) %>% 
  dplyr::select(-cell_ID) %>% 
  as.matrix()


## Covariates: spatial (dim: ncell) ------------------------------------------- 

static <- read.csv("Covariates for modelling/site_level_covariates_site.csv") %>% 
  dplyr::select(-c(elevation, cities_md, cities_sm, x, y)) %>% #from Data-Explore code 
  filter(cell_ID %in% cell_IDs) %>% 
  arrange(cell_ID)
nrow(static) #select out each column want as a vector 


## Covariates: primary occasion (dim: ncell x nperiods) -----------------------

# Environmental: retain crop, gpw, gfc, precseas, wdpa, evi (from Data-Explore code)
crop <- cov_subset("Covariates for modelling/primary_occ_covariates_crop_sites.csv", periods)
precseas <- cov_subset("Covariates for modelling/primary_occ_covariates_precseas_sites.csv", periods)
wdpa <- cov_subset("Covariates for modelling/primary_occ_covariates_WDPA_sites.csv", periods)
evi <- cov_subset("Covariates for modelling/primary_occ_covariates_EVI_sites.csv", periods) 
gfc <- cov_subset("Covariates for modelling/primary_occ_covariates_GFC_sites.csv", periods)

# log-transform 
gpw <- read.csv("Covariates for modelling/primary_occ_covariates_GPW_sites.csv") %>% 
  dplyr::select(-any_of("X")) %>% 
  filter(cell_ID %in% cell_IDs) %>% 
  arrange(cell_ID)
gpw <- gpw %>% mutate(across(-cell_ID, log)) #log-transform 
keep_cols <- intersect(paste0("X", unique(periods$period_counter)), names(gpw))
gpw <- gpw[, keep_cols] %>% as.matrix()

# Number of deployments 
ndepl <- matrix(0, nrow = ncell, ncol = nperiod)
rownames(ndepl) <- unique(sort(covs$cell_ID))
colnames(ndepl) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))
for(i in 1:nrow(covs)){
  ndepl[as.character(covs$cell_ID[i]),
        as.character(paste0("p", covs$period_counter[i]))] <- covs$n_depl[i]
}

# Project IDs  
proj <- matrix(0, nrow = ncell, ncol = nperiod)
rownames(proj) <- unique(sort(covs$cell_ID))
colnames(proj) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))
covs$proj_ft <- as.numeric(as.factor(covs$proj))
for(i in seq_len(nrow(covs))){
  r <- as.character(covs$cell_ID[i])
  c <- paste0("p", covs$period_counter[i])
  proj[r, c] <- covs$proj_ft[i]
}

# Trend 
trend <- data.frame(cell_ID = unique(covs$cell_ID)) %>%
  tidyr::crossing(periods[1:2]) %>% 
  pivot_wider(id_cols = cell_ID, names_from = period_counter, values_from = period_counter) %>% 
  arrange(cell_ID) %>% 
  dplyr::select(-cell_ID) %>% 
  as.matrix() 


## Covariates: observation-level (dim: ncell x nperiod x nweek) ---------------

# Effort 
effort_temp <- covs %>% 
  dplyr::select(cell_ID, period_counter, effort_w1:effort_w13) %>% 
  mutate_at(vars(matches("effort_w")), scale2) %>%   
  pivot_longer(effort_w1:effort_w13, names_to = "week", values_to = "effort", 
               values_drop_na = TRUE) %>% 
  mutate(week = as.numeric(str_remove(week, "effort_w")),
         period_counter = paste0("p", period_counter))

effort <- array(NA, dim = c(ncell, nperiod, nweek)) #change NA to 0? 
rownames(effort) <- unique(sort(covs$cell_ID))
colnames(effort) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))

for(i in 1:nrow(effort_temp)){
  effort[as.character(effort_temp$cell_ID[i]),
         as.character(effort_temp$period_counter[i]),
         effort_temp$week[i]] <- effort_temp$effort[i]
}

str(effort)  
hist(effort[,,3]) #data is pre-scaled: no need to scale further

# Minjul 
minjul_temp <- covs %>% 
  dplyr::select(cell_ID, period_counter, starts_with("minjul_w")) %>% 
  mutate_at(vars(matches("minjul_w")), scale3) %>% 
  pivot_longer(minjul_w1:minjul_w13, names_to = "week", values_to = "minjul", 
               values_drop_na = TRUE) %>% 
  mutate(week = as.numeric(str_remove(week, "minjul_w")),
         period_counter = paste0("p", period_counter))

minjul <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(minjul) <- unique(sort(covs$cell_ID))
colnames(minjul) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))

for(i in 1:nrow(minjul_temp)){
  minjul[as.character(minjul_temp$cell_ID[i]),
         as.character(minjul_temp$period_counter[i]),
         minjul_temp$week[i]] <- minjul_temp$minjul[i]
}

str(minjul) 
hist(minjul[,,3]) #data is pre-scaled: no need to scale further

# Minjul Squared  
minjul_sq <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(minjul_sq) <- unique(covs$cell_ID)
colnames(minjul_sq) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))

for(i in 1:nrow(minjul_temp)){
  minjul_sq[as.character(minjul_temp$cell_ID[i]),
            as.character(minjul_temp$period_counter[i]),
            minjul_temp$week[i]] <- minjul_temp$minjul[i]^2
}

str(minjul_sq)
hist(minjul_sq[,,3]) #data is pre-scaled: no need to scale further


## Modelling ------------------------------------------------------------------

# Following vignette found at: 
# https://doserlab.com/files/spoccupancy-web/articles/svcmodels#introduction

## Look at raw occupancy data -------------------------------------------------

# Raw occurrence proprortions  
raw.occ.prob <- apply(yarray, 2, mean, na.rm = TRUE)
raw.occ.prob.df <- data.frame(x = names(raw.occ.prob), y = raw.occ.prob)
raw.occ.prob.df$x <- factor(raw.occ.prob.df$x, levels = mixedsort(unique(raw.occ.prob.df$x)))

# If necessary: 
sum(is.na(raw.occ.prob.df$y))
raw.occ.prob.df <- raw.occ.prob.df[!is.na(raw.occ.prob.df$y),]

# Format breaks 
period_dates <- periods %>%
  mutate(month = case_when(
    season == "Winter" ~ 1,
    season == "Spring" ~ 4,
    season == "Summer" ~ 7,
    season == "Fall"   ~ 10),
    period_date = ymd(paste(year, month, "01", sep = "-")),
    label_year = ifelse(month == 4, as.character(year), ""))

raw.occ.prob.df <- raw.occ.prob.df %>%
  mutate(period_counter = as.integer(sub("p", "", x))) %>%
  left_join(period_dates, by = "period_counter") %>%
  dplyr::select(period_date, y, label_year)

# Fill in missing periods with y = 0
all_period_dates <- seq(
  min(period_dates$period_date),
  max(period_dates$period_date),
  by = "3 months")

raw.occ.prob.df <- tibble(period_date = all_period_dates) %>%
  left_join(raw.occ.prob.df, by = "period_date") %>%
  mutate(y = ifelse(is.na(y), 0, y),
         label_year = ifelse(month(period_date) == 4, year(period_date), ""))

# Plot
(raw_occ_plot <- ggplot(raw.occ.prob.df, aes(x = period_date, y = y)) +
    geom_point() +
    theme_bw() +
    xlab("Year") +
    ylab("Raw Occurrence Proportion") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
    scale_x_date(breaks = raw.occ.prob.df$period_date,
                 labels = raw.occ.prob.df$label_year) +
    labs(title = "Raw Occupancy",
         subtitle = bquote(italic(.(target_species)))))

file_name <- paste(new_dir_path, "/", target_species, "_rawOccProp.jpg", sep="")
ggsave(filename = file_name, plot = raw_occ_plot, width = 10, height = 4, units = "in")

# If outliers, remove: 
# yarray 
#cols_to_keep <- dimnames(yarray)[[2]] != "p3"
#yarray <- yarray[, cols_to_keep, ]

# 3d covariates 
#cols_to_keep <- dimnames(effort)[[2]] != "p3"
#effort <- effort[, cols_to_keep, ]
#cols_to_keep <- dimnames(minjul)[[2]] != "p3"
#minjul <- minjul[, cols_to_keep, ]
#cols_to_keep <- dimnames(minjul_sq)[[2]] != "p3"
#minjul_sq <- minjul_sq[, cols_to_keep, ]

# 2d covariates
#crop <- crop[, dimnames(crop)[[2]] != "X3"]
#precseas <- precseas[, dimnames(precseas)[[2]] != "X3"]
#wdpa <- wdpa[, dimnames(wdpa)[[2]] != "X3"]
#evi <- evi[, dimnames(evi)[[2]] != "X3"]
#gfc <- gfc[, dimnames(gfc)[[2]] != "X3"]
#gpw <- gpw[, dimnames(gpw)[[2]] != "X3"]
#ndepl <- ndepl[, dimnames(ndepl)[[2]] != "p3"]
#proj <- proj[, dimnames(proj)[[2]] != "p3"]
#trend <- trend[, dimnames(trend)[[2]] != "3"]

# Naive plot of occurrence detections -----------------------------------------

# FIX THIS 

# Bounding box
ref <- rast("10x10 km spatial layers/raster_100km2_with_cellID.tif")
plot_coords <- as.data.frame(coords)  # coords should match yarray
bbox <- ext(min(plot_coords$x), max(plot_coords$x),
            min(plot_coords$y), max(plot_coords$y))
r_cropped <- crop(ref, bbox)
r_df <- as.data.frame(r_cropped, xy = TRUE, cells = TRUE)

# Naive occupancy 
naive_vals <- rowSums(yarray, na.rm = TRUE)
naive_df <- data.frame(
  cell_ID = as.numeric(rownames(yarray)),
  naive_occupancy = factor(ifelse(naive_vals == 0, 0, 1))
)

naive_plot <- r_df %>% left_join(naive_df, by = "cell_ID")

# Plot 
ggplot(naive_plot, aes(x = x, y = y, fill = naive_occupancy)) +
  geom_raster() +
  coord_equal() +
  theme_bw(base_size = 16) +
  labs(title = "Naive occurrence", fill = "") +
  scale_fill_viridis_d(option = "plasma", na.value = "grey80")


## Model formulas -------------------------------------------------------------
# Note: because renamed covariates in list, don't need to rename (e.g., "crop_test") here 

occ.formula = ~ scale(crop) + scale(gpw) + scale(precseas) + scale(wdpa) + 
  scale(evi) + scale(gfc) + scale(road_dist) + scale(tri) + scale(river_dist) + 
  scale(coeff_var) + scale(cities_lg) + scale(trend)
det.formula = ~ effort + minjul + minjul_sq + ndepl + (1|proj) #already scaled 

svc.cols = c(1:13)  #covariates whose effects are estimated as SVCs, including intercept

# If running full model 
dat_ls <- list(y = yarray,
               occ.covs = list(crop = crop, 
                               gpw = gpw,
                               gfc = gfc, 
                               precseas = precseas,
                               wdpa = wdpa,
                               evi = evi, 
                               road_dist = static$road_dist,
                               tri = static$tri,
                               river_dist = static$river_dist,
                               coeff_var = static$coeff_var,
                               cities_lg = static$cities_lg,
                               trend = trend),
               det.covs = list(effort = effort,        # already scaled! 
                               minjul = minjul,        # already scaled! 
                               minjul_sq = minjul_sq,  # already scaled! 
                               ndepl = ndepl, 
                               proj = proj),
               coords = coords)


# Prep inits -------------------------------------------------------------------

y_dat <- yarray      # choose yarray or y.fit 
coord_dat <- coords  # choose coords or coord.fit 

z.init <- apply(y_dat, c(1, 2), function(a) as.numeric(sum(a, na.rm = TRUE) > 0))

dist.data <- dist(coord_dat)            # pairwise distance between all sites 

inits.list <- list(beta = 0,            # occurrence coefficients
                   alpha = 0,           # detection coefficients
                   z = z.init,          # latent occurrence values
                   phi = 3 / mean(dist.data),  
                   sigma.sq = 0.5,      
                   w = matrix(0, length(svc.cols), nrow(y_dat)),  
                   #rho = 0,  
                   sigma.sq.t = 0.5)    # occurrence random effect variances 

priors.list <- list(alpha.normal = list(mean = 0, var = 2.72), 
                    beta.normal = list(mean = 0, var = 2.72),
                    sigma.sq.ig = list(a = 2, b = 0.5),
                    phi.unif = list(a = 3 / max(dist.data), 
                                    b = 3 / min(dist.data)))


# Tuning ----------------------------------------------------------------------

n.chains <- 3
tuning.list <- list(phi = 1, rho = 1)
n.batch <- 400  
n.burn = 2000  
n.thin = 10 
n.report = 40  
batch.length <- 25
n.iter <- n.batch * batch.length 

## Run model ------------------------------------------------------------------

dat <- dat_ls  # choose dat_ls or dat_ls_split 

start_time <- Sys.time()
out <- svcTPGOcc(occ.formula = occ.formula, 
                 det.formula = det.formula,
                 data = dat, 
                 inits = inits.list,
                 n.batch = n.batch,
                 batch.length = batch.length, 
                 priors = priors.list,  
                 svc.cols = svc.cols,
                 cov.model = "exponential",  #alternative to test could be 'spherical'
                 NNGP = TRUE, 
                 n.neighbors = 5,  
                 tuning = tuning.list, 
                 n.report = n.report, 
                 n.burn = n.burn,
                 n.thin = n.thin,
                 n.chains = n.chains,
                 verbose = TRUE,
                 ar1 = TRUE) 

# if using k-fold cross validation, would add k-fold parameters to the model: 
#k.fold = 4, 
#k.fold.threads = 4,
#k.fold.only = TRUE) 
#str(k.fold.sp)

end_time <- Sys.time()
end_time - start_time 

## IF THIS ERRORS ON DETECTION COVARIATES ## 

# Where is there detection but missing effort? 
discrepancies <- !is.na(yarray) & is.na(effort)
(idx <- which(discrepancies, arr.ind = TRUE)) #three instances 
length(idx)

# For now, add 1 to effort where there is a detection; 
# HOWEVER, working on backtracking to figure out where this issue is coming from
effort[discrepancies] <- 1
sum(!is.na(yarray) & is.na(effort))  # should be 0

# Now, rerun code from line 380 ('Organize data') 


## Convergence assessment -----------------------------------------------------

# Visually examine trace plots
# -> We can first visually examine trace plots for convergence by plotting chains for each parameter. Trace plots indicative of convergence look like random scatter around a mean value and do not display any trends. For these reasons, trace plots indicative of convergence are often described as looking like “fuzzy caterpillars.” Below, the trace plots for both the occurrence and detection coefficients are consistent with model convergence.

# Occurrence parameters:
# The spocc sampling did sample 3 chains, but then stacked them on top of each other; to plot them separately we have to divide them back into chains:
occ.samps <- as.data.frame(out$beta.samples)
occ.samps$x <- rep(1:(nrow(occ.samps)/n.chains), n.chains)
occ.samps$chain <- as.factor(rep(1:3, each = nrow(occ.samps)/n.chains))
names(occ.samps)[c(1:13)] <- c("Intercept", "Crop", "GPW", "Precip", "WDPA", "EVI", "GFC", "Road Dist", "TRI", "River Dist", "Coeff Var", "City Dist", "Trend")
occ.samps.long <- gather(occ.samps, parameter, value,
                         Intercept:Trend, factor_key = T)

ggplot(occ.samps.long) +
  geom_line(aes(x = x, y = value, col = chain, group = chain)) +
  labs(x ="Iteration") +
  facet_wrap(~ parameter, ncol = 2, scales = "free_y") +
  theme_bw()

# Detection parameters:
# The spocc sampling did sample 3 chains, but then stacked them on top of each other; to plot them separately we have to divide them back into chains:
det.samps <- as.data.frame(out$alpha.samples)
det.samps$x <- rep(1:(nrow(det.samps)/n.chains), n.chains)
det.samps$chain <- as.factor(rep(1:3, each = nrow(det.samps)/n.chains))
names(det.samps)[c(1:5)] <- c("Intercept", "Effort", "Min Jul", "Min Jul Sq", "N. Depl")
det.samps.long <- gather(det.samps, parameter, value, Intercept:`Min Jul Sq`, factor_key = T )

ggplot(det.samps.long) +
  geom_line(aes(x = x, y = value, col = chain, group = chain)) +
  labs(x ="Iteration") +
  facet_wrap(~ parameter, ncol = 2, scales = "free_y") +
  theme_bw()

# Gelman-Rubin diagnostics:
# The Gelman-Rubin diagnostic, rˆ (r-hate), formally assesses model convergence by comparing variance of parameter estimates among chains to variance of parameter estimates within chains. A large rˆ means variance among chains is greater than within chains, implying lack of convergence. When assessing convergence it is standard to accept rˆ values below 1.1 (Hobbs and Hooten 2015). The rˆ value can be found for each parameter in the summary output under Rhat.
plot(out, 'beta', density = FALSE) #occupancy convergence
plot(out, 'alpha', density = FALSE) #detection convergence

# Effective Sample Size (ESS):
# ESS estimates the number of samples the MCMC sample would contain if correlated samples were removed, providing a measure of the information contained in the MCMC sample. If ESS is low compared to the MCMC sample size this may imply poor mixing and high autocorrelation, and thinning may help to address this. Larger ESS indicates a better sample, but there is no distinct rule for a “large enough” ESS.
# We compare the ESS (denoted as ESS in the summary output) to the MCMC sample size and confirm there is a large ESS for each parameter.
summary(out) 

# Set sampling parameters to reach satisfactory diagnostics:
# If any of the three diagnostics (trace plots, Gelman-Rubin Diagnostics, and ESS) are not satisfactory for any of the parameters, adjust the sampling parameters (n.thin and n.samples) and resample. For example, we first ran this model with n.thin = 2 and n.samples = 9000 and got unsatisfactory trace plots and ESSs for the occurrence coefficients. Trace plots were showing spikes indicating high autocorrelation and poor mixing, so we increased the thinning interval and total number of posterior samples.


## Model fit ------------------------------------------------------------------

# Widely Applicable Information Criterion (WAIC)
waicOcc(out)

# Goodness of Fit: Posterior predictive checks

# -> A Bayesian p-value can be used to measure the significance of the deviation of the data from the model predictions, i.e., summarize the previous posterior predictive distribution plots in a single value (Gelman et al. 1995). To evaluate the level to which the distribution of predicted values aligns with the data, the Bayesian p-value relies on a test statistic or discrepency measure, which provides a scalar value that can be compared for observed data versus the posterior predictive distribution. The Bayesian p-value is then defined as the probability that the test statistic for the predicted data is more extreme than the test statistic for the observed data, given the observed data. Multiple test statistics can be used to evaluate model fit, and we use the Freeman-Tukey statistic recommended by Kéry and Royle (2016) and included within the Spocc package. Grouping or binning the binary data based on cells (group=1) or site-nights (group=2) provides two test statistics that may reveal different shortcomings of the model (i.e., problems of model fit in occurrence versus detection)

# -> A Bayesian p-value close to 0 or 1 implies the model does not accurately represent the distribution of the data (Hobbs and Hooten 2015). Note that the Bayesian p-value is a general goodness of fit statistic and won’t provide more information about where in the model potential lack of fit may be occurring (Warton et al. 2017). [if Bayesian p-values close to 0.5, indicates adequate model fit; values >0.1 indicate no lack of fit; values close to 1 require further investigation]

ppc.out.1 <- ppcOcc(out, fit.stat = 'freeman-tukey', group = 1)
summary(ppc.out.1)

ppc.out.2 <- ppcOcc(out, fit.stat = 'freeman-tukey', group = 2)
summary(ppc.out.2)


## Examine outputs ------------------------------------------------------------
summary(out)
plogis(2.13) #convert logit values to probability scale

# If performed K-fold cross validation (want lower values):
#k.fold.non.sp$k.fold.deviance
#k.fold.sp$k.fold.deviance


## Model comparison -----------------------------------------------------------

#may want to try running with and without AR1=T vs F ?
#could try cov.model = "exponential" vs 'spherical'


## Save files -------------------------------------------------------------------

(filename <- paste("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights/Model outputs/", file_species, "/", file_species, "_svcTPGOcc", "_", Sys.Date(), ".RData", sep = ""))
save(yarray, covs, target_species, out, coords, dat_ls, periods, file = filename)
