##########################
##### Running models ##### 
##########################

# Set working directory
setwd("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights")
rm(list=ls()); gc()
set.seed(123)

# Load libraries 
library(dplyr)
library(forcats)
library(lubridate)
library(purrr)
library(reshape2)
library(sf)
library(spOccupancy)
library(stringr)
library(terra)
library(tibble)
library(tidyr)
library(tidyverse) 
library(coda)
library(stars)
library(ggplot2)
library(gtools)

## User-defined functions -----------------------------------------------------

# Scaling functions
scale2 <- function(x, na.rm = FALSE) ((x - mean(1:7))/sd(1:7))
scale3 <- function(x, na.rm = FALSE) ((x - mean(1:365))/sd(1:365))

# Covariate formatting function 
cov_subset <- function(cov_dataframe_name, period_data){
  covariate <- read.csv(cov_dataframe_name) %>% 
    dplyr::select(-any_of("X")) %>% 
    filter(cell_ID %in% cell_IDs) %>% 
    arrange(cell_ID)
  keep_cols <- intersect(paste0("X", unique(period_data$period_counter)), names(covariate))
  df_subset <- covariate[, keep_cols] %>% 
    as.matrix()
  return(df_subset)
}


## Load prepped data ----------------------------------------------------------

(filenames <- list.files("Data for modelling", full.names = TRUE))

#CHANGE
filename <- filenames[1]

load(filename)
print(target_species)
print(nrow(ystack))
print(nrow(covs))

# Create new directory to store outputs 
new_dir_path <- paste("Model outputs/",target_species, "_", Sys.Date(), sep="")
if (!dir.exists(new_dir_path)) {
  dir.create(new_dir_path)
} else {
  print("Directory already exists")
}

## Organize data and covariates -----------------------------------------------

# Remove cases without cell_ID
ystack <- cbind(ystack, covs %>% dplyr::select(cell_ID)) %>% drop_na(cell_ID)
covs <- covs %>% drop_na(cell_ID)

# Set dimensions
(ncell <- length(unique(covs$cell_ID)))
(nperiod <- length(seq(min(covs$period_counter), max(covs$period_counter), by = 1)))
nweek <- 13
(min_year <- min(covs$year))
(max_year <- max(covs$year))
cell_IDs <- unique(ystack$cell_ID)

# -> if identify outliers, remove period here
# min(covs$period_counter)
periods <- data.frame(period_counter = seq(min(covs$period_counter), 
                                           max(covs$period_counter), by = 1)) %>% 
  left_join(covs %>% dplyr::select(period_counter, season, year) %>% distinct()) %>% 
  mutate(year = as.numeric(year))
head(periods)

## Fill in missing seasons and years
# Define the order of seasons
season_order <- c("Spring", "Summer", "Fall", "Winter")

# If first row is NA, initialize
if(is.na(periods$season[1])) periods$season[1] <- "Fall"
if(is.na(periods$year[1])) periods$year[1] <- 2000  # choose a starting year
head(periods)

# Fill remaining rows
for(i in 2:nrow(periods)) {
  prev_season <- periods$season[i-1]
  prev_year   <- periods$year[i-1]
  
  if(is.na(periods$season[i])) {
    # Compute next season
    next_season <- season_order[(match(prev_season, season_order) %% 4) + 1]
    periods$season[i] <- next_season
    
    # Increment year if season wraps from Winter -> Spring
    if(prev_season == "Winter") {
      periods$year[i] <- prev_year + 1
    } else {
      periods$year[i] <- prev_year
    }
  }
}
head(periods)

## Observations ---------------------------------------------------------------
ystack2 <- cbind(ystack, covs %>% dplyr::select(cell_ID, period_counter)) %>% 
  pivot_longer(V1:V13, names_to = "week", values_to = "det", values_drop_na = TRUE) %>% 
  mutate(week = as.numeric(str_remove(week, pattern = "V")),
         period_counter = paste0("p", period_counter)) 
ystack2 <- ystack2[order(ystack2$cell_ID),]

yarray <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(yarray) <- unique(covs$cell_ID)
colnames(yarray) <- paste0("p", seq(min(covs$period_counter), 
                                    max(covs$period_counter), by = 1))

for(i in 1:nrow(ystack2)){
  yarray[as.character(ystack2$cell_ID[i]),
         as.character(ystack2$period_counter[i]),
         ystack2$week[i]] <- ystack2$det[i]
}


## Coordinates ----------------------------------------------------------------
coords <- as.data.frame(rast("10x10 km spatial layers/raster_100km2_with_cellID.tif"), xy=T) %>% 
  filter(cell_ID %in% cell_IDs) %>% 
  arrange(cell_ID) %>% 
  dplyr::select(-cell_ID) %>% 
  as.matrix()


## Covariates (all) -----------------------------------------------------------

# We scale continuous occurrence and detection covariates to have mean 0 and standard deviation of 1 for model fitting, with occurrence covariates scaled across the entire study area. Scaling facilitates selection of priors and manages possible differences in scales of covariate measurements. Scaling was performed in the data formatting section. Scale before inclusion in model to make easier to scale full covariates for prediction. 

## Covariates: spatial (dim: ncell) ------------------------------------------- 

static <- read.csv("Covariates for modelling/site_level_covariates_site.csv") %>% 
  dplyr::select(-c(elevation, cities_md, cities_sm, x, y)) %>% #from Data-Explore code 
  filter(cell_ID %in% cell_IDs) %>% 
  arrange(cell_ID)
nrow(static) #select out each column want as a vector 

static_scaled <- static %>% 
  mutate(across(
    .cols = c(road_dist, tri, river_dist, coeff_var, cities_lg),
    .fns = ~ (. - mean(., na.rm = TRUE)) / sd(., na.rm = TRUE),
    .names = "{.col}_scaled"
  )) %>% 
  dplyr::select(-c(road_dist, tri, river_dist, coeff_var, cities_lg))


## Covariates: primary occasion (dim: ncell x nperiods) -----------------------

# Environmental: retain crop, gpw, gfc, precseas, wdpa, evi (from Data-Explore code)
crop <- cov_subset("Covariates for modelling/primary_occ_covariates_crop_sites.csv", periods)
precseas <- cov_subset("Covariates for modelling/primary_occ_covariates_precseas_sites.csv", periods)
wdpa <- cov_subset("Covariates for modelling/primary_occ_covariates_WDPA_sites.csv", periods)
evi <- cov_subset("Covariates for modelling/primary_occ_covariates_EVI_sites.csv", periods) 
gfc <- cov_subset("Covariates for modelling/primary_occ_covariates_GFC_sites.csv", periods)
gpw <- cov_subset("Covariates for modelling/primary_occ_covariates_GPW_sites.csv", periods) 

# Number of deployments 
ndepl <- matrix(0, nrow = ncell, ncol = nperiod)
rownames(ndepl) <- unique(sort(covs$cell_ID))
colnames(ndepl) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))
for(i in 1:nrow(covs)){
  ndepl[as.character(covs$cell_ID[i]),
        as.character(paste0("p", covs$period_counter[i]))] <- covs$n_depl[i]
}

# Project IDs  
proj <- matrix(0, nrow = ncell, ncol = nperiod)
rownames(proj) <- unique(sort(covs$cell_ID))
colnames(proj) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))
covs$proj_ft <- as.numeric(as.factor(covs$proj))
for(i in seq_len(nrow(covs))){
  r <- as.character(covs$cell_ID[i])
  c <- paste0("p", covs$period_counter[i])
  proj[r, c] <- covs$proj_ft[i]
}

# Trend 
trend <- data.frame(cell_ID = unique(covs$cell_ID)) %>%
  tidyr::crossing(periods[1:2]) %>% 
  pivot_wider(id_cols = cell_ID, names_from = period_counter, values_from = period_counter) %>% 
  arrange(cell_ID) %>% 
  dplyr::select(-cell_ID) %>% 
  as.matrix() 


## Covariates: observation-level (dim: ncell x nperiod x nweek) ---------------

# Effort 
effort_temp <- covs %>% 
  dplyr::select(cell_ID, period_counter, effort_w1:effort_w13) %>% 
  mutate_at(vars(matches("effort_w")), scale2) %>%   
  pivot_longer(effort_w1:effort_w13, names_to = "week", values_to = "effort", 
               values_drop_na = TRUE) %>% 
  mutate(week = as.numeric(str_remove(week, "effort_w")),
         period_counter = paste0("p", period_counter))

effort <- array(NA, dim = c(ncell, nperiod, nweek)) #change NA to 0? 
rownames(effort) <- unique(sort(covs$cell_ID))
colnames(effort) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))

for(i in 1:nrow(effort_temp)){
  effort[as.character(effort_temp$cell_ID[i]),
         as.character(effort_temp$period_counter[i]),
         effort_temp$week[i]] <- effort_temp$effort[i]
}

str(effort)  
hist(effort[,,3]) #data is pre-scaled: no need to scale further

### NOTE #### 

# There are some "effort" values in covs that are NA when there are detection 
# values in ystack; this code identifies and throws a quick patch on these issues
# HOWEVER, need to go back to review how effort is generated to understand underlying
# issue 

## Detect issues 

# Ensure rows align by cell_ID
common_cells <- intersect(ystack$cell_ID, covs$cell_ID)
det_aligned <- ystack[match(common_cells, ystack$cell_ID), ]
effort_aligned <- covs[match(common_cells, covs$cell_ID), ]

# Columns of interest
det_cols <- paste0("V", 1:13)
effort_cols <- paste0("effort_w", 1:13)

# Make sure the number of columns match
if(length(det_cols) != length(effort_cols)) {
  stop("Detection and effort columns must be the same length.")
}

# If stopped, locate discrepancies 
discrepancies <- (!is.na(det_aligned[det_cols])) & (is.na(effort_aligned[effort_cols]))
which_discrepancies <- which(discrepancies, arr.ind = TRUE)
discrepancy_table <- data.frame(
  cell_ID = det_aligned$cell_ID[which_discrepancies[, "row"]],
  period  = det_cols[which_discrepancies[, "col"]],
  stringsAsFactors = FALSE
)
nrow(discrepancy_table)

## Patch effort: set NA to 1 wherever a detection exists
for (i in seq_along(det_cols)) {
  det_col <- det_cols[i]
  effort_col <- effort_cols[i]
  
  idx <- !is.na(det_aligned[[det_col]]) & is.na(effort_aligned[[effort_col]])
  effort_aligned[[effort_col]][idx] <- 1
}

# Replace the original effort_df rows for these cell_IDs
covs[match(common_cells, covs$cell_ID), effort_cols] <- effort_aligned[effort_cols]

#########

# Minjul 
minjul_temp <- covs %>% 
  dplyr::select(cell_ID, period_counter, starts_with("minjul_w")) %>% 
  mutate_at(vars(matches("minjul_w")), scale3) %>% 
  pivot_longer(minjul_w1:minjul_w13, names_to = "week", values_to = "minjul", 
               values_drop_na = TRUE) %>% 
  mutate(week = as.numeric(str_remove(week, "minjul_w")),
         period_counter = paste0("p", period_counter))

minjul <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(minjul) <- unique(sort(covs$cell_ID))
colnames(minjul) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))

for(i in 1:nrow(minjul_temp)){
  minjul[as.character(minjul_temp$cell_ID[i]),
         as.character(minjul_temp$period_counter[i]),
         minjul_temp$week[i]] <- minjul_temp$minjul[i]
}

str(minjul) 
hist(minjul[,,3]) #data is pre-scaled: no need to scale further

# Minjul Squared  
minjul_sq <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(minjul_sq) <- unique(covs$cell_ID)
colnames(minjul_sq) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))

for(i in 1:nrow(minjul_temp)){
  minjul_sq[as.character(minjul_temp$cell_ID[i]),
            as.character(minjul_temp$period_counter[i]),
            minjul_temp$week[i]] <- minjul_temp$minjul[i]^2
}

str(minjul_sq)
hist(minjul_sq[,,3]) #data is pre-scaled: no need to scale further


## Modelling ------------------------------------------------------------------

# Following vignette found at: 
# https://doserlab.com/files/spoccupancy-web/articles/svcmodels#introduction

## Look at raw occupancy data -------------------------------------------------

# Raw occurrence proprortions -------------------------------------------------
# How adequate is a linear trend for our analysis?
raw.occ.prob <- apply(yarray, 2, mean, na.rm = TRUE)
raw.occ.prob.df <- data.frame(x = names(raw.occ.prob), y = raw.occ.prob)
raw.occ.prob.df$x <- factor(raw.occ.prob.df$x, levels = mixedsort(unique(raw.occ.prob.df$x)))
(raw_occ_plot <- ggplot(raw.occ.prob.df, aes(x, y)) + geom_point() + theme_bw() + 
    xlab("Period") + ylab("Raw Occurrence Proportion") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)))
file_name <- paste(new_dir_path, "/", target_species, "_rawOccProp.jpg", sep="")
ggsave(filename = file_name, plot = raw_occ_plot, width = 10, height = 4, units = "in")

## If outliers, remove --- 

# yarray 
cols_to_keep <- dimnames(yarray)[[2]] != "p3"
yarray <- yarray[, cols_to_keep, ]

# 3d covariates 
cols_to_keep <- dimnames(effort)[[2]] != "p3"
effort <- effort[, cols_to_keep, ]
cols_to_keep <- dimnames(minjul)[[2]] != "p3"
minjul <- minjul[, cols_to_keep, ]
cols_to_keep <- dimnames(minjul_sq)[[2]] != "p3"
minjul_sq <- minjul_sq[, cols_to_keep, ]

# 2d covariates
crop <- crop[, dimnames(crop)[[2]] != "X3"]
precseas <- precseas[, dimnames(precseas)[[2]] != "X3"]
wdpa <- wdpa[, dimnames(wdpa)[[2]] != "X3"]
evi <- evi[, dimnames(evi)[[2]] != "X3"]
gfc <- gfc[, dimnames(gfc)[[2]] != "X3"]
gpw <- gpw[, dimnames(gpw)[[2]] != "X3"]
ndepl <- ndepl[, dimnames(ndepl)[[2]] != "p3"]
proj <- proj[, dimnames(proj)[[2]] != "p3"]
trend <- trend[, dimnames(trend)[[2]] != "3"]

# Naive plot of occurrence detections -----------------------------------------

# Bounding box
ref <- rast("10x10 km spatial layers/raster_100km2_with_cellID.tif")
plot_coords <- as.data.frame(coords)  # coords should match yarray
bbox <- ext(min(plot_coords$x), max(plot_coords$x),
            min(plot_coords$y), max(plot_coords$y))
r_cropped <- crop(ref, bbox)
r_df <- as.data.frame(r_cropped, xy = TRUE, cells = TRUE)

# Naive occupancy 
naive_vals <- rowSums(yarray, na.rm = TRUE)
naive_df <- data.frame(
  cell_ID = as.numeric(rownames(yarray)),
  naive_occupancy = factor(ifelse(naive_vals == 0, 0, 1))
)

naive_plot <- r_df %>% left_join(naive_df, by = "cell_ID")

# Plot 
ggplot(naive_plot, aes(x = x, y = y, fill = naive_occupancy)) +
  geom_raster() +
  coord_equal() +
  theme_bw(base_size = 16) +
  labs(title = "Naive occurrence", fill = "") +
  scale_fill_viridis_d(option = "plasma", na.value = "grey80")


## Subset data if appropriate -------------------------------------------------

# IF TESTING: for speed  
ncell = 200

yarray_test <- yarray[1:ncell,,]
coords_test <- coords[1:ncell,]

static_test <- static[1:ncell,]
gpw_test <- gpw[1:ncell,]
evi_test <- evi[1:ncell,]
crop_test <- crop[1:ncell,]
gfc_test <- gfc[1:ncell,]
precseas_test <- precseas[1:ncell,]
wdpa_test <- wdpa[1:ncell,]

ndepl_test <- ndepl[1:ncell,]
proj_test <- proj[1:ncell,]
trend_test <- trend[1:ncell,]
effort_test <- effort[1:ncell,,]  
minjul_test <- minjul[1:ncell,,]  
minjul_sq_test <- minjul_sq[1:ncell,,]

# If SPLITTING: fit model with 75% of locations to train, remaining 25% to predict 
J <- length(unique(covs$cell_ID)) #1102 sites
pred.indx <- sample(1:J, round(J * .25), replace = FALSE)

y.fit <- yarray[-pred.indx,,]
y.pred <- yarray[pred.indx,,]
coords.fit <- coords[-pred.indx,]
coords.pred <- coords[pred.indx,]

static.fit <- static[-pred.indx,]
static.pred <- static[pred.indx,]
gpw.fit <- gpw[-pred.indx,]
gpw.pred <- gpw[pred.indx,]
evi.fit <- evi[-pred.indx,]
evi.pred <- evi[pred.indx,]
crop.fit <- crop[-pred.indx,]
crop.pred <- crop[pred.indx,]
gfc.fit <- gfc[-pred.indx,]
gfc.pred <- gfc[pred.indx,]
precseas.fit <- precseas[-pred.indx,]
precseas.pred <- precseas[pred.indx,]
wdpa.fit <- wdpa[-pred.indx,]
wdpa.pred <- wdpa[pred.indx,]
ndepl.fit <- ndepl[-pred.indx,]
ndepl.pred <- ndepl[pred.indx,]
proj.fit <- proj[-pred.indx,]
proj.pred <- proj[pred.indx,]
trend.fit <- trend[-pred.indx,]
trend.pred <- trend[pred.indx,]
effort.fit <- effort[-pred.indx,,] 
effort.pred <- effort[pred.indx,,] 
minjul.fit <- minjul[-pred.indx,,]  
minjul.pred <- minjul[pred.indx,,]  
minjul_sq.fit <- minjul_sq[-pred.indx,,]
minjul_sq.pred <- minjul_sq[pred.indx,,]


# Organize data for svcTPGOcc -------------------------------------------------

# if TEST 
dat_ls_test <- list(y = yarray_test,
                    occ.covs = list(crop = crop_test, 
                                    gpw = gpw_test,
                                    gfc = gfc_test, 
                                    precseas = precseas_test,
                                    wdpa = wdpa_test,
                                    evi = evi_test, 
                                    road_dist = static_test$road_dist,
                                    tri = static_test$tri,
                                    river_dist = static_test$river_dist,
                                    coeff_var = static_test$coeff_var,
                                    cities_lg = static_test$cities_lg,
                                    trend = trend_test),
                    det.covs = list(effort = effort_test, #already scaled! 
                                    minjul = minjul_test, #already scaled! 
                                    minjul_sq = minjul_sq_test, #already scaled! 
                                    ndepl = ndepl_test, 
                                    proj = proj_test),
                    coords = coords_test)

# if SPLITTNG 
dat_ls_split <- list(y = y.fit,
                     occ.covs = list(crop = crop.fit, 
                                     gpw = gpw.fit,
                                     gfc = gfc.fit, 
                                     precseas = precseas.fit,
                                     wdpa = wdpa.fit,
                                     evi = evi.fit,
                                     road_dist = static.fit$road_dist,
                                     tri = static.fit$tri,
                                     river_dist = static.fit$river_dist,
                                     coeff_var = static.fit$coeff_var,
                                     cities_lg = static.fit$cities_lg,
                                     trend = trend.fit),
                     det.covs = list(effort = effort.fit, #already scaled! 
                                     minjul = minjul.fit, #already scaled! 
                                     minjul_sq = minjul_sq.fit, #already scaled! 
                                     ndepl = ndepl.fit, 
                                     proj = proj.fit),
                     coords = coords.fit)

# if RUNNING FULL MODEL
dat_ls <- list(y = yarray,
               occ.covs = list(crop = crop, 
                               gpw = gpw,
                               gfc = gfc, 
                               precseas = precseas,
                               wdpa = wdpa,
                               evi = evi, 
                               road_dist = static_scaled$road_dist_scaled,
                               tri = static_scaled$tri_scaled,
                               river_dist = static_scaled$river_dist_scaled,
                               coeff_var = static_scaled$coeff_var_scaled,
                               cities_lg = static_scaled$cities_lg_scaled,
                               trend = trend),
               det.covs = list(effort = effort, #already scaled! 
                               minjul = minjul, #already scaled! 
                               minjul_sq = minjul_sq, #already scaled! 
                               ndepl = ndepl, 
                               proj = proj),
               coords = coords)

## Formulas -------------------------------------------------------------------

# DO NOT FORGET TO SCALE COVARIATES 
# note: because renamed covariates in list, don't need to rename (e.g., "crop_test") here 

occ.formula = ~ scale(crop) + scale(gpw) + scale(precseas) + scale(wdpa) + scale(evi) + scale(gfc) + road_dist + tri + river_dist + coeff_var + cities_lg + scale(trend)
det.formula = ~ effort + minjul + minjul_sq + ndepl + (1|proj)

svc.cols = c(1:13)  #covariates whose effects are estimated as SVCs, including intercept

# Prep inits -------------------------------------------------------------------

y_dat <- yarray #as yarray, yarray_test, or y.fit 
coord_dat <- coords #as coords, coords_test, or coord.fit 

z.init <- apply(y_dat, c(1, 2), function(a) as.numeric(sum(a, na.rm = TRUE) > 0))
#z <- apply(data.list$y, 1, max, na.rm = TRUE)  if what is in tutorial... 

dist.data <- dist(coord_dat)           #pairwise distance between all sites 

inits.list <- list(beta = 0,            #occurrence coefficients
                   alpha = 0,           #detection coefficients
                   z = z.init,          #latent occurrence values
                   phi = 3 / mean(dist.data), #instead of mean, fabiola has 0.5... 
                   sigma.sq = 0.5,      #fabiola had as 2... 
                   w = matrix(0, length(svc.cols), nrow(y_dat)),  #fabiola had rep(0, ncell) 
                   #rho = 0,  
                   sigma.sq.t = 0.5)    #occurrence random effect variances 

# Note: don't have to specify priors, can use svcPGOcc() defaults 
priors.list <- list(alpha.normal = list(mean = 0, var = 2.72), 
                    beta.normal = list(mean = 0, var = 2.72),
                    sigma.sq.ig = list(a = 2, b = 0.5),
                    phi.unif = list(a = 3 / max(dist.data), 
                                    b = 3 / min(dist.data)))


# Tuning ----------------------------------------------------------------------
n.chains <- 3
tuning.list <- list(phi = 1, rho = 1)

# testing 
  n.batch <- 10
  n.burn = 50
  n.thin = 1
  n.report = 10 

# otherwise 
n.batch <- 400  
n.burn = 2000  
n.thin = 10 
n.report = 40  

n.batch <- 800 #tutorial has 800
n.burn = 10000 #tutorial has 10000
n.thin = 10 
n.report = 100 #tutorial has 100

batch.length <- 25
n.iter <- n.batch * batch.length 


## Run model ------------------------------------------------------------------

dat <- dat_ls #CHANGE FOR EACH training, split, regular 

start_time <- Sys.time()
out <- svcTPGOcc(occ.formula = occ.formula, 
                 det.formula = det.formula,
                 data = dat, 
                 inits = inits.list,
                 n.batch = n.batch,
                 batch.length = batch.length, 
                 priors = priors.list,  
                 svc.cols = svc.cols,
                 cov.model = "exponential",  #alternative to test could be 'spherical'
                 NNGP = TRUE, 
                 n.neighbors = 5,  
                 tuning = tuning.list, 
                 n.report = n.report, 
                 n.burn = n.burn,
                 n.thin = n.thin,
                 n.chains = n.chains,
                 #search.type = 'cb', #in fabiola's code, not in tutorial? 
                 verbose = TRUE,
                 ar1 = TRUE) 
# would add k-fold parameters to this model if using k-fold cross validation ... 
#k.fold = 4, 
#k.fold.threads = 4,
#k.fold.only = TRUE) 
#str(k.fold.sp)

end_time <- Sys.time()
end_time - start_time 

## IF THIS ERRORS ON DETECTION COVARIATES ## 

# where is there detection but missing effort? 
discrepancies <- !is.na(yarray) & is.na(effort)
(idx <- which(discrepancies, arr.ind = TRUE)) #three instances 
length(idx)

# for now, add 1 to effort where there is a detection; 
# HOWEVER, working on backtracking to figure out where this issue is coming from
effort[discrepancies] <- 1
sum(!is.na(yarray) & is.na(effort))  # should be 0

# now, rerun code from line 378 ('organize data') 


## Convergence assessment -----------------------------------------------------

# Visually examine trace plots
# -> We can first visually examine trace plots for convergence by plotting chains for each parameter. Trace plots indicative of convergence look like random scatter around a mean value and do not display any trends. For these reasons, trace plots indicative of convergence are often described as looking like “fuzzy caterpillars.” Below, the trace plots for both the occurrence and detection coefficients are consistent with model convergence.

# -> Occurrence parameters:
# the spocc sampling did sample 3 chains, but then stacked them on top of each other; to plot them separately we have to divide them back into chains:
occ.samps <- as.data.frame(out$beta.samples)
occ.samps$x <- rep(1:(nrow(occ.samps)/n.chains), n.chains)
occ.samps$chain <- as.factor(rep(1:3, each =nrow(occ.samps)/n.chains))
names(occ.samps)[c(1:3)] <- c("Intercept", "Trend", "Crop")
occ.samps.long <- gather(
  occ.samps, parameter, value,
  Intercept:Crop, factor_key = T
)

ggplot(occ.samps.long) +
  geom_line(
    aes(x = x, y = value,
        col = chain, group = chain)) +
  labs(x ="Iteration") +
  facet_wrap(~ parameter, nrow = length(unique(occ.samps.long$parameter)), scales = "free_y") +
  theme_bw()

# -> Detection parameters:
# the spocc sampling did sample 3 chains, but then stacked them on top of each other; to plot them separately we have to divide them back into chains:
det.samps <- as.data.frame(out$alpha.samples)
det.samps$x <- rep(1:(nrow(det.samps)/n.chains), n.chains)
det.samps$chain <- as.factor(rep(1:3, each =nrow(det.samps)/n.chains))
names(det.samps)[c(1:4)] <- c("Intercept", "Effort", "Min_Jul", "Min_Jul_Sq")
det.samps.long <- gather(det.samps, parameter, value, Intercept:Min_Jul_Sq, factor_key = T )

ggplot(det.samps.long) +
  geom_line(
    aes(x = x, y = value,
        col = chain, group = chain)) +
  labs(x ="Iteration") +
  facet_wrap(~ parameter, nrow = length(unique(det.samps.long$parameter)), scales = "free_y") +
  theme_bw()

# Gelman-Rubin diagnostics
# -> The Gelman-Rubin diagnostic, rˆ (r-hate), formally assesses model convergence by comparing variance of parameter estimates among chains to variance of parameter estimates within chains. A large rˆ means variance among chains is greater than within chains, implying lack of convergence. When assessing convergence it is standard to accept rˆ values below 1.1 (Hobbs and Hooten 2015). The rˆ value can be found for each parameter in the summary output under Rhat.
plot(out, 'beta', density = FALSE) #occupancy convergence
plot(out, 'alpha', density = FALSE) #detection convergence

# Effective Sample Size (ESS)
# -> ESS estimates the number of samples the MCMC sample would contain if correlated samples were removed, providing a measure of the information contained in the MCMC sample. If ESS is low compared to the MCMC sample size this may imply poor mixing and high autocorrelation, and thinning may help to address this. Larger ESS indicates a better sample, but there is no distinct rule for a “large enough” ESS.
# -> We compare the ESS (denoted as ESS in the summary output) to the MCMC sample size and confirm there is a large ESS for each parameter.
summary(out) 

# Set sampling parameters to reach satisfactory diagnostics
# -> If any of the three diagnostics (trace plots, Gelman-Rubin Diagnostics, and ESS) are not satisfactory for any of the parameters, adjust the sampling parameters (n.thin and n.samples) and resample. For example, we first ran this model with n.thin = 2 and n.samples = 9000 and got unsatisfactory trace plots and ESSs for the occurrence coefficients. Trace plots were showing spikes indicating high autocorrelation and poor mixing, so we increased the thinning interval and total number of posterior samples.


## Model fit ------------------------------------------------------------------

# Widely Applicable Information Criterion (WAIC)
waicOcc(out)

# Goodness of Fit: posterior predictive checks

# -> A Bayesian p-value can be used to measure the significance of the deviation of the data from the model predictions, i.e., summarize the previous posterior predictive distribution plots in a single value (Gelman et al. 1995). To evaluate the level to which the distribution of predicted values aligns with the data, the Bayesian p-value relies on a test statistic or discrepency measure, which provides a scalar value that can be compared for observed data versus the posterior predictive distribution. The Bayesian p-value is then defined as the probability that the test statistic for the predicted data is more extreme than the test statistic for the observed data, given the observed data. Multiple test statistics can be used to evaluate model fit, and we use the Freeman-Tukey statistic recommended by Kéry and Royle (2016) and included within the Spocc package. Grouping or binning the binary data based on cells (group=1) or site-nights (group=2) provides two test statistics that may reveal different shortcomings of the model (i.e., problems of model fit in occurrence versus detection)

# -> A Bayesian p-value close to 0 or 1 implies the model does not accurately represent the distribution of the data (Hobbs and Hooten 2015). Note that the Bayesian p-value is a general goodness of fit statistic and won’t provide more information about where in the model potential lack of fit may be occurring (Warton et al. 2017). [if Bayesian p-values close to 0.5, indicates adequate model fit; values >0.1 indicate no lack of fit; values close to 1 require further investigation]

ppc.out.1 <- ppcOcc(out, fit.stat = 'freeman-tukey', group = 1)
summary(ppc.out.1)

ppc.out.2 <- ppcOcc(out, fit.stat = 'freeman-tukey', group = 2)
summary(ppc.out.2)


## Examine outputs ------------------------------------------------------------
summary(out)
plogis(2.13) #convert logit values to probability scale

# [IF PERFORMED] K-fold cross validation (want lower values):
#k.fold.non.sp$k.fold.deviance
#k.fold.sp$k.fold.deviance


## Model comparison -----------------------------------------------------------

#may want to try running with and without AR1=T vs F ?
#could try cov.model = "exponential" vs 'spherical'
#or could just be fine for now 


## Save files -------------------------------------------------------------------

(filename <- paste("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights/Model outputs/", target_species, "_", Sys.Date(), "/", target_species, "_", Sys.Date(), "_svcTPGOcc", ".RData", sep = ""))
save(yarray, covs, target_species, out, coords, file = filename)

## ------------------------------------------------------------------------------
