##########################
##### Running models ##### 
##########################

# Set working directory
setwd("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights")
rm(list=ls()); gc()
set.seed(123)

# Load libraries 
library(dplyr)
library(forcats)
library(lubridate)
library(purrr)
library(reshape2)
library(sf)
library(spOccupancy)
library(stringr)
library(terra)
library(tibble)
library(tidyr)
library(tidyverse) 
library(coda)
library(stars)
library(ggplot2)
library(gtools)

## User-defined functions -----------------------------------------------------

# Function to create initial values for covariates with NAs
prep_inits <- function(x = NULL){
  # initial conditions. 1 should be NA, NA should be a 0 or 1
  xinits <- x
  xinits[!is.na(x)] <- NA
  xinits[is.na(x)] <- sample(unique(c(x))[!is.na(unique(c(x)))], sum(is.na(x)), replace=TRUE)
  return(xinits)
}

# Scaling functions
scale2 <- function(x, na.rm = FALSE) ((x - mean(1:7))/sd(1:7))
scale3 <- function(x, na.rm = FALSE) ((x - mean(1:365))/sd(1:365))

## Reload data if saved -------------------------------------------------------

(filenames <- list.files("Data for modelling", full.names = TRUE))
filename <- filenames[1]

load(filename)
print(target_species)
print(nrow(ystack))
print(nrow(covs))

## Organize data and covariates -----------------------------------------------

# Remove cases without cell_ID
ystack <- cbind(ystack, covs %>% dplyr::select(cell_ID)) %>% drop_na(cell_ID)
covs <- covs %>% drop_na(cell_ID)

# Set dimensions
(ncell <- length(unique(covs$cell_ID)))
(nperiod <- length(seq(min(covs$period_counter), max(covs$period_counter), by = 1)))
nweek <- 13
(min_year <- min(covs$year))
(max_year <- max(covs$year))
cell_IDs <- unique(ystack$cell_ID)

periods <- data.frame(period_counter = seq(min(covs$period_counter), 
                                           max(covs$period_counter), by = 1)) %>% 
  left_join(covs %>% dplyr::select(period_counter, season, year) %>% distinct()) 

# fill gaps
for(i in 1:nrow(periods)){
  if(is.na(periods$season[i])){
    
    # fix NAs in season
    if(as.character(periods$season[i-1]) == "Spring") {
      periods$season[i] <-  "Summer"
    } else if(as.character(periods$season[i-1]) == "Summer") {
      periods$season[i] <-  "Fall"
    } else if(as.character(periods$season[i-1]) == "Fall") {
      periods$season[i] <-  "Winter"
    } else {
      periods$season[i] <-  "Spring"
    }
    
    # fix NAs in year
    if(as.character(periods$season[i-1]) == "Winter") {
      periods$year[i] = periods$year[i-1]+1
    } else {
      periods$year[i] = periods$year[i-1] 
    }
  }
}

## Observations ---------------------------------------------------------------
ystack2 <- cbind(ystack, covs %>% dplyr::select(cell_ID, period_counter)) %>% 
  pivot_longer(V1:V13, names_to = "week", values_to = "det", values_drop_na = TRUE) %>% 
  mutate(week = as.numeric(str_remove(week, pattern = "V")),
         period_counter = paste0("p", period_counter)) 
ystack2 <- ystack2[order(ystack2$cell_ID),]

yarray <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(yarray) <- unique(covs$cell_ID)
colnames(yarray) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))

for(i in 1:nrow(ystack2)){
  yarray[as.character(ystack2$cell_ID[i]),
         as.character(ystack2$period_counter[i]),
         ystack2$week[i]] <- ystack2$det[i]
}

# WHAT IS THIS? 
zobs <- apply(yarray, c(1,2), function(x) max(x, na.rm = TRUE))
table(zobs)
zobs <- na_if(zobs, -Inf)
(psi.obs <- apply(zobs, 2, sum, na.rm = TRUE) / apply(zobs, 2, function(x) sum(!is.na(x))))


## Coordinates ----------------------------------------------------------------
coords <- as.data.frame(rast("10x10 km spatial layers/raster_100km2_with_cellID.tif"), xy=T) %>% 
  filter(cell_ID %in% cell_IDs) %>% 
  arrange(cell_ID) %>% 
  select(-cell_ID) %>% 
  as.matrix()


## Covariates: spatial (dim: ncell) ------------------------------------------- 
csv_files <- list.files("Covariates for modelling", pattern = "\\.csv$", full.names = TRUE)

static <- read.csv("Covariates for modelling/site_level_covariates.csv") %>% 
  select(-c(elevation, cities_md, cities_sm)) %>% #from Data-Explore code 
  filter(cell_ID %in% cell_IDs) %>% 
  arrange(cell_ID)
nrow(static) #select out each column want as a vector 

## Covariates: primary occasion (dim: ncell x nperiods) -----------------------

# Formatting function 
cov_subset <- function(cov_dataframe_name, period_data){
  covariate <- read.csv(cov_dataframe_name) %>% 
    filter(cell_ID %in% cell_IDs) %>% 
    arrange(cell_ID) 
  keep_cols <- intersect(paste0("X", unique(period_data$period_counter)), names(covariate))
  df_subset <- covariate[, keep_cols] %>% 
    as.matrix()
  return(df_subset)
}

# Environmental ---
# retain: crop, gpw, gfc, precseas, wdpa, evi; from Data-Explore code 
crop <- cov_subset("Covariates for modelling/primary_occ_covariates_crop.csv", periods)
gpw <- cov_subset("Covariates for modelling/primary_occ_covariates_EVI.csv", periods) 
gfc <- cov_subset("Covariates for modelling/primary_occ_covariates_GFC.csv", periods)
precseas <- cov_subset("Covariates for modelling/primary_occ_covariates_precseas.csv", periods)
wdpa <- cov_subset("Covariates for modelling/primary_occ_covariates_wdpa.csv", periods)
evi <- cov_subset("Covariates for modelling/primary_occ_covariates_EVI.csv", periods)

# Number of deployments ---  
ndepl <- matrix(0, nrow = ncell, ncol = nperiod)
rownames(ndepl) <- unique(sort(covs$cell_ID))
colnames(ndepl) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))
for(i in 1:nrow(covs)){
  ndepl[as.character(covs$cell_ID[i]),
        as.character(paste0("p", covs$period_counter[i]))] <- covs$n_depl[i]
}

# Project IDs ---
proj <- matrix(0, nrow = ncell, ncol = nperiod)
rownames(proj) <- unique(sort(covs$cell_ID))
colnames(proj) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))
covs$proj_ft <- as.numeric(as.factor(covs$proj))
for(i in seq_len(nrow(covs))){
  r <- as.character(covs$cell_ID[i])
  c <- paste0("p", covs$period_counter[i])
  proj[r, c] <- covs$proj_ft[i]
}

# Season ---
Season <- data.frame(cell_ID = unique(covs$cell_ID)) %>%
  tidyr::crossing(periods[1:2]) %>% 
  mutate(season_num = as.factor(season),
         season_num = as.numeric(fct_relevel(season_num, "Winter", "Spring", "Summer", "Fall"))) %>% 
  pivot_wider(id_cols = cell_ID, names_from = period_counter, values_from = season_num) %>% 
  arrange(cell_ID) %>% 
  dplyr::select(-cell_ID) %>% 
  as.matrix()

# Year 
Year <- data.frame(cell_ID = unique(covs$cell_ID)) %>%
  tidyr::crossing(periods[, c(1,3)]) %>% 
  mutate(year = as.numeric(year)) %>% 
  pivot_wider(id_cols = cell_ID, names_from = period_counter, values_from = year) %>%
  arrange(cell_ID) %>% 
  dplyr::select(-cell_ID) %>% 
  as.matrix()

# Trend 
trend <- data.frame(cell_ID = unique(covs$cell_ID)) %>%
  tidyr::crossing(periods[1:2]) %>% 
  pivot_wider(id_cols = cell_ID, names_from = period_counter, values_from = period_counter) %>% 
  arrange(cell_ID) %>% 
  dplyr::select(-cell_ID) %>% 
  as.matrix() 


## Covariates: observation-level (dim: ncell x nperiod x nweek) ---------------

# Effort 
effort_temp <- covs %>% 
  dplyr::select(cell_ID, period_counter, effort_w1:effort_w13) %>% 
  mutate_at(vars(matches("effort_w")), scale2) %>%  ## WHY ARE WE RESCALING HERE 
  pivot_longer(effort_w1:effort_w13, names_to = "week", values_to = "effort", 
               values_drop_na = TRUE) %>% 
  mutate(week = as.numeric(str_remove(week, "effort_w")),
         period_counter = paste0("p", period_counter))

effort <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(effort) <- unique(sort(covs$cell_ID))
colnames(effort) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))

for(i in 1:nrow(effort_temp)){
  effort[as.character(effort_temp$cell_ID[i]),
         as.character(effort_temp$period_counter[i]),
         effort_temp$week[i]] <- effort_temp$effort[i]
}

str(effort) #this is the df to use 
hist(effort[,,3]) #data is pre-scaled? no need to scale further? 

# Minjul 
minjul_temp <- covs %>% 
  dplyr::select(cell_ID, period_counter, starts_with("minjul_w")) %>% 
  mutate_at(vars(matches("minjul_w")), scale3) %>% 
  pivot_longer(minjul_w1:minjul_w13, names_to = "week", values_to = "minjul", 
               values_drop_na = TRUE) %>% 
  mutate(week = as.numeric(str_remove(week, "minjul_w")),
         period_counter = paste0("p", period_counter))

minjul <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(minjul) <- unique(sort(covs$cell_ID))
colnames(minjul) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))

for(i in 1:nrow(minjul_temp)){
  minjul[as.character(minjul_temp$cell_ID[i]),
         as.character(minjul_temp$period_counter[i]),
         minjul_temp$week[i]] <- minjul_temp$minjul[i]
}

str(minjul) #this is the df to use 
hist(minjul[,,3]) #data is pre-scaled? no need to scale further 

# Minjul Squared ------
minjul_sq <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(minjul_sq) <- unique(covs$cell_ID)
colnames(minjul_sq) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))
for(i in 1:nrow(minjul_temp)){
  
  minjul_sq[as.character(minjul_temp$cell_ID[i]),
            as.character(minjul_temp$period_counter[i]),
            minjul_temp$week[i]] <- minjul_temp$minjul[i]^2
  
}

## Modelling ------------------------------------------------------------------

# Following vignette found at: 
# https://doserlab.com/files/spoccupancy-web/articles/svcmodels#introduction

# Look at raw occupancy data -------------------------------------------------

# how adequate is a linear trend for our analysis?
raw.occ.prob <- apply(yarray, 2, mean, na.rm = TRUE)
raw.occ.prob.df <- data.frame(x = names(raw.occ.prob), y = raw.occ.prob)
raw.occ.prob.df$x <- factor(raw.occ.prob.df$x, levels = mixedsort(unique(raw.occ.prob.df$x)))
ggplot(raw.occ.prob.df, aes(x, y)) + geom_point() + theme_bw() + 
  xlab("Year") + ylab("Raw Occurrence Proportion")

# Plot spatially varying trend across study region ---------------------------

**FIX THIS**

y <- yarray            #detection-nondetection data
X <- dat$X             #occurrence design matrix for fixed effects
X.p <- dat$X.p         #detection design matrix for fixed effets  
psi <- dat$psi         #occurrence values
coords <- coords       #spatial coordinates
w <- dat$w.            #spatially varying intercept and covariate effects

cov.effect <- beta[2] + w[, 2]
plot.dat <- data.frame(x = coords[, 1], 
                       y = coords[, 2], 
                       cov.effect = cov.effect)
ggplot(plot.dat, aes(x = x, y = y, fill = cov.effect)) + 
  geom_raster() + 
  scale_fill_gradient2(midpoint = 0, low = '#B2182B', mid = 'white', 
                       high = '#2166AC', na.value = NA) + 
  theme_bw() 
# (decrease in occurrence = red, increase in occurrence = blue)

# [IF TESTING] Subset data for testing ----------------------------------------
ncell = 200

static_test <- static[1:ncell,]
gpw_test <- gpw[1:ncell,]
evi_test <- evi[1:ncell,]
crop_test <- crop[1:ncell,]
gfc_test <- gfc[1:ncell,]
precseas_test <- precseas[1:ncell,]
wdpa_test <- wdpa[1:ncell,]

ndepl_test <- ndepl[1:ncell,]
proj_test <- proj[1:ncell,]
season_test <- Season[1:ncell,]
year_test <- Year[1:ncell,]
trend_test <- trend[1:ncell,]

yarray_test <- yarray[1:ncell,,]
effort_test <- effort[1:ncell,,]  
minjul_test <- minjul[1:ncell,,]  
minjul_sq_test <- minjul_sq[1:ncell,,]

# Prep inits -------------------------------------------------------------------
z.init <- apply(yarray_test, c(1, 2), function(a) as.numeric(sum(a, na.rm = TRUE) > 0))
inits.list <- list(beta = 0,            #occurrence coefficients
                   alpha = 0,           #detection coefficients
                   z = z.init,          #latent occurrence values
                   phi = 3 / .5, 
                   sigma.sq = 2, 
                   w = rep(0, ncell), 
                   rho = 0, 
                   sigma.sq.t = 0.5)    #occurrence random effect variances

CHECK THAT THIS IS NOT SOMETHING NEED FOR OUR MODEL? 
#revi.sp.inits <- list(beta = 0, alpha = 0, z = z.inits,
#                      sigma.sq = 1, phi = 3 / mean(dist.hbef), 
#                      sigma.sq.t = 1.5, rho = 0.2)

ADD THIS IN SOMEWHERE
dist.hbef <- dist(revi.data$coords)     #pair-wise distance between all sites 

DO WE HAVE PRIORS? WHAT IS APPROPRIATE FOR OUR MODEL? 
revi.sp.priors <- list(beta.normal = list(mean = 0, var = 2.72), 
                       alpha.normal = list(mean = 0, var = 2.72), 
                       sigma.sq.t.ig = c(2, 0.5), 
                       rho.unif = c(-1, 1),
                       sigma.sq.ig = c(2, 1), 
                       phi.unif = c(3 / max(dist.hbef), 3 / min(dist.hbef)))

#revi.priors <- list(beta.normal = list(mean = 0, var = 2.72), 
#                    alpha.normal = list(mean = 0, var = 2.72), 
#                    sigma.sq.psi.ig = list(a = 0.1, b = 0.1))


# Tuning ----------------------------------------------------------------------
tuning.list <- list(phi = 1, rho = 1)

if(testing){
  n.batch <- 10
  n.burn = 50
  n.thin = 1
  n.report = 10 
}else{
  n.batch <- 400
  n.burn = 2000
  n.thin = 10
  n.report = 40
}

batch.length <- 25
n.iter <- n.batch * batch.length

# Organize data for svcTPGOcc -------------------------------------------------

# if TEST 
dat_ls <- list(y = yarray_test,
               occ.covs = list(crop = crop_test, 
                               gpw = gpw_test,
                               gfc = gfc_test, 
                               precseas = precseas_test,
                               wdpa = wdpa_test,
                               evi = evi_test, 
                               trend = trend_test),
               det.covs = list(effort = effort_test, #already scaled! 
                               minjul = minjul_test, #already scaled! 
                               minjul_sq = minjul_sq_test, #already scaled! 
                               ndepl = ndepl_test, 
                               proj = proj_test),
               coords = coords[c(1:ncell),])

# if NOT TEST 
dat_ls <- list(y = yarray,
               occ.covs = list(crop = crop, 
                               gpw = gpw,
                               gfc = gfc, 
                               precseas = precseas,
                               wdpa = wdpa,
                               evi = evi, 
                               trend = trend),
               det.covs = list(effort = effort, #already scaled! 
                               minjul = minjul, #already scaled! 
                               minjul_sq = minjul_sq, #already scaled! 
                               ndepl = ndepl, 
                               proj = proj),
               coords = coords)

# run model --------------------------------------------------------------------

We finally package up the data into the data list format for multi-season occupancy models. As before, we will fit the model with 75% of the spatial locations and then predict at the remaining 25% of the locations. With multi-season occupancy models, the occurrence covariates can now vary across both space and time, and so we specify occ.covs as a list rather than as a matrix/data frame as we saw with svcPGOcc(). See the multi-season occupancy model vignette for additional details. (https://www.doserlab.com/files/spoccupancy-web/articles/spacetimemodelshtml)

# Subset data for prediction. 
# Split into fitting and prediction data set
pred.indx <- sample(1:J, round(J * .25), replace = FALSE)
y.fit <- y[-pred.indx, , ]
y.pred <- y[pred.indx, , ]
X.fit <- X[-pred.indx, , ]
X.pred <- X[pred.indx, , ]
X.p.fit <- X.p[-pred.indx, , , ]
X.p.pred <- X.p[pred.indx, , , ]
coords.fit <- coords[-pred.indx, ]
coords.pred <- coords[pred.indx, ]
psi.fit <- psi[-pred.indx, ]
psi.pred <- psi[pred.indx, ]
w.fit <- w[-pred.indx, ]
w.pred <- w[pred.indx, ]

# Package all data into a list
# Occurrence covariates
occ.covs <- list(trend = X.fit[, , 2])
# Detection covariates
det.covs <- list(det.cov.1 = X.p.fit[, , , 2])
# Package into a list for spOccupancy
data.list <- list(y = y.fit,
                  occ.covs = occ.covs,
                  det.covs = det.covs,
                  coords = coords.fit)
# Take a look at the data structure.
str(data.list)


svcTPGOcc(occ.formula, det.formula, data, inits, priors, 
          tuning, svc.cols = 1, cov.model = 'exponential', NNGP = TRUE, 
          n.neighbors = 15, search.type = 'cb', n.batch, 
          batch.length, accept.rate = 0.43, n.omp.threads = 1, 
          verbose = TRUE, ar1 = FALSE, n.report = 100, 
          n.burn = round(.10 * n.batch * batch.length), 
          n.thin = 1, n.chains = 1, k.fold, k.fold.threads = 1, 
          k.fold.seed = 100, k.fold.only = FALSE, ...)

#Next, we extract the full SVC values using the getSVCSamples() function and then compare the estimated values to those used to generate the model.
# Intercept ---------------------------------------------------------------
svc.samples <- getSVCSamples(out.svc.trend)
int.quants <- apply(svc.samples[["(Intercept)"]], 2, quantile, 
                    probs = c(0.025, 0.5, 0.975))
svc.true.fit <- beta + w.fit
plot(svc.true.fit[, 1], int.quants[2, ], pch = 19, 
     ylim = c(min(int.quants[1, ]), max(int.quants[3, ])),
     xlab = "True", ylab = "Fit", main = 'Intercept')
abline(0, 1)
arrows(svc.true.fit[, 1], int.quants[2, ], svc.true.fit[, 1], col = 'gray', 
       int.quants[1, ], length = 0.02, angle = 90)
arrows(svc.true.fit[, 1], int.quants[1, ], svc.true.fit[, 1], col = 'gray', 
       int.quants[3, ], length = 0.02, angle = 90)
points(svc.true.fit[, 1], int.quants[2, ], pch = 19)

# Trend -------------------------------------------------------------------
trend.quants <- apply(svc.samples[["trend"]], 2, quantile, 
                      probs = c(0.025, 0.5, 0.975))
plot(svc.true.fit[, 2], trend.quants[2, ], pch = 19, 
     ylim = c(min(trend.quants[1, ]), max(trend.quants[3, ])),
     xlab = "True", ylab = "Fit", main = 'Spatially-Varying Trends')
abline(0, 1)
arrows(svc.true.fit[, 2], trend.quants[2, ], svc.true.fit[, 2], col = 'gray', 
       trend.quants[1, ], length = 0.02, angle = 90)
arrows(svc.true.fit[, 2], trend.quants[1, ], svc.true.fit[, 2], col = 'gray', 
       trend.quants[3, ], length = 0.02, angle = 90)
points(svc.true.fit[, 2], trend.quants[2, ], pch = 19)


out <- svcTPGOcc(occ.formula = ~ trend,   #SCALE covariates 
                 det.formula = ~ effort + minjul + I(minjul^2) + (1|proj), 
                 svc.cols = c(1,2,3),
                 data = dat_ls, 
                 inits = inits.list, 
                 cov.model = "exponential", 
                 NNGP = TRUE, 
                 ar1 = TRUE,
                 n.neighbors = 5, 
                 search.type = 'cb', 
                 n.report = n.report, 
                 n.burn = n.burn,
                 n.thin = n.thin,
                 n.chains = 3,  
                 batch.length = batch.length, 
                 n.batch = n.batch,
                 verbose = TRUE)

summary(out)

#convert logit values to probability scale
plogis(2.13)

#evaluate convergence
ESS = effective sample size, can be used to assess convergence 
(how to evaluate whether indicates adequate mizing of MCMC chain?)
rhat < 1.1
plot(model, 'beta', density = FALSE) #occupancy convergence
plot(model, 'alpha', density = FALSE) #detection convergence


# Save files -------------------------------------------------------------------
filename <- paste("/gpfs/gibbs/pi/jetz/wildlife_insights/RWT/output/model_results/", Sys.Date(), "_svcTPGOcc_", species, ".RData", sep = "")
save(yarray, covs, species, out, coords, file = filename)
toc()


# data exploration -------------------------------------------------------------

# posterior predictive checks
ppc.out <- ppcOcc(out.svc.trend, fit.stat = 'freeman-tukey', group = 1)
summary(ppc.out)

# model comparison
priors.stPGOcc <- list(beta.normal = priors$beta.normal, 
                       alpha.normal = priors$alpha.normal, 
                       sigma.sq.t.ig = priors$sigma.sq.t.ig,
                       rho.unif = priors$rho.unif,
                       phi.unif = c(3 / 1, 3 / 0.1), 
                       sigma.sq.ig = c(2, 1))
out.stPGOcc <- stPGOcc(occ.formula = occ.formula,
                       det.formula = det.formula,
                       data = data,
                       inits = inits,
                       priors = priors.stPGOcc,
                       cov.model = cov.model,
                       n.neighbors = n.neighbors,
                       n.batch = n.batch,
                       batch.length = batch.length,
                       verbose = TRUE,
                       ar1 = ar1,
                       n.report = 200,
                       n.burn = n.burn,
                       n.thin = n.thin,
                       n.chains = 1)
# SVC multi-season occupancy model
waicOcc(out.svc.trend)
# Spatial multi-season occupancy model
waicOcc(out.stPGOcc)

## prediction
# Predict occupancy at the 400 new sites
out.pred <- predict(out.svc.trend, X.pred, coords.pred, t.cols = 1:n.time.max)

# Use the getSVCSamples() function to extract the SVC values
# at the prediction locations
svc.pred.samples <- getSVCSamples(out.svc.trend, pred.object = out.pred) 
# True covariate effect values at new locations
trend.pred <- beta[2] + w.pred[, 2]
# Get meian and 95% CIs of the SVC for the trendariate effect
trend.pred.quants <- apply(svc.pred.samples[["trend"]], 2, quantile, 
                           probs = c(0.025, 0.5, 0.975))
plot(trend.pred, trend.pred.quants[2, ], pch = 19, 
     ylim = c(min(trend.pred.quants[1, ]), max(trend.pred.quants[3, ])),
     xlab = "True", ylab = "Fit", main = 'Spatially varying trend')
abline(0, 1)
arrows(trend.pred, trend.pred.quants[2, ], trend.pred, col = 'gray', 
       trend.pred.quants[1, ], length = 0.02, angle = 90)
arrows(trend.pred, trend.pred.quants[1, ], trend.pred, col = 'gray', 
       trend.pred.quants[3, ], length = 0.02, angle = 90)
points(trend.pred, trend.pred.quants[2, ], pch = 19)





## Raw occcupancy ------
# raw.occ.prob <- apply(dat_ls$y, 2, mean, na.rm = TRUE)
# plot(seq(min(dat_ls$occ.covs$trend), max(dat_ls$occ.covs$trend), by = 1), raw.occ.prob, pch = 16, 
#      xlab = 'Year', ylab = 'Raw Occurrence Proportion', 
#      cex = 1.5, frame = FALSE, ylim = c(0, 1))
# 
# ## WAIC --------
# waicOcc(out)

## Goodness of Fit --------
# posterior predictive checks
# (group 2 - by replicate - instead of by site, for spatial models)
ppc.out <- ppcOcc(out, fit.stat = 'freeman-tukey', group = 2)
summary(ppc.out)

# traceplots
# plot(out$beta.samples, density = TRUE)
# plot(out$alpha.samples, density = TRUE)
# 
# ## Predict -------
# ?predict.stPGOcc()

REVISIT: 
  https://doserlab.com/files/spoccupancy-web/articles/modelfitting#prediction 


# Number of prediction sites.
J.pred <- nrow(hbefElev)
# Number of prediction years.
n.years.pred <- 2
# Number of predictors (including intercept)
p.occ <- ncol(out.ar1$beta.samples)
# Get covariates and standardize them using values used to fit the model
elev.pred <- (hbefElev$val - mean(revi.data$occ.covs$elev)) / sd(revi.data$occ.covs$elev)
year.pred <- matrix(rep((c(2010, 2018) - mean(revi.data$occ.covs$years)) / 
                          sd(revi.data$occ.covs$years), 
                        length(elev.pred)), J.pred, n.years.pred, byrow = TRUE)
# Create three-dimensional array
X.0 <- array(1, dim = c(J.pred, n.years.pred, p.occ))
# Fill in the array
# Years
X.0[, , 2] <- year.pred
# Elevation
X.0[, , 3] <- elev.pred
# Elevation^2
X.0[, , 4] <- elev.pred^2
# Check out the structure
str(X.0)

# Indicate which primary time periods (years) we are predicting for
t.cols <- c(1, 9)
# Approx. run time: < 30 sec
out.pred <- predict(out.ar1, X.0, t.cols = t.cols, ignore.RE = TRUE, type = 'occupancy')
# Check out the structure
str(out.pred)

#we plot the mean of REVI occurrence probability in 2009 and 2018 across the forest.
plot.dat <- data.frame(x = hbefElev$Easting, 
                       y = hbefElev$Northing, 
                       mean.2009.psi = apply(out.pred$psi.0.samples[, , 1], 2, mean), 
                       mean.2018.psi = apply(out.pred$psi.0.samples[, , 2], 2, mean), 
                       sd.2009.psi = apply(out.pred$psi.0.samples[, , 1], 2, sd), 
                       sd.2018.psi = apply(out.pred$psi.0.samples[, , 2], 2, sd), 
                       stringsAsFactors = FALSE)
# Make a species distribution map showing the point estimates,
# or predictions (posterior means)
dat.stars <- st_as_stars(plot.dat, dims = c('x', 'y'))
# 2009
ggplot() + 
  geom_stars(data = dat.stars, aes(x = x, y = y, fill = mean.2009.psi)) +
  scale_fill_viridis_c(na.value = 'transparent') +
  labs(x = 'Easting', y = 'Northing', fill = '', 
       title = 'Mean REVI occurrence probability 2009') +
  theme_bw()


## kfold cross validation
# Non-spatial (Approx. run time: ~ 2.5 min)
k.fold.non.sp <- tPGOcc(occ.formula = ~ scale(years) + scale(elev) + I(scale(elev)^2) + 
                          (1 | site.effect), 
                        det.formula = revi.det.formula, 
                        data = revi.data, 
                        n.batch = 200, 
                        batch.length = 25,
                        inits = revi.inits,
                        priors = revi.priors,
                        ar1 = TRUE,
                        verbose = FALSE,
                        n.burn = 2000, 
                        n.thin = 12, 
                        n.chains = n.chains, 
                        n.report = 50, 
                        k.fold = 4,
                        k.fold.threads = 4,
                        k.fold.only = TRUE)
# Spatial (Approx run time: ~ 2.5 min)
k.fold.sp <- stPGOcc(occ.formula = revi.sp.occ.formula, 
                     det.formula = revi.sp.det.formula, 
                     data = revi.data, 
                     inits = revi.sp.inits, 
                     priors = revi.sp.priors, 
                     cov.model = cov.model, 
                     n.neighbors = n.neighbors,
                     n.batch = n.batch, 
                     batch.length = batch.length, 
                     verbose = FALSE, 
                     ar1 = TRUE,
                     n.report = 50,
                     n.burn = n.burn, 
                     n.thin = n.thin, 
                     n.chains = 3, 
                     k.fold = 4, 
                     k.fold.threads = 4,
                     k.fold.only = TRUE) 
str(k.fold.sp)
k.fold.non.sp$k.fold.deviance
k.fold.sp$k.fold.deviance
# want lower values 
