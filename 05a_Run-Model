##########################
##### Running models ##### 
##########################

# Set working directory
setwd("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights")
rm(list=ls()); gc()
set.seed(123)

# Load libraries 
library(dplyr)
library(forcats)
library(lubridate)
library(purrr)
library(reshape2)
library(sf)
library(spOccupancy)
library(stringr)
library(terra)
library(tibble)
library(tidyr)
library(tidyverse) 
library(coda)
library(stars)
library(ggplot2)
library(gtools)

## User-defined functions -----------------------------------------------------

# Function to create initial values for covariates with NAs
prep_inits <- function(x = NULL){
  # initial conditions. 1 should be NA, NA should be a 0 or 1
  xinits <- x
  xinits[!is.na(x)] <- NA
  xinits[is.na(x)] <- sample(unique(c(x))[!is.na(unique(c(x)))], sum(is.na(x)), replace=TRUE)
  return(xinits)
}

# Scaling functions
scale2 <- function(x, na.rm = FALSE) ((x - mean(1:7))/sd(1:7))
scale3 <- function(x, na.rm = FALSE) ((x - mean(1:365))/sd(1:365))

## Reload data if saved -------------------------------------------------------

(filenames <- list.files("Data for modelling", full.names = TRUE))
filename <- filenames[1]

load(filename)
print(target_species)
print(nrow(ystack))
print(nrow(covs))

## Organize data and covariates -----------------------------------------------

# Remove cases without cell_ID
ystack <- cbind(ystack, covs %>% dplyr::select(cell_ID)) %>% drop_na(cell_ID)
covs <- covs %>% drop_na(cell_ID)

# Set dimensions
(ncell <- length(unique(covs$cell_ID)))
(nperiod <- length(seq(min(covs$period_counter), max(covs$period_counter), by = 1)))
nweek <- 13
(min_year <- min(covs$year))
(max_year <- max(covs$year))
cell_IDs <- unique(ystack$cell_ID)

periods <- data.frame(period_counter = seq(min(covs$period_counter), 
                                           max(covs$period_counter), by = 1)) %>% 
  left_join(covs %>% dplyr::select(period_counter, season, year) %>% distinct()) 

# fill gaps
for(i in 1:nrow(periods)){
  if(is.na(periods$season[i])){
    
    # fix NAs in season
    if(as.character(periods$season[i-1]) == "Spring") {
      periods$season[i] <-  "Summer"
    } else if(as.character(periods$season[i-1]) == "Summer") {
      periods$season[i] <-  "Fall"
    } else if(as.character(periods$season[i-1]) == "Fall") {
      periods$season[i] <-  "Winter"
    } else {
      periods$season[i] <-  "Spring"
    }
    
    # fix NAs in year
    if(as.character(periods$season[i-1]) == "Winter") {
      periods$year[i] = periods$year[i-1]+1
    } else {
      periods$year[i] = periods$year[i-1] 
    }
  }
}

## Observations ---------------------------------------------------------------
ystack2 <- cbind(ystack, covs %>% dplyr::select(cell_ID, period_counter)) %>% 
  pivot_longer(V1:V13, names_to = "week", values_to = "det", values_drop_na = TRUE) %>% 
  mutate(week = as.numeric(str_remove(week, pattern = "V")),
         period_counter = paste0("p", period_counter)) 
ystack2 <- ystack2[order(ystack2$cell_ID),]

yarray <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(yarray) <- unique(covs$cell_ID)
colnames(yarray) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))

for(i in 1:nrow(ystack2)){
  yarray[as.character(ystack2$cell_ID[i]),
         as.character(ystack2$period_counter[i]),
         ystack2$week[i]] <- ystack2$det[i]
}

# WHAT IS THIS? 
zobs <- apply(yarray, c(1,2), function(x) max(x, na.rm = TRUE))
table(zobs)
zobs <- na_if(zobs, -Inf)
(psi.obs <- apply(zobs, 2, sum, na.rm = TRUE) / apply(zobs, 2, function(x) sum(!is.na(x))))


## Coordinates ----------------------------------------------------------------
coords <- as.data.frame(rast("10x10 km spatial layers/raster_100km2_with_cellID.tif"), xy=T) %>% 
  filter(cell_ID %in% cell_IDs) %>% 
  arrange(cell_ID) %>% 
  select(-cell_ID) %>% 
  as.matrix()


## Covariates: spatial (dim: ncell) ------------------------------------------- 
csv_files <- list.files("Covariates for modelling", pattern = "\\.csv$", full.names = TRUE)

static <- read.csv("Covariates for modelling/site_level_covariates.csv") %>% 
  select(-c(elevation, cities_md, cities_sm)) %>% #from Data-Explore code 
  filter(cell_ID %in% cell_IDs) %>% 
  arrange(cell_ID)
nrow(static) #select out each column want as a vector 

## Covariates: primary occasion (dim: ncell x nperiods) -----------------------

# Formatting function 
cov_subset <- function(cov_dataframe_name, period_data){
  covariate <- read.csv(cov_dataframe_name) %>% 
    filter(cell_ID %in% cell_IDs) %>% 
    arrange(cell_ID) 
  keep_cols <- intersect(paste0("X", unique(period_data$period_counter)), names(covariate))
  df_subset <- covariate[, keep_cols] %>% 
    as.matrix()
  return(df_subset)
}

# Environmental ---
# retain: crop, gpw, gfc, precseas, wdpa, evi; from Data-Explore code 
crop <- cov_subset("Covariates for modelling/primary_occ_covariates_crop.csv", periods)
gpw <- cov_subset("Covariates for modelling/primary_occ_covariates_EVI.csv", periods) 
gfc <- cov_subset("Covariates for modelling/primary_occ_covariates_GFC.csv", periods)
precseas <- cov_subset("Covariates for modelling/primary_occ_covariates_precseas.csv", periods)
wdpa <- cov_subset("Covariates for modelling/primary_occ_covariates_wdpa.csv", periods)
evi <- cov_subset("Covariates for modelling/primary_occ_covariates_EVI.csv", periods)

# Number of deployments ---  
ndepl <- matrix(0, nrow = ncell, ncol = nperiod)
rownames(ndepl) <- unique(sort(covs$cell_ID))
colnames(ndepl) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))
for(i in 1:nrow(covs)){
  ndepl[as.character(covs$cell_ID[i]),
        as.character(paste0("p", covs$period_counter[i]))] <- covs$n_depl[i]
}

# Project IDs ---
proj <- matrix(0, nrow = ncell, ncol = nperiod)
rownames(proj) <- unique(sort(covs$cell_ID))
colnames(proj) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))
covs$proj_ft <- as.numeric(as.factor(covs$proj))
for(i in seq_len(nrow(covs))){
  r <- as.character(covs$cell_ID[i])
  c <- paste0("p", covs$period_counter[i])
  proj[r, c] <- covs$proj_ft[i]
}

# Season ---
Season <- data.frame(cell_ID = unique(covs$cell_ID)) %>%
  tidyr::crossing(periods[1:2]) %>% 
  mutate(season_num = as.factor(season),
         season_num = as.numeric(fct_relevel(season_num, "Winter", "Spring", "Summer", "Fall"))) %>% 
  pivot_wider(id_cols = cell_ID, names_from = period_counter, values_from = season_num) %>% 
  arrange(cell_ID) %>% 
  dplyr::select(-cell_ID) %>% 
  as.matrix()

# Year 
Year <- data.frame(cell_ID = unique(covs$cell_ID)) %>%
  tidyr::crossing(periods[, c(1,3)]) %>% 
  mutate(year = as.numeric(year)) %>% 
  pivot_wider(id_cols = cell_ID, names_from = period_counter, values_from = year) %>%
  arrange(cell_ID) %>% 
  dplyr::select(-cell_ID) %>% 
  as.matrix()

# Trend 
trend <- data.frame(cell_ID = unique(covs$cell_ID)) %>%
  tidyr::crossing(periods[1:2]) %>% 
  pivot_wider(id_cols = cell_ID, names_from = period_counter, values_from = period_counter) %>% 
  arrange(cell_ID) %>% 
  dplyr::select(-cell_ID) %>% 
  as.matrix() 


## Covariates: observation-level (dim: ncell x nperiod x nweek) ---------------

# Effort 
effort_temp <- covs %>% 
  dplyr::select(cell_ID, period_counter, effort_w1:effort_w13) %>% 
  mutate_at(vars(matches("effort_w")), scale2) %>%  ## WHY ARE WE RESCALING HERE 
  pivot_longer(effort_w1:effort_w13, names_to = "week", values_to = "effort", 
               values_drop_na = TRUE) %>% 
  mutate(week = as.numeric(str_remove(week, "effort_w")),
         period_counter = paste0("p", period_counter))

effort <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(effort) <- unique(sort(covs$cell_ID))
colnames(effort) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))

for(i in 1:nrow(effort_temp)){
  effort[as.character(effort_temp$cell_ID[i]),
         as.character(effort_temp$period_counter[i]),
         effort_temp$week[i]] <- effort_temp$effort[i]
}

str(effort) #this is the df to use 
hist(effort[,,3]) #data is pre-scaled? no need to scale further? 

# Minjul 
minjul_temp <- covs %>% 
  dplyr::select(cell_ID, period_counter, starts_with("minjul_w")) %>% 
  mutate_at(vars(matches("minjul_w")), scale3) %>% 
  pivot_longer(minjul_w1:minjul_w13, names_to = "week", values_to = "minjul", 
               values_drop_na = TRUE) %>% 
  mutate(week = as.numeric(str_remove(week, "minjul_w")),
         period_counter = paste0("p", period_counter))

minjul <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(minjul) <- unique(sort(covs$cell_ID))
colnames(minjul) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))

for(i in 1:nrow(minjul_temp)){
  minjul[as.character(minjul_temp$cell_ID[i]),
         as.character(minjul_temp$period_counter[i]),
         minjul_temp$week[i]] <- minjul_temp$minjul[i]
}

str(minjul) #this is the df to use 
hist(minjul[,,3]) #data is pre-scaled? no need to scale further 

# Minjul Squared ------
minjul_sq <- array(NA, dim = c(ncell, nperiod, nweek))
rownames(minjul_sq) <- unique(covs$cell_ID)
colnames(minjul_sq) <- paste0("p", seq(min(covs$period_counter), max(covs$period_counter), by = 1))
for(i in 1:nrow(minjul_temp)){
  
  minjul_sq[as.character(minjul_temp$cell_ID[i]),
            as.character(minjul_temp$period_counter[i]),
            minjul_temp$week[i]] <- minjul_temp$minjul[i]^2
  
}

## Modelling ------------------------------------------------------------------

# Following vignette found at: 
# https://doserlab.com/files/spoccupancy-web/articles/svcmodels#introduction

# Look at raw occupancy data -------------------------------------------------

# how adequate is a linear trend for our analysis?
raw.occ.prob <- apply(yarray, 2, mean, na.rm = TRUE)
raw.occ.prob.df <- data.frame(x = names(raw.occ.prob), y = raw.occ.prob)
raw.occ.prob.df$x <- factor(raw.occ.prob.df$x, levels = mixedsort(unique(raw.occ.prob.df$x)))
ggplot(raw.occ.prob.df, aes(x, y)) + geom_point() + theme_bw() + 
  xlab("Year") + ylab("Raw Occurrence Proportion")

# Plot spatially varying trend across study region ---------------------------

**FIX THIS**
  
y <- yarray            #detection-nondetection data
X <- dat$X             #occurrence design matrix for fixed effects
X.p <- dat$X.p         #detection design matrix for fixed effets  
psi <- dat$psi         #occurrence values
coords <- coords       #spatial coordinates
w <- dat$w.            #spatially varying intercept and covariate effects

cov.effect <- beta[2] + w[, 2]
plot.dat <- data.frame(x = coords[, 1], 
                       y = coords[, 2], 
                       cov.effect = cov.effect)
ggplot(plot.dat, aes(x = x, y = y, fill = cov.effect)) + 
  geom_raster() + 
  scale_fill_gradient2(midpoint = 0, low = '#B2182B', mid = 'white', 
                       high = '#2166AC', na.value = NA) + 
  theme_bw() 
# (decrease in occurrence = red, increase in occurrence = blue)

# [IF TESTING] Subset data for testing ----------------------------------------
ncell = 200

yarray_test <- yarray[1:ncell,,]
coords_test <- coords[1:ncell,]

static_test <- static[1:ncell,]
gpw_test <- gpw[1:ncell,]
evi_test <- evi[1:ncell,]
crop_test <- crop[1:ncell,]
gfc_test <- gfc[1:ncell,]
precseas_test <- precseas[1:ncell,]
wdpa_test <- wdpa[1:ncell,]

ndepl_test <- ndepl[1:ncell,]
proj_test <- proj[1:ncell,]
season_test <- Season[1:ncell,]
year_test <- Year[1:ncell,]
trend_test <- trend[1:ncell,]
effort_test <- effort[1:ncell,,]  
minjul_test <- minjul[1:ncell,,]  
minjul_sq_test <- minjul_sq[1:ncell,,]


# [IF SPLITTING] Split into fitting and prediction data sets ------------------
# Fit model with 75% of locations to train, predict at remaining 25% of locations 
J <- length(unique(covs$cell_ID)) #1102 sites
pred.indx <- sample(1:J, round(J * .25), replace = FALSE)

y.fit <- yarray[-pred.indx, , ]
y.pred <- yarray[pred.indx, , ]
coords.fit <- coords[-pred.indx, ]
coords.pred <- coords[pred.indx, ]

static.fit <- static[-pred.indx,]
static.pred <- static[pred.indx,]
gpw.fit <- gpw[-pred.indx,]
gpw.pred <- gpw[pred.indx,]
evi.fit <- evi[-pred.indx,]
evi.pred <- evi[pred.indx,]
crop.fit <- crop[-pred.indx,]
crop.pred <- crop[pred.indx,]
gfc.fit <- gfc[-pred.indx,]
gfc.pred <- gfc[pred.indx,]
precseas.fit <- precseas[-pred.indx,]
precseas.pred <- precseas[pred.indx,]
wdpa.fit <- wdpa[-pred.indx,]
wdpa.pred <- wdpa[pred.indx,]
ndepl.fit <- ndepl[-pred.indx,]
ndepl.pred <- ndepl[pred.indx,]
proj.fit <- proj[-pred.indx,]
proj.pred <- proj[pred.indx,]
season.fit <- Season[-pred.indx,]
season.pred <- Season[pred.indx,]
year.fit <- Year[-pred.indx,]
year.pred <- Year[pred.indx,]
trend.fit <- trend[-pred.indx,]
trend.pred <- trend[pred.indx,]
effort.fit <- effort[-pred.indx,,] 
effort.pred <- effort[pred.indx,,] 
minjul.fit <- minjul[-pred.indx,,]  
minjul.pred <- minjul[pred.indx,,]  
minjul_sq.fit <- minjul_sq[-pred.indx,,]
minjul_sq.pred <- minjul_sq[pred.indx,,]


# Prep inits -------------------------------------------------------------------

# UPDATE yarray as yarray, yarray_test, or y.fit; coords, etc. 

z.init <- apply(yarray_test, c(1, 2), function(a) as.numeric(sum(a, na.rm = TRUE) > 0))
#z <- apply(data.list$y, 1, max, na.rm = TRUE)  if what is in tutorial... 

dist.data <- dist(coords_test)          #pairwise distance between all sites 
inits.list <- list(beta = 0,            #occurrence coefficients
                   alpha = 0,           #detection coefficients
                   z = z.init,          #latent occurrence values
                   phi = 3 / mean(dist.data), #instead of mean, fabiola has 0.5... 
                   sigma.sq = 0.5, #fabiola had as 2... 
                   w = matrix(0, length(svc.cols), nrow(yarray_test))),  #fabiola had rep(0, ncell) 
                   rho = 0, #not in tutorial? 
                   sigma.sq.t = 0.5)    #occurrence random effect variances (not in tutorial?)

# Note: don't have to specify priors, can use svcPGOcc() defaults 
priors.list <- list(alpha.normal = list(mean = 0, var = 2.72), 
                    beta.normal = list(mean = 0, var = 2.72),
                    sigma.sq.ig = list(a = 2, b = 0.5),
                    phi.unif = list(a = 3 / max(dist.data), 
                                    b = 3 / min(dist.data)))


# Tuning ----------------------------------------------------------------------
n.chains <- 3
tuning.list <- list(phi = 0.2) #fabiola has tuning.list <- list(phi = 1, rho = 1)

if(testing){
  n.batch <- 10
  n.burn = 50
  n.thin = 1
  n.report = 10 
}else{
  n.batch <- 400 #tutorial has 800
  n.burn = 2000 #tutorial has 10000
  n.thin = 10 
  n.report = 40 #tutorial has 100
}

batch.length <- 25
n.iter <- n.batch * batch.length #not in tutorial? 


# Organize data for svcTPGOcc -------------------------------------------------

# if TEST 
dat_ls_test <- list(y = yarray_test,
               occ.covs = list(crop = crop_test, 
                               gpw = gpw_test,
                               gfc = gfc_test, 
                               precseas = precseas_test,
                               wdpa = wdpa_test,
                               evi = evi_test, 
                               road.dist = static_test$road_dist,
                               tri = static_test$tri,
                               river_dist = static_test$river_dist,
                               coeff_var = static_test$coeff_var,
                               cities_lg = static_test$cities_lg,
                               trend = trend_test),
               det.covs = list(effort = effort_test, #already scaled! 
                               minjul = minjul_test, #already scaled! 
                               minjul_sq = minjul_sq_test, #already scaled! 
                               ndepl = ndepl_test, 
                               proj = proj_test),
               coords = coords_test,])

# if SPLITTNG 
dat_ls_split <- list(y = y.fit,
                     occ.covs = list(crop = crop.fit, 
                                     gpw = gpw.fit,
                                     gfc = gfc.fit, 
                                     precseas = precseas.fit,
                                     wdpa = wdpa.fit,
                                     evi = evi.fit,
                                     road.dist = static.fit$road_dist,
                                     tri = static.fit$tri,
                                     river_dist = static.fit$river_dist,
                                     coeff_var = static.fit$coeff_var,
                                     cities_lg = static.fit$cities_lg,
                                     trend = trend.fit),
                     det.covs = list(effort = effort.fit, #already scaled! 
                                     minjul = minjul.fit, #already scaled! 
                                     minjul_sq = minjul_sq.fit, #already scaled! 
                                     ndepl = ndepl.fit, 
                                     proj = proj.fit),
                     coords = coords.fit,])

# if NOT TEST NOR SPLITTING 
dat_ls <- list(y = yarray,
               occ.covs = list(crop = crop, 
                               gpw = gpw,
                               gfc = gfc, 
                               precseas = precseas,
                               wdpa = wdpa,
                               evi = evi, 
                               road.dist = static$road_dist,
                               tri = static$tri,
                               river_dist = static$river_dist,
                               coeff_var = static$coeff_var,
                               cities_lg = static$cities_lg,
                               trend = trend),
               det.covs = list(effort = effort, #already scaled! 
                               minjul = minjul, #already scaled! 
                               minjul_sq = minjul_sq, #already scaled! 
                               ndepl = ndepl, 
                               proj = proj),
               coords = coords)

## Formulas -------------------------------------------------------------------

# [IF TESTING]
occ.formula = ~ 
det.formula = ~ effort + minjul + I(minjul^2) + (1|proj)
  
# [IF SPLITTING]
  occ.formula = ~ trend 
det.formula = ~ effort + minjul + I(minjul^2) + (1|proj)

# [REGULAR]
occ.formula = ~ trend 
det.formula = ~ effort + minjul + I(minjul^2) + (1|proj)
  SCALE COVARIATES 

svc.cols = c(1,2,3), #update with NAMES (see below)
#if so, need to include c('(Intercept)', 'occ.cov.1') INTERCEPT  
The svc.cols argument is used to specify the covariates whose effects are estimated as SVCs. svc.cols can either be a numeric indexing vector with integer numbers corresponding to the order in which you specified covariates in the occ.formula argument, or can be a character vector with the names of the covariates specified in occ.formula.  

## Run model ------------------------------------------------------------------

out <- svcTPGOcc(occ.formula = occ.formula, 
                 det.formula = det.formula,
                 data = dat_ls, #CHANGE FOR EACH training, split, regular 
                 inits = inits.list,
                 n.batch = n.batch,
                 batch.length = batch.length, 
                 priors = priors.list, #in tutorial, not in fabiola's code
                 svc.cols = svc.cols,
                 cov.model = "exponential",  #alternative to test could be 'spherical'
                 NNGP = TRUE, 
                 n.neighbors = 5,  
                 tuning = tuning.list, #not in fabiola's code...         
                 n.report = n.report, 
                 n.burn = n.burn,
                 n.thin = n.thin,
                 n.chains = nchains,
                 #search.type = 'cb', #in fabiola's code, not in tutorial? 
                 verbose = TRUE,
                 ar1 = TRUE) 
# would add k-fold parameters to this model if using k-fold cross validation ... 

summary(out)
plogis(2.13) #convert logit values to probability scale


#To extract the estimates of the spatially varying coefficients at each of the spatial locations in the data set used to fit the model, we need to combine the non-spatial component of the coefficient (contained in out.svc$beta.samples) and the spatial component of the coefficient (contained in out.svc$w.samples). Recall that in an SVC occupancy model, the total effect of a covariate at any given location is the sum of the non-spatial effect and the adjustment of the effect at that specific location. We provide the function getSVCSamples() to extract the SVCs at each location.

svc.samples <- getSVCSamples(out.svc) 
str(svc.samples)

#The resulting object, here called svc.samples, is a list with each component corresponding to a matrix of the MCMC samples of each spatially varying coefficient estimated in the model, with rows corresponding to MCMC sample and column corresponding to site.


#Next, we extract the full SVC values using the getSVCSamples() function and then compare the estimated values to those used to generate the model.

# Intercept ---------------------------------------------------------------
svc.samples <- getSVCSamples(out.svc.trend)
int.quants <- apply(svc.samples[["(Intercept)"]], 2, quantile, 
                    probs = c(0.025, 0.5, 0.975))
svc.true.fit <- beta + w.fit
plot(svc.true.fit[, 1], int.quants[2, ], pch = 19, 
     ylim = c(min(int.quants[1, ]), max(int.quants[3, ])),
     xlab = "True", ylab = "Fit", main = 'Intercept')
abline(0, 1)
arrows(svc.true.fit[, 1], int.quants[2, ], svc.true.fit[, 1], col = 'gray', 
       int.quants[1, ], length = 0.02, angle = 90)
arrows(svc.true.fit[, 1], int.quants[1, ], svc.true.fit[, 1], col = 'gray', 
       int.quants[3, ], length = 0.02, angle = 90)
points(svc.true.fit[, 1], int.quants[2, ], pch = 19)

# Trend -------------------------------------------------------------------
trend.quants <- apply(svc.samples[["trend"]], 2, quantile, 
                      probs = c(0.025, 0.5, 0.975))
plot(svc.true.fit[, 2], trend.quants[2, ], pch = 19, 
     ylim = c(min(trend.quants[1, ]), max(trend.quants[3, ])),
     xlab = "True", ylab = "Fit", main = 'Spatially-Varying Trends')
abline(0, 1)
arrows(svc.true.fit[, 2], trend.quants[2, ], svc.true.fit[, 2], col = 'gray', 
       trend.quants[1, ], length = 0.02, angle = 90)
arrows(svc.true.fit[, 2], trend.quants[1, ], svc.true.fit[, 2], col = 'gray', 
       trend.quants[3, ], length = 0.02, angle = 90)
points(svc.true.fit[, 2], trend.quants[2, ], pch = 19)



#evaluate convergence
ESS = effective sample size, can be used to assess convergence 
(how to evaluate whether indicates adequate mizing of MCMC chain?)
rhat < 1.1
plot(model, 'beta', density = FALSE) #occupancy convergence
plot(model, 'alpha', density = FALSE) #detection convergence


# Save files -------------------------------------------------------------------
filename <- paste("/gpfs/gibbs/pi/jetz/wildlife_insights/RWT/output/model_results/", Sys.Date(), "_svcTPGOcc_", species, ".RData", sep = "")
save(yarray, covs, species, out, coords, file = filename)
toc()


# data exploration -------------------------------------------------------------

# posterior predictive checks
ppc.out <- ppcOcc(out.svc.trend, fit.stat = 'freeman-tukey', group = 1)
summary(ppc.out)

# model comparison
priors.stPGOcc <- list(beta.normal = priors$beta.normal, 
                       alpha.normal = priors$alpha.normal, 
                       sigma.sq.t.ig = priors$sigma.sq.t.ig,
                       rho.unif = priors$rho.unif,
                       phi.unif = c(3 / 1, 3 / 0.1), 
                       sigma.sq.ig = c(2, 1))
out.stPGOcc <- stPGOcc(occ.formula = occ.formula,
                       det.formula = det.formula,
                       data = data,
                       inits = inits,
                       priors = priors.stPGOcc,
                       cov.model = cov.model,
                       n.neighbors = n.neighbors,
                       n.batch = n.batch,
                       batch.length = batch.length,
                       verbose = TRUE,
                       ar1 = ar1,
                       n.report = 200,
                       n.burn = n.burn,
                       n.thin = n.thin,
                       n.chains = 1)
# SVC multi-season occupancy model
waicOcc(out.svc.trend)
# Spatial multi-season occupancy model
waicOcc(out.stPGOcc)

## prediction
# Predict occupancy at the 400 new sites
out.pred <- predict(out.svc.trend, X.pred, coords.pred, t.cols = 1:n.time.max)

# Use the getSVCSamples() function to extract the SVC values
# at the prediction locations
svc.pred.samples <- getSVCSamples(out.svc.trend, pred.object = out.pred) 
# True covariate effect values at new locations
trend.pred <- beta[2] + w.pred[, 2]
# Get meian and 95% CIs of the SVC for the trendariate effect
trend.pred.quants <- apply(svc.pred.samples[["trend"]], 2, quantile, 
                           probs = c(0.025, 0.5, 0.975))
plot(trend.pred, trend.pred.quants[2, ], pch = 19, 
     ylim = c(min(trend.pred.quants[1, ]), max(trend.pred.quants[3, ])),
     xlab = "True", ylab = "Fit", main = 'Spatially varying trend')
abline(0, 1)
arrows(trend.pred, trend.pred.quants[2, ], trend.pred, col = 'gray', 
       trend.pred.quants[1, ], length = 0.02, angle = 90)
arrows(trend.pred, trend.pred.quants[1, ], trend.pred, col = 'gray', 
       trend.pred.quants[3, ], length = 0.02, angle = 90)
points(trend.pred, trend.pred.quants[2, ], pch = 19)





## Raw occcupancy ------
# raw.occ.prob <- apply(dat_ls$y, 2, mean, na.rm = TRUE)
# plot(seq(min(dat_ls$occ.covs$trend), max(dat_ls$occ.covs$trend), by = 1), raw.occ.prob, pch = 16, 
#      xlab = 'Year', ylab = 'Raw Occurrence Proportion', 
#      cex = 1.5, frame = FALSE, ylim = c(0, 1))
# 
# ## WAIC --------
# waicOcc(out)

## Goodness of Fit --------
# posterior predictive checks
# (group 2 - by replicate - instead of by site, for spatial models)
ppc.out <- ppcOcc(out, fit.stat = 'freeman-tukey', group = 2)
summary(ppc.out)

# traceplots
# plot(out$beta.samples, density = TRUE)
# plot(out$alpha.samples, density = TRUE)
# 
# ## Predict -------
# ?predict.stPGOcc()

REVISIT: 
  https://doserlab.com/files/spoccupancy-web/articles/modelfitting#prediction 


# Number of prediction sites.
J.pred <- nrow(hbefElev)
# Number of prediction years.
n.years.pred <- 2
# Number of predictors (including intercept)
p.occ <- ncol(out.ar1$beta.samples)
# Get covariates and standardize them using values used to fit the model
elev.pred <- (hbefElev$val - mean(revi.data$occ.covs$elev)) / sd(revi.data$occ.covs$elev)
year.pred <- matrix(rep((c(2010, 2018) - mean(revi.data$occ.covs$years)) / 
                          sd(revi.data$occ.covs$years), 
                        length(elev.pred)), J.pred, n.years.pred, byrow = TRUE)
# Create three-dimensional array
X.0 <- array(1, dim = c(J.pred, n.years.pred, p.occ))
# Fill in the array
# Years
X.0[, , 2] <- year.pred
# Elevation
X.0[, , 3] <- elev.pred
# Elevation^2
X.0[, , 4] <- elev.pred^2
# Check out the structure
str(X.0)

# Indicate which primary time periods (years) we are predicting for
t.cols <- c(1, 9)
# Approx. run time: < 30 sec
out.pred <- predict(out.ar1, X.0, t.cols = t.cols, ignore.RE = TRUE, type = 'occupancy')
# Check out the structure
str(out.pred)

#we plot the mean of REVI occurrence probability in 2009 and 2018 across the forest.
plot.dat <- data.frame(x = hbefElev$Easting, 
                       y = hbefElev$Northing, 
                       mean.2009.psi = apply(out.pred$psi.0.samples[, , 1], 2, mean), 
                       mean.2018.psi = apply(out.pred$psi.0.samples[, , 2], 2, mean), 
                       sd.2009.psi = apply(out.pred$psi.0.samples[, , 1], 2, sd), 
                       sd.2018.psi = apply(out.pred$psi.0.samples[, , 2], 2, sd), 
                       stringsAsFactors = FALSE)
# Make a species distribution map showing the point estimates,
# or predictions (posterior means)
dat.stars <- st_as_stars(plot.dat, dims = c('x', 'y'))
# 2009
ggplot() + 
  geom_stars(data = dat.stars, aes(x = x, y = y, fill = mean.2009.psi)) +
  scale_fill_viridis_c(na.value = 'transparent') +
  labs(x = 'Easting', y = 'Northing', fill = '', 
       title = 'Mean REVI occurrence probability 2009') +
  theme_bw()


## kfold cross validation
# Non-spatial (Approx. run time: ~ 2.5 min)
k.fold.non.sp <- tPGOcc(occ.formula = ~ scale(years) + scale(elev) + I(scale(elev)^2) + 
                          (1 | site.effect), 
                        det.formula = revi.det.formula, 
                        data = revi.data, 
                        n.batch = 200, 
                        batch.length = 25,
                        inits = revi.inits,
                        priors = revi.priors,
                        ar1 = TRUE,
                        verbose = FALSE,
                        n.burn = 2000, 
                        n.thin = 12, 
                        n.chains = n.chains, 
                        n.report = 50, 
                        k.fold = 4,
                        k.fold.threads = 4,
                        k.fold.only = TRUE)
# Spatial (Approx run time: ~ 2.5 min)
k.fold.sp <- stPGOcc(occ.formula = revi.sp.occ.formula, 
                     det.formula = revi.sp.det.formula, 
                     data = revi.data, 
                     inits = revi.sp.inits, 
                     priors = revi.sp.priors, 
                     cov.model = cov.model, 
                     n.neighbors = n.neighbors,
                     n.batch = n.batch, 
                     batch.length = batch.length, 
                     verbose = FALSE, 
                     ar1 = TRUE,
                     n.report = 50,
                     n.burn = n.burn, 
                     n.thin = n.thin, 
                     n.chains = 3, 
                     k.fold = 4, 
                     k.fold.threads = 4,
                     k.fold.only = TRUE) 
str(k.fold.sp)
k.fold.non.sp$k.fold.deviance
k.fold.sp$k.fold.deviance
# want lower values 
