#################### Covariate download and pre-processing #####################

# Some data downloaded manually, others through following code
# Code also contains scripts for aggregating and summarizing values at 10x10km2 scale 

# Run ONE TIME 

################################################################################
### Bulk Download EVI ##########################################################
################################################################################

# Downloads EVI files by continent from EOSDIS 

# Set workspace
rm(list=ls())
setwd("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights")

# Load libraries 
library(modisfast)
library(sf)
library(terra)
library(spData)
library(ggplot2)

# Grab countries 
data(world)
countries <- world$name_long

# Select data of interest -----------------------------------------------------

# Login to EOSDIS Earthdata with your username and password --> UPDATE WITH YOUR OWN 
log <- mf_login(credentials = c("USERNAME", "PASSWORD"))

# Define MODIS collections and variables (bands) of interest
collection <- "MOD13A3.061" # run mf_list_collections() for an exhaustive list of collections available
variables <- c("_1_km_monthly_EVI") # run mf_list_variables("MOD13A3.061") for an exhaustive list of variables available for the collection "MOD13A3.061"

# Run for each country for each year ------------------------------------------

# Manually set (easier than looping)
i <-1 

# Set ROI of interest
(where <- countries[i])
country <- world[world$name_long == where,]
roi <- st_as_sf(data.frame(id = where, geom = st_as_sfc(st_bbox(country))))
ggplot() + geom_sf(data=roi) + geom_sf(data=country) #check


# Loop through years 
for(year in seq(2001,2024)){ 
  
  # set time frame of interest 
  (start.date <- paste0(year,"-01-01"))
  (end.date <- paste0(year,"-12-31"))
  (time_range <- as.Date(c(start.date, end.date)))
  
  # get the URLs of the data
  urls <- mf_get_url(
    collection = collection,
    variables = variables,
    roi = roi,
    time_range = time_range)
  
  # download the data
  res_dl <- mf_download_data(urls, parallel = TRUE)
  
  # transform to raster 
  r <- mf_import_data(
    path = dirname(res_dl$destfile[1]),
    collection = collection,
    proj_epsg = 4326
  )
  
  # check and save 
  terra::plot(r, col = rev(terrain.colors(20)))
  (filename <- paste0(where,"_",year,".tif"))
  writeRaster(r, filename)
}


################################################################################
### Aggregate 1km2 data to 100km2 ##############################################
################################################################################

# Downloads EVI files by continent from EOSDIS 

# Set workspace 
rm(list=ls())
setwd("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights")

# Load libraries 
library(terra)
library(stringr)

# Create 10x10 km raster grid -------------------------------------------------
ref_raster <- rast("Raw spatial layers/Global_1km_CEA_reference_raster.tif")
crs(ref_raster)
nrow(ref_raster); ncol(ref_raster)

raster_100km2 <- aggregate(ref_raster, fact=10) #10 units in each direction 
crs(raster_100km2)
nrow(raster_100km2); ncol(raster_100km2)

# Load data layers ------------------------------------------------------------

# Roads
roads <- rast("Spatial layers/distance_to_GRIP_roads.tif")
plot(roads)
crs(roads) == crs(raster_100km2) #same projection? 
origin(roads); origin(ref_raster) #align
nrow(roads); ncol(roads)
roads_100km2 <- aggregate(roads, fact=10, fun="mean")
plot(roads_100km2)
origin(roads_100km2); origin(raster_100km2) 
rm(roads)
writeRaster(roads_100km2, "Spatial layers/10x10km layers/roads_100km2.tif")

# Elevation
ele <- rast("Spatial layers/elevation_1KMmean_SRTM.tif")
plot(ele)
crs(ele) == crs(raster_100km2) #same projection? 
origin(ele); origin(ref_raster) #align
nrow(ele); ncol(ele)
ele_100km2 <- aggregate(ele, fact=10, fun="mean")
plot(ele_100km2)
origin(ele_100km2); origin(raster_100km2) 
rm(ele)
writeRaster(ele_100km2, "Spatial layers/10x10km layers/ele_100km2.tif")

# Terrain ruggedness index  
tri <- rast("Spatial layers/tri_1KMmn_SRTM.tif")
plot(tri)
crs(tri) == crs(raster_100km2) #same projection? 
origin(tri); origin(ref_raster) #align
nrow(tri); ncol(tri)
tri_100km2 <- aggregate(tri, fact=10, fun="mean")
plot(tri_100km2)
origin(tri_100km2); origin(raster_100km2) 
rm(tri)
writeRaster(tri_100km2, "Spatial layers/10x10km layers/tri_100km2.tif")

# Distance to rivers
riv <- rast("Spatial layers/distance_to_wwf_hydrosheds_v1_river.tif")
plot(riv)
crs(riv) == crs(raster_100km2) #same projection? 
origin(riv); origin(ref_raster) #align
nrow(riv); ncol(riv)
riv_100km2 <- aggregate(riv, fact=10, fun="mean")
plot(riv_100km2)
origin(riv_100km2); origin(raster_100km2) 
rm(riv)
writeRaster(riv_100km2, "Spatial layers/10x10km layers/riv_100km2.tif")

# Human population density THIS HAS 5 LAYERS 
gwp <- rast("Spatial layers/gpw_v4_PopDensity_rev11.tif")
plot(gwp)
crs(gwp) == crs(raster_100km2) #same projection? 
origin(gwp); origin(ref_raster) #align
nrow(gwp); ncol(gwp)
gwp_100km2 <- aggregate(gwp, fact=10, fun="mean")
plot(gwp_100km2)
origin(gwp_100km2); origin(raster_100km2) 
rm(gwp)
writeRaster(riv_100km2, "Spatial layers/10x10km layers/riv_100km2.tif")

# Coefficient of variation 
cv <- rast("Spatial layers/cv_01_05_1km_uint16.tif")
plot(cv)
crs(cv) == crs(raster_100km2) #same projection? 
origin(cv); origin(ref_raster) #align
nrow(cv); ncol(cv)
cv_100km2 <- aggregate(cv, fact=10, fun="mean")
plot(cv_100km2)
origin(cv_100km2); origin(raster_100km2) 
rm(cv)
writeRaster(cv_100km2, "Spatial layers/10x10km layers/cv_100km2.tif")

# Human access: small cities 
ha_small <- rast("Raw spatial layers/Human access/Static_HumanAccess_travel_time_to_small_cities_7_to_9_1km_CEA.tif")
plot(ha_small) #note that highest values are ocean
crs(ha_small) == crs(raster_100km2)
origin(ha_small); origin(ref_raster) #align
nrow(ha_small); ncol(ha_small)
ha_small_100km2 <- aggregate(ha_small, fact=10, fun="mean")
plot(ha_small_100km2)
origin(ha_small_100km2); origin(raster_100km2) 
rm(ha_small)
writeRaster(ha_small_100km2, "10x10 km spatial layers/humacc_small_100km2_REDONE.tif")

# Human access: medium cities 
ha_med <- rast("Raw spatial layers/Human access/Static_HumanAccess_travel_time_to_medium_cities_4_to_6_1km_CEA.tif")
plot(ha_med) #note that highest values are ocean
crs(ha_med) == crs(raster_100km2)
origin(ha_med); origin(ref_raster) #align
nrow(ha_med); ncol(ha_med)
ha_med_100km2 <- aggregate(ha_med, fact=10, fun="mean")
plot(ha_med_100km2)
origin(ha_med_100km2); origin(raster_100km2) 
rm(ha_med)
writeRaster(ha_med_100km2, "10x10 km spatial layers/humacc_medium_100km2_REDONE.tif")

# Human access: large cities 
ha_large <- rast("Raw spatial layers/Human access/Static_HumanAccess_travel_time_to_large_cities_1_to_3_1km_CEA.tif")
plot(ha_large) #note that highest values are ocean
crs(ha_large) == crs(raster_100km2)
origin(ha_large); origin(ref_raster) #align
nrow(ha_large); ncol(ha_large)
ha_large_100km2 <- aggregate(ha_large, fact=10, fun="mean")
plot(ha_large_100km2)
origin(ha_large_100km2); origin(raster_100km2) 
rm(ha_large)
writeRaster(ha_large_100km2, "10x10 km spatial layers/humacc_large_100km2_REDONE.tif")

# For new CHELSA layers (2011-2040), note that using GFDL-ESM4 model 
# --> National Oceanic and Atmospheric Administration, Geophysical Fluid Dynamics Laboratory, Princeton, NJ 08540, USA 
# Reproject (using "nearest neighbour"), set extent and resolution of these layers in QGIS, making the extent/resolution of the reference layer 

# Annual temp: bio1 - 1981 to 2010
oldbio1 <- rast("Spatial layers/CHELSA_bio1_1981-2010_v2-1.tif") #HPC preprocessed 
plot(oldbio1)
crs(oldbio1) == crs(raster_100km2) #same projection? 
origin(oldbio1); origin(ref_raster) #align
nrow(oldbio1); ncol(oldbio1)
ob1_100km2 <- aggregate(oldbio1, fact=10, fun="mean")
plot(ob1_100km2)
origin(ob1_100km2); origin(raster_100km2) 
rm(cv)
writeRaster(ob1_100km2, "Spatial layers/10x10km layers/anntemp_2010_100km2.tif")

# Annual temp: bio1 - 2011 to 2024 (reprojected in QGIS)
newbio1 <- rast("Spatial layers/Projected CHELSA/warped_CHELSA_bio1_2011-2040_v2-1.tif") 
plot(newbio1)
crs(newbio1) == crs(raster_100km2) #same projection? 
ext(newbio1) == ext(ref_raster)
origin(newbio1); origin(ref_raster) #align
nrow(newbio1); ncol(newbio1)
newbio1_100km2 <- aggregate(newbio1, fact=10, fun="mean")
plot(newbio1_100km2)
origin(newbio1_100km2); origin(raster_100km2) 
rm(newbio1)
writeRaster(newbio1_100km2, "Spatial layers/10x10km layers/anntemp_2040_100km2.tif")

# Annual precipitation: bio12 - 1981 to 2010
oldbio12 <- rast("Spatial layers/CHELSA_bio12_1981-2010_v2-1.tif") #HPC preprocessed 
plot(oldbio12)
crs(oldbio12) == crs(raster_100km2) #same projection? 
origin(oldbio12); origin(ref_raster) #align
nrow(oldbio12); ncol(oldbio12)
ob12_100km2 <- aggregate(oldbio12, fact=10, fun="mean")
plot(ob12_100km2)
origin(ob12_100km2); origin(raster_100km2) 
rm(oldbio12)
writeRaster(ob12_100km2, "Spatial layers/10x10km layers/annprec_2010_100km2.tif")

# Annual precipitation: bio12 - 2011 to 2040
newbio12 <- rast("Spatial layers/Projected CHELSA/warped_CHELSA_bio12_2011-2040_v2-1.tif") 
plot(newbio12)
crs(newbio12) == crs(raster_100km2) #same projection? 
ext(newbio12) == ext(ref_raster) #same extent
origin(newbio12); origin(ref_raster) #align
nrow(newbio12); ncol(newbio12)
nb12_100km2 <- aggregate(newbio12, fact=10, fun="mean")
plot(nb12_100km2)
origin(nb12_100km2); origin(raster_100km2) 
rm(newbio12)
writeRaster(nb12_100km2, "Spatial layers/10x10km layers/annprec_2040_100km2.tif")

# Seasonal precipitation: bio15 - 1981 to 2010
# note: HPC layer threw errors, so redownloaded and processed original data 
oldbio15 <- rast("Spatial layers/Projected CHELSA/warped_CHELSA_bio15_1981-2010_v2-1.tif")
plot(oldbio15) 
crs(oldbio15) == crs(raster_100km2) #same projection? 
origin(oldbio15); origin(ref_raster) #align
nrow(oldbio15); ncol(oldbio15)
ob15_100km2 <- aggregate(oldbio15, fact=10, fun="mean")
plot(ob15_100km2) 
origin(ob15_100km2); origin(raster_100km2) 
rm(oldbio15)
writeRaster(ob15_100km2, "Spatial layers/10x10km layers/precseas_2010_100km2.tif")

# Seasonal precipitation: bio15 - 2011 to 2040
newbio15 <- rast("Spatial layers/Projected CHELSA/warped_CHELSA_bio15_2011-2040_v2-1.tif") 
plot(newbio15)
crs(newbio15) == crs(raster_100km2) #same projection? 
ext(newbio15) == ext(ref_raster) #same extent
origin(newbio15); origin(ref_raster) #align
nrow(newbio15); ncol(newbio15)
nb15_100km2 <- aggregate(newbio15, fact=10, fun="mean")
plot(nb15_100km2)
origin(nb15_100km2); origin(raster_100km2) 
rm(newbio15)
writeRaster(nb15_100km2, "Spatial layers/10x10km layers/precseas_2040_100km2.tif")

# Precip warmest quarter: bio18 - 1981 to 2010
# note: no HPC layer, so downloaded and processed 
oldbio18 <- rast("Spatial layers/Projected CHELSA/warped_CHELSA_bio18_1981-2010_v2-1.tif")
plot(oldbio18) 
crs(oldbio18) == crs(raster_100km2) #same projection? 
origin(oldbio18); origin(ref_raster) #align
nrow(oldbio18); ncol(oldbio18)
ob18_100km2 <- aggregate(oldbio18, fact=10, fun="mean")
plot(ob18_100km2) 
origin(ob18_100km2); origin(raster_100km2) 
rm(oldbio18)
writeRaster(ob18_100km2, "Spatial layers/10x10km layers/precwarm_2010_100km2.tif")

# Seasonal precipitation: bio18 - 2011 to 2040
newbio18 <- rast("Spatial layers/Projected CHELSA/warped_CHELSA_bio18_2011-2040_v2-1.tif") 
plot(newbio18)
crs(newbio18) == crs(raster_100km2) #same projection? 
ext(newbio18) == ext(ref_raster) #same extent
origin(newbio18); origin(ref_raster) #align
nrow(newbio18); ncol(newbio18)
nb18_100km2 <- aggregate(newbio18, fact=10, fun="mean")
plot(nb18_100km2)
origin(nb18_100km2); origin(raster_100km2) 
rm(newbio18)
writeRaster(nb18_100km2, "Spatial layers/10x10km layers/precwarm_2040_100km2.tif")

# GFC - create 2022, 2023 rasters
# because files so large, only merge files that fall within reference raster 
reference <- rast("10x10 km spatial layers/raster_100km2_with_cellID.tif")
site_cells <- read.csv("10x10 km spatial layers/WI_loc_matching_cell_ID.csv") 
reference_sub <- mask(reference, reference %in% site_cells$cell_ID, maskvalues=F) 

# gfc 2022 
hansen_files <- list.files("Raw spatial layers/hansen_data_2022", pattern = "\\.tif$", full.names = TRUE)
vrt_raster <- vrt(hansen_files)
vrt_raster #because file so large, immediately crop to reference raster rather than saving
vrt_reproj <- project(vrt_raster, reference_sub, method = "near")
test <- crop(vrt_reproj, reference_sub)
extracted <- terra::zonal(test, reference_sub, fun = mean, na.rm = TRUE, df = TRUE) 
head(extracted)
summary(extracted)
sum(is.na(extracted$spat_756866bc4e4a6_480902_o8up7yuDdJ6XHBX))/nrow(extracted) * 100 #<1%
write.csv(extracted, "gfc_2022.csv", row.names=F)

# gfc 2023 
hansen_files <- list.files("Raw spatial layers/hansen_data_2023", pattern = "\\.tif$", full.names = TRUE)
vrt_raster <- vrt(hansen_files)
vrt_raster #because file so large, immediately crop to reference raster rather than saving
vrt_reproj <- project(vrt_raster, reference_sub, method = "near")
test <- crop(vrt_reproj, reference_sub)
extracted <- terra::zonal(test, reference_sub, fun = mean, na.rm = TRUE, df = TRUE) 
head(extracted)
summary(extracted)
sum(is.na(extracted$spat_756866bc4e4a6_480902_o8up7yuDdJ6XHBX))/nrow(extracted) * 100 #<1%
write.csv(extracted, "gfc_2023.csv", row.names=F)

# gfc 2024 -- downloading direct from URL 
tile_urls <- readLines("https://storage.googleapis.com/earthenginepartners-hansen/GFC-2024-v1.12/lossyear.txt")
tile_names <- basename(tile_urls)

# Download files
# Note: some produced HTTP status 404 errors - skip and record these 
tmp_dir <- tempdir()
local_files <- file.path(tmp_dir, tile_names)

# Data frame to track download status
download_status <- data.frame(
  url = tile_urls,
  destfile = local_files,
  success = NA,
  stringsAsFactors = FALSE
)

for (i in seq_along(tile_urls)) {
  if (!file.exists(local_files[i])) {
    result <- tryCatch({
      download.file(tile_urls[i], destfile = local_files[i], mode="wb", quiet=TRUE)
      TRUE
    }, error = function(e) {
      warning(paste("Failed to download:", tile_urls[i]))
      FALSE
    })
    download_status$success[i] <- result
  } else {
    download_status$success[i] <- TRUE
  }
}

# Filter to only successfully downloaded files
successful_files <- download_status$destfile[download_status$success]
failed_files <- download_status[!download_status$success, ]
nrow(failed_files) / length(tile_names) * 100 
# -> almost 50% of urls do not have data associated with them... could be ocean files? see how much matches up with reference raster before panicking 

# Create VRT from successfully downloaded tiles
vrt_file <- file.path(tmp_dir, "lossyear.vrt")
vrt_raster <- vrt(successful_files, filename = vrt_file, overwrite = TRUE)

# Reproject 
vrt_reproj <- project(vrt_raster, reference_sub, method = "near")
vrt_crop <- crop(vrt_reproj, reference_sub)
extracted <- terra::zonal(vrt_crop, reference_sub, fun = mean, na.rm = TRUE, df = TRUE) 

# Check 
head(extracted)
summary(extracted)
sum(is.na(extracted$spat_756866bc4e4a6_480902_o8up7yuDdJ6XHBX))/nrow(extracted) * 100 #<1%

# Save
write.csv(extracted, "gfc_2024.csv", row.names=F)

################################################################################
### Prop WDPA ##################################################################
################################################################################

# Calculates the proportion of each grid cell contained within a WDPA polygon

setwd("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights/Raw spatial layers/WDPA_Jan2025_Public_shp")
rm(list=ls()); gc()

library(terra)
library(sf)
library(rnaturalearth)
library(dplyr)
library(tidyr)

# load terrestrial reference raster
#ref_100km <- rast("../../10x10 km spatial layers/raster_100km2_with_cellID_terrestrial.tif") 
ref_100km <- rast("../../10x10 km spatial layers/raster_100km2_with_cellID.tif") 


# create 1km2 raster
ref_1km <- rast("../../Raw spatial layers/Global_1km_CEA_reference_raster.tif") 
#land <- ne_countries(scale = "medium", returnclass = "sf") 
#land_vect <- vect(land)
#ref_1km_land <- mask(ref_1km, land_vect) ## CRS DO NOT MATCH 

# load reference polygons 
shape1 <- st_read("WDPA_Jan2025_Public_shp_0/WDPA_Jan2025_Public_shp-polygons.shp")
shape2 <- st_read("WDPA_Jan2025_Public_shp_1/WDPA_Jan2025_Public_shp-polygons.shp")
shape3 <- st_read("WDPA_Jan2025_Public_shp_2/WDPA_Jan2025_Public_shp-polygons.shp")
wdpa <- rbind(shape1, shape2, shape3)
#wdpa <- st_transform(wdpa, crs(ref_1km_land))
wdpa <- st_transform(wdpa, crs(ref_1km))

# rasterize protected areas
years <- 1999:2024
results_list <- vector("list", length(years))

for (i in seq_along(years)) {
  yr <- years[i]
  wdpa_yr <- wdpa %>% filter(STATUS_YR <= yr)
  
  r1_pa <- rasterize(vect(wdpa_yr), ref_1km, field=1, touches=TRUE)
  r1_pa[is.na(r1_pa)] <- 0  # convert NA to 0
  
  # Use nearest assignment: each 1km cell inherits ID of 10km cell it falls in
  #r1_id <- resample(ref_100km, ref_1km_land, method="near")
  r1_id <- resample(ref_100km, ref_1km, method="near")
  
  # Extract values as data frame
  df <- data.frame(
    cell_ID = as.vector(r1_id$cell_ID),
    inPA   = as.vector(r1_pa$layer)) 
  sum(is.na(df$cell_ID))
  
  # Proportion of 1km cells inside PA per 10km cell
  summary_df <- df %>%
    group_by(cell_ID) %>%
    summarise(prop_in_PA = mean(inPA, na.rm=TRUE)) %>% 
    mutate(year = yr)
  
  results_list[[i]] <- summary_df
}

results_df <- dplyr::bind_rows(results_list)
head(results_df); unique(results_df$year)
write.csv(results_df, "/gpfs/gibbs/pi/jetz/projects/WildlifeInsights/10x10 km spatial layers/prop_wdpa_full_new.csv", row.names=F)


################################################################################
### Prop Crop & Urban ##########################################################
################################################################################

# Calculates the proportion of each grid cell containing crop or urban landscape

# Set wd 
setwd("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights/Raw spatial layers")
rm(list = ls()); gc()

# Load libraries 
library(terra)
library(dplyr)
library(stringr)

# Load reference data -- sites 
reference <- rast("../10x10 km spatial layers/raster_100km2_with_cellID.tif") 
site_cells <- read.csv("../10x10 km spatial layers/WI_loc_matching_cell_ID.csv") 
reference_sub <- mask(reference, reference %in% site_cells$cell_ID, maskvalues=F) 

# Load reference data -- all terrestrial land 
reference_sub <- rast("../10x10 km spatial layers/raster_100km2_with_cellID_terrestrial.tif")

# Load coordinates
coords <- as.data.frame(rast("../10x10 km spatial layers/raster_100km2_with_cellID.tif"), xy=T) 

# Classes
crop_class <- c(10,11,12,20,30,40)
urban_class <- 190

# Function to calculate proportions using extract()
calculate_proportion <- function(class_values, lc_raster, grid_raster) {
  # Format rasters 
  binary_raster <- lc_raster %in% class_values
  clipped_grid <- crop(grid_raster, binary_raster)
  clipped_grid <- resample(clipped_grid, binary_raster, method="near")
  
  # Convert to data.frame
  df <- cbind(cell_ID = values(clipped_grid), value = values(binary_raster)) %>%
    as.data.frame() %>%
    filter(!is.na(cell_ID))  # remove NA zones
  
  names(df)[2] <- "value"
  
  # Aggregate mean per zone
  prop_df <- df %>%
    group_by(cell_ID) %>%
    summarise(proportion = mean(value, na.rm=TRUE))
  
  return(na.omit(prop_df))
}

# Input files
lc_files <- list.files("CCI landcover 1km2", pattern="\\.tif$", full.names=TRUE)

crop_all <- data.frame()
#urban_all <- data.frame()

for (file in lc_files) {
  
  message("\n--- Processing: ", file)
  landcover <- rast(file)
  
  # Reproject CRS if needed but keep native resolution
  if (crs(landcover) != crs(reference_sub)) {
    landcover <- project(landcover, crs(reference_sub), method="near")
  }
  
  # Extract year
  year <- as.numeric(str_extract(basename(file), "\\d{4}"))
  
  # Calculate proportions at native resolution
  crop_prop  <- calculate_proportion(crop_class,  landcover, reference_sub)
  #urban_prop <- calculate_proportion(urban_class, landcover, reference_sub)
  
  # Add year column
  crop_prop$year  <- year
  #urban_prop$year <- year
  
  # Append
  crop_all  <- bind_rows(crop_all, crop_prop)
  #urban_all <- bind_rows(urban_all, urban_prop)
}

# Predict values for 2023, 2024 
library(mgcv)     
library(dplyr)
library(tidyr)

crop_all <- read.csv("../10x10 km spatial layers/crop_proportion_all_years.csv")
crop_all <- merge(crop_all, coords, all.x=T) %>% 
  rename(crop_prop = proportion)

# Ensure response is in (0,1). If you truly have 0/1s, shrink slightly toward the interior Beta regression requires open interval).
eps <- 1e-6
crop_all <- crop_all %>%
  mutate(
    crop_prop_beta = pmin(pmax(crop_prop, eps), 1 - eps),
    cell_ID = as.factor(cell_ID)
  )

# Fit a spatio-temporal GAM with Beta regression.
#    - s(x,y): smooth spatial surface
#    - s(year): smooth temporal trend
#    - ti(x,y,year): space-time interaction (requires the main effects above)
#    - s(cell_ID, bs="re"): random intercept per cell to capture persistent differences
fit <- bam(
  crop_prop_beta ~ s(x, y, bs = "tp", k = 150) +
    s(year, bs = "cr", k = 12) +
    s(cell_ID, bs = "re"),
  data    = df,
  family  = betar(link = "logit"),
  method  = "fREML",
  discrete = TRUE   # works again without ti()
)

# Predict on the response scale (already in (0,1) because of betar + logit)
newdata$pred_crop_prop <- predict(fit, newdata = newdata, type = "response")
newdata$pred_crop_prop <- pmin(pmax(newdata$pred_crop_prop, 0), 1)
head(newdata)


# ^ this takes a LONG time to run; do short version for now: 
library(purrr)
future_years <- data.frame(year = c(2023, 2024))

pred_df <- crop_all %>%
  group_by(cell_ID, x, y) %>%
  nest() %>%
  mutate(
    fit = map(data, ~ lm(crop_prop ~ splines::ns(year, df = 4), data = .x)),  # flexible but extrapolates
    preds = map(fit, ~ {
      tibble(
        year = future_years$year,
        crop_prop = predict(.x, newdata = future_years)
      )
    })
  ) %>%
  select(-data, -fit) %>%
  unnest(preds)

head(pred_df)

crop_all <- crop_all %>% select(-crop_prop_beta)
crop_all <- rbind(crop_all, pred_df)

#write.csv(crop_all,  "crop_proportion_all_years_predicted.csv",  row.names=FALSE)
#write.csv(urban_all, "urban_proportion_all_years.csv", row.names=FALSE)
write.csv(crop_all,  "crop_proportion_all_years_full.csv",  row.names=FALSE)


############################################################################
#######  GFC  ##############################################################
############################################################################

library(terra)

## Create reference raster ----------------------------------------------------
# Load
reference <- rast("raster_100km2_with_cellID.tif")

# Limit to cell_IDs present in dataset 
site_cells <- read.csv("WI_loc_matching_cell_ID.csv") 

# Mask reference raster 
reference_sub <- mask(reference, reference %in% site_cells$cell_ID, maskvalues=F) 

## Load Hansen tile URLS ------------------------------------------------------

# Extract filenames 
tile_urls <- readLines("https://storage.googleapis.com/earthenginepartners-hansen/GFC-2024-v1.12/lossyear.txt")
tile_names <- basename(tile_urls)

# -> if lots of missing data, use the 2023 version (https://storage.googleapis.com/earthenginepartners-hansen/GFC-2023-v1.11/lossyear.txt)

# Download tiles with error handling 
tmp_dir <- tempdir()
local_files <- file.path(tmp_dir, tile_names)

download_status <- data.frame(
  url = tile_urls,
  destfile = local_files,
  success = NA,
  stringsAsFactors = FALSE
)

for (i in seq_along(tile_urls)) {
  if (!file.exists(local_files[i])) {
    result <- tryCatch({
      download.file(tile_urls[i], destfile = local_files[i], mode = "wb", quiet = TRUE)
      TRUE
    }, error = function(e) {
      warning(paste("Failed to download:", tile_urls[i]))
      FALSE
    })
    download_status$success[i] <- result
  } else {
    download_status$success[i] <- TRUE
  }
}

# Keep only successfully downloaded tiles
successful_files <- download_status$destfile[download_status$success]

# Create VRT 
vrt_file <- file.path(tmp_dir, "lossyear.vrt")
loss_raster <- vrt(successful_files, filename = vrt_file, overwrite = TRUE)

# Crop and mask to reference raster
loss_raster_proj <- project(loss_raster, crs(reference_sub))
#vrt_reproj <- project(vrt_raster, reference_sub, method = "near")

loss_raster_crop <- crop(loss_raster_proj, reference_sub)
#vrt_crop <- crop(vrt_reproj, reference_sub)
loss_raster_mask <- mask(loss_raster_crop, reference_sub)

## Compute annual forest loss -------------------------------------------------

# Hansen lossyear values: 1-23 (or 1:23 for 2001-2023)
years <- 1:23
yearly_loss <- list()

for (y in years) {
  # 1 if loss occurred in year y, 0 otherwise
  r_y <- classify(loss_raster_mask, cbind(-Inf, y-1, 0,
                                          y, y, 1,
                                          y+1, Inf, 0))
  yearly_loss[[y]] <- r_y
}
names(yearly_loss) <- paste0("year_", years)

# Summarize loss per year
annual_loss_count <- sapply(yearly_loss, function(r) {
  global(r, fun = "sum", na.rm = TRUE)
})

# Convert pixel counts to area in hectares
# Assuming raster resolution in meters CHECK!! 
res_m <- res(loss_raster_mask)[1]  # x and y should be same
pixel_area_ha <- (res_m^2) / 10000
annual_loss_area_ha <- annual_loss_count * pixel_area_ha

# Results
loss_summary <- data.frame(
  year = 2000 + years,  # adjust according to Hansen version
  pixels_lost = annual_loss_count,
  area_ha = annual_loss_area_ha
)
print(loss_summary)

# Save masked raster 
writeRaster(loss_raster_mask, "lossyear_mosaic_cropped.tif", overwrite = TRUE)
cat("Masked mosaic saved to lossyear_mosaic_cropped.tif\n")


## trying 
setwd("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights")
library(terra)

## Reference raster ---------------------------------------------------------
reference <- rast("10x10 km spatial layers/raster_100km2_with_cellID.tif")
site_cells <- read.csv("10x10 km spatial layers/WI_loc_matching_cell_ID.csv")
reference_sub <- mask(reference, reference %in% site_cells$cell_ID, maskvalues = FALSE)

## Hansen data --------------------------------------------------------------
tile_urls <- readLines("https://storage.googleapis.com/earthenginepartners-hansen/GFC-2024-v1.12/lossyear.txt")
tile_names <- basename(tile_urls)

# Download tiles with error handling 
tmp_dir <- tempdir()
local_files <- file.path(tmp_dir, tile_names)

download_status <- data.frame(
  url = tile_urls,
  destfile = local_files,
  success = NA,
  stringsAsFactors = FALSE
)

for (i in seq_along(tile_urls)) {
  if (!file.exists(local_files[i])) {
    result <- tryCatch({
      download.file(tile_urls[i], destfile = local_files[i], mode = "wb", quiet = TRUE)
      TRUE
    }, error = function(e) {
      warning(paste("Failed to download:", tile_urls[i]))
      FALSE
    })
    download_status$success[i] <- result
  } else {
    download_status$success[i] <- TRUE
  }
}

# Keep only successfully downloaded tiles
successful_files <- download_status$destfile[download_status$success]

# Create VRT 
vrt_file <- file.path(tmp_dir, "lossyear.vrt")
loss_raster <- vrt(successful_files, filename = vrt_file, overwrite = TRUE)

# Project → crop → mask in one clean pass
loss_raster <- project(loss_raster, reference_sub, method = "near")
loss_raster <- crop(loss_raster, reference_sub)
loss_raster <- mask(loss_raster, reference_sub)

# Optional sanity check
compareGeom(loss_raster, reference_sub, crs = TRUE, res = TRUE, orig = TRUE)

## Summarize forest loss ----------------------------------------------------
loss_freq <- freq(loss_raster, digits = 0, value = TRUE)
pixel_area_ha <- (res(loss_raster)[1]^2) / 10000

# Filter to valid Hansen lossyear values (1:23)
loss_freq <- loss_freq[loss_freq$value %in% 1:23, ]

loss_summary <- data.frame(
  year = 2000 + loss_freq$value,
  pixels_lost = loss_freq$count,
  area_ha = loss_freq$count * pixel_area_ha
)
print(loss_summary)
write.csv(loss_summary, "loss_summary.csv", row.names = FALSE)

## Save masked raster -------------------------------------------------------
writeRaster(loss_raster, "lossyear_mosaic_cropped.tif", overwrite = TRUE)
cat("Masked mosaic saved to lossyear_mosaic_cropped.tif\n")
