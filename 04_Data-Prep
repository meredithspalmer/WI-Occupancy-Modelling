########################
###### Data prep #######
########################

# Project: WI Occupancy Range Wide Trends
# This file contains code to format the data to run species-level occupancy models.

# Set working directory 
rm(list=ls()); gc()
setwd("/gpfs/gibbs/pi/jetz/projects/WildlifeInsights")
set.seed(123)

# Load libraries
library(arrow)
library(coda)
library(dplyr)
library(forcats)
library(ggmap)
library(gridExtra)
library(lubridate)
library(purrr)
library(raster)
library(reshape2)
library(rlist)
library(sf)
library(spOccupancy)
library(stringr)
library(terra)
library(tibble)
library(tictoc)
library(tidyr)
library(tidyverse) 
library(tmap)
library(tmaptools)
library(tsibble)
library(viridis)

## Prep detection matrix, effort, and julian date -----------------------------

# Load and format WI data -----------------------------------------------------

# Load data 
#overview_seq <- open_dataset("WI data/sequences_updated_20240710_parquet")
overview_seq <- read.csv("WI data/sequences_updated_20240710.csv")
#overview_images <- open_dataset("WI data/images_updated_20240714_parquet")
overview_images <- read.csv("WI data/images_updated_20240714.csv")

# Format data
dat_seq <- overview_seq |>
  mutate(record_date = sequence_date) |>
  dplyr::select(-c(sequence_date, sequence_datetime)) |>
  collect()

# if loaded csv: 
dat_seq <- dat_seq %>%
  dplyr::select(project_id, deployment_location_id, record_date, sensor_start_date_and_time, 
                sensor_end_date_and_time, latitude, longitude, class, sp_binomial)

dat_images <- overview_images |>
  mutate(record_date = photo_date) |>
  mutate(sp_binomial = Accepted_MOL) |>
  dplyr::select(-c(photo_date, photo_datetime, Accepted_MOL)) |>
  collect()

dat_images <- dat_images[!dat_images$sp_binomial == "Accepted_MOL",] |> #header added as row
  mutate(latitude = as.numeric(latitude)) |>
  mutate(longitude = as.numeric(longitude)) 

# if loaded csv: 
dat_images <- dat_images %>% 
  dplyr::select(project_id, deployment_location_id, record_date, sensor_start_date_and_time, 
                sensor_end_date_and_time, latitude, longitude, class, sp_binomial)

# Combine data 
all_dat <- rbind(dat_images, dat_seq) %>% 
  mutate(sensor_start_date = as.Date(sensor_start_date_and_time),
         sensor_end_date = as.Date(sensor_end_date_and_time),
         record_date = as.Date(record_date)) %>% 
  dplyr::select(-c(sensor_start_date_and_time, sensor_end_date_and_time))

all_dat <- all_dat[!is.na(all_dat$record_date),] 

rm(dat_seq, dat_images, overview_images, overview_seq)
gc()


## Load GBIF data (for all) ---------------------------------------------------

gbif <- read.csv("WI_GBIF/GBIF_clean.csv")

## Dispersal data -------------------------------------------------------------

# Pull home range size (https://esapubs.org/archive/ecol/E090/184/)
pan1 <- read.csv("Species data/PanTHERIA_1-0_WR05_Aug2008.csv") %>% 
  dplyr::select(MSW05_Binomial, X22.1_HomeRange_km2) %>% 
  rename(Species = MSW05_Binomial, 
         Homerange = X22.1_HomeRange_km2)
pan2 <- read.csv("Species data/PanTHERIA_1-0_WR93_Aug2008.csv") %>% 
  dplyr::select(MSW93_Binomial, X22.1_HomeRange_km2) %>% 
  rename(Species = MSW93_Binomial, 
         Homerange = X22.1_HomeRange_km2)

common_species <- intersect(pan1$Species, pan2$Species)
pan1_filtered <- pan1 %>% filter(!Species %in% common_species)
homerange <- bind_rows(pan2, pan1_filtered)

# Pull dispersal distances (from https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.1365-2656.2012.02030.x) 
disp <- read.csv("Species data/Dispersal.csv")

# Clean units and convert all values to km
disp_clean <- disp %>%
  mutate(unit_short = case_when(
    str_detect(Units, regex("km", ignore_case = TRUE)) ~ "km",
    str_detect(Units, regex("metres|m", ignore_case = TRUE)) ~ "m",
    str_detect(Units, regex("mile", ignore_case = TRUE)) ~ "mi",
    TRUE ~ NA_character_),
    Value_km = case_when(unit_short == "km" ~ Value,
                         unit_short == "m"  ~ Value / 1000,
                         unit_short == "mi" ~ Value * 1.60934,
                         TRUE ~ NA_real_)) %>%
  filter(!is.na(Value_km))  

# Assign weights to measure type
measure_weights <- c("Maximum" = 0.5, "Mean" = 0.3, "Median" = 0.2)

# Compute weighted average per species
disp_weighted <- disp_clean %>%
  mutate(weight = measure_weights[Measure]) %>%
  group_by(Species) %>%
  summarise(avg_km = weighted.mean(Value_km, w = weight, na.rm = TRUE))


# Define periods --------------------------------------------------------------

# Define time in the year-week format 
all_dat <- all_dat %>% mutate(record_yearweek = yearweek(record_date))

# Create periods template
min_year <- 2000  
max_year <- 2024 
range_year <- c(min_year, max_year)
min_date <- as.Date(paste(range_year[1] - 1, "-12-01", sep = ""))
max_date <- as.Date(paste(range_year[2] + 1, "-03-01", sep = "")) 
start_period <-  seq.Date(min_date, max_date, by =  "91 day") #"3 month"
end_period <-  c(start_period[-1] - 1, NA)

periods <- data.frame(period = paste("Period", seq(1, length(start_period), by = 1), sep = "_"),
                      start_period = start_period, 
                      end_period = end_period) 
periods <- periods[-nrow(periods),]
periods$season <- rep(c("Winter", "Spring", "Summer", "Fall"), length.out = nrow(periods))
periods$year <- year(periods$start_period)
periods$period_counter <- 1:nrow(periods)
head(periods)


# Load and format reference map -----------------------------------------------
ref <- rast("10x10 km spatial layers/raster_100km2_with_cellID.tif")


# Load and format species range map data -------------------------------------

# Target species name
target_species <- "Ceratotherium simum"

# Download data 
species_file <- gsub(" ", "_", target_species)

# GBIF data 
gbif_species <- gbif %>% filter(verbatimsciname == target_species) 

# Range map data 
mmd_map_path <- paste("/gpfs/gibbs/pi/jetz/data/species_datasets/rangemaps/mammals/mdd_mammals/rasters_cea/", species_file, ".tif", sep="")
mmd_map <- rast(mmd_map_path)


## Compute extended range map -------------------------------------------------

# Species detections 
dat_sp <- all_dat %>% filter(sp_binomial == target_species) %>% 
  dplyr::select(latitude, longitude) %>% 
  distinct()
plot_sp_coords <- st_as_sf(dat_sp, coords = c("longitude", "latitude"), crs = 4326)
plot_sp_coords_proj <- st_transform(plot_sp_coords, crs = st_crs(ref))

# Home range size
homerange_species <- homerange %>% filter(Species == target_species) 
#disp_species <- disp_weighted %>% filter(Species == target_species) 

# Buffer of 1 order of magnitude the home range size (km â†’ m) 
buffer <- homerange_species$Homerange * 10 
buffer_m <- buffer * 1000 

# GBIF data
gbif_sf <- st_as_sf(gbif_species, coords = c("longitude", "latitude"), crs = 4326)
gbif_sf_proj <- st_transform(gbif_sf, crs = st_crs(ref))

# Gather detection points 
all_points <- rbind(gbif_sf_proj %>% 
                      dplyr::select(geometry), plot_sp_coords_proj %>% 
                      dplyr::select(geometry)) 

# Convert to sf and buffer 
all_points_sf <- st_as_sf(all_points, crs = st_crs(ref)) 
points_buffered <- st_buffer(all_points_sf, dist = buffer_m) 

# Dissolve to one polygon 
points_union <- st_union(points_buffered) %>% 
  st_make_valid() %>% 
  st_buffer(0) %>% 
  st_cast("MULTIPOLYGON")

# Convert expert range raster to polygon
mmd_poly <- as.polygons(mmd_map, dissolve = TRUE) %>% 
  st_as_sf() %>% 
  st_make_valid() %>% 
  st_buffer(0) %>% 
  st_cast("MULTIPOLYGON") 

# Union expert range with buffered detections 
extended_range <- st_sf(geometry = st_union(mmd_poly, points_union) %>% st_make_valid())
extended_range$presence <- 1

# Set CRS and resolution 
crs_use <- crs(mmd_map)  
res_km <- 10000  # 10 km resolution
st_crs(extended_range) <- crs_use

# Convert to SpatVector
ext_vect <- vect(extended_range)

# Create an empty raster template covering the polygon extent
ext_raster <- rast(xmin = xmin(ext_vect),
                   xmax = xmax(ext_vect),
                   ymin = ymin(ext_vect),
                   ymax = ymax(ext_vect),
                   resolution = res_km,
                   crs = crs_use)

# Rasterize polygon
extended_raster <- rasterize(ext_vect, ext_raster, field = "presence", background = NA, touches = TRUE)

# Plot to check
plot(extended_range$geometry)
plot(extended_raster, add = TRUE, border = "red")

file_name = paste0("Model outputs/", species_file, "/", species_file, "_extendedRange.tif")
writeRaster(extended_raster, filename = file_name, overwrite = TRUE)


# Subset data by species ------------------------------------------------------

# Extract data for a species
dat_sp <- all_dat %>% filter(sp_binomial == target_species)
nrow(dat_sp)

# Retain all records of projects that fall within extended range map AND projects that 
# contain species (some are outside the MMD range maps)
# -> this problem should be solved by using extended rather than MMD range maps

sites <- all_dat[c("project_id", "latitude", "longitude")] %>% distinct()
#mmd_map_wgs <- project(mmd_map, "EPSG:4326")
mmd_map_wgs <- project(extended_raster, "EPSG:4326") 
cells <- terra::cellFromXY(mmd_map_wgs, as.matrix(sites[, c("longitude", "latitude")]))
vals <- terra::values(mmd_map_wgs)[cells]

sites_in_range <- sites[vals == 1, ]
sites_in_range_clean <- sites_in_range %>% filter(!is.na(longitude), !is.na(latitude))

sites_sp <- dat_sp %>% dplyr::select(project_id, latitude, longitude) %>% distinct()
sites_keep <- bind_rows(sites_in_range_clean, sites_sp) %>% distinct()
dat <- all_dat %>% semi_join(sites_keep, by = c("project_id", "latitude", "longitude"))

spp_sites <- dat[c("project_id", "latitude", "longitude")] %>% distinct()

# Check by plotting 
ras_df <- as.data.frame(mmd_map_wgs, xy = TRUE, na.rm = TRUE)
names(ras_df)[3] <- "species"

ggplot() +
  geom_tile(data = ras_df, aes(x = x, y = y, fill = species)) +
  scale_fill_viridis_c(na.value = NA, option = "viridis") +
  geom_point(data = sites, aes(x = longitude, y = latitude),
             color = "gray50", size = 1, alpha = 0.5, na.rm = TRUE) +
  geom_point(data = spp_sites, aes(x = longitude, y = latitude),
             color = "red", size = 1.5, na.rm = TRUE) +
  coord_equal() +
  theme_minimal() +
  labs(title = paste("Species distribution and site points:", target_species),
       x = "Longitude", y = "Latitude", fill = "Presence")

# Compare to number of sites pulled before
nrow(spp_sites)
length(unique(dat_sp$project_id))

# Check start and end dates: if issue, flip (note: many of these are 1 day depls)
nrow(dat %>% filter(sensor_end_date <= sensor_start_date))
head(dat %>% filter(sensor_end_date <= sensor_start_date))
dat_sp <- dat_sp %>%
  mutate(sensor_start_date = pmin(sensor_start_date, sensor_end_date),
         sensor_end_date   = pmax(sensor_start_date, sensor_end_date))
dat <- dat %>%
  mutate(sensor_start_date = pmin(sensor_start_date, sensor_end_date),
         sensor_end_date   = pmax(sensor_start_date, sensor_end_date))

## Spatial and temporal checks ------------------------------------------------

# Spatial coverage ------------------------------------------------------------
loc <- dat %>% 
  dplyr::select(latitude, longitude, project_id) %>% 
  distinct() %>% 
  mutate(project_id = as.factor(project_id))

# EXAMINE DATA -> manually update 
#loc <- loc %>% filter(latitude != min(latitude)) #remove weird location

# Spatial check --- 
loc_sp <- st_as_sf(loc, coords = c("longitude", "latitude"), remove = FALSE)
st_crs(loc_sp) = 4326

data("World")
World <- st_transform(World, crs = 4326)

tmap_mode("plot")
dat_map_range <- tm_shape(World) + 
  tm_fill(alpha = 0.3) +        
  tm_borders() +                
  tm_shape(loc_sp) + 
  tm_dots(col = "red", size = 0.3, popup.vars = c("Lat" = "latitude", "Long" = "longitude")) +
  tm_grid() +                 
  tm_scale_bar(breaks = c(0, 500, 1000), text.size = 1, position = c("LEFT", "BOTTOM")) +
  tm_style("col_blind") +
  tm_layout(legend.show = FALSE)
print(dat_map_range)

# Temporal coverage ----------------------------------------------------------- 

# Prepare data
dat_temp <- dat %>%
  mutate(proj_depl = paste(project_id, deployment_location_id, sep = "_"),
         record_date = as.Date(record_date)) %>%
  dplyr::select(project_id, deployment_location_id, proj_depl, record_date,
                sensor_start_date, sensor_end_date) %>%
  distinct()

#  Aggregate deployments to get one start/end per deployment
deploy_dates <- dat_temp %>%
  group_by(project_id, deployment_location_id, proj_depl) %>%
  summarise(start = min(sensor_start_date, na.rm = TRUE),
            end   = max(sensor_end_date, na.rm = TRUE),
            .groups = "drop") %>%
  arrange(start) %>%
  mutate(y_pos = row_number())  

# Merge y_pos back to dat_temp (for deployments)
dat_temp <- dat_temp %>%
  dplyr::select(project_id, deployment_location_id, proj_depl, record_date) %>%
  distinct() %>%
  left_join(deploy_dates %>% dplyr::select(proj_depl, y_pos, start, end), 
            by = "proj_depl")

# Prepare species detection points, aligned to deployments
dat_sp_temp <- dat_sp %>%
  mutate(proj_depl = paste(project_id, deployment_location_id, sep = "_"),
         record_date = as.Date(record_date)) %>%
  dplyr::select(project_id, deployment_location_id, proj_depl, record_date) %>%
  distinct() %>%
  semi_join(deploy_dates, by = "proj_depl") %>%
  left_join(deploy_dates %>% dplyr::select(proj_depl, y_pos), by = "proj_depl") %>%
  filter(!is.na(y_pos) & !is.na(record_date))

# Calculate summary statistics for subtitle
num_deployments <- n_distinct(dat_temp$proj_depl)
date_range <- range(dat_temp$record_date, na.rm = TRUE)
date_range_text <- paste0(format(date_range[1], "%Y-%m-%d"), " to ", 
                          format(date_range[2], "%Y-%m-%d"))

# Create Gantt-style plot
(deployPlot <- ggplot() +
    geom_linerange(data = deploy_dates, aes(y = y_pos, xmin = start, xmax = end,
                                            color = as.factor(project_id)), 
                   linewidth = 2, alpha = 0.7) + #deployment timelines
    geom_point(data = dat_sp_temp, aes(x = record_date, y = y_pos), color = "black",
               size = 1.5, alpha = 0.8) + #species detections 
    scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
    scale_y_continuous(breaks = deploy_dates$y_pos,
                       labels = deploy_dates$proj_depl,
                       expand = expansion(add = c(0.5, 0.5))) +
    labs(y = "Deployments (chronological)",
         x = "Date",
         title = bquote("Deployments through time: " * italic(.(target_species))),
         subtitle = paste0("Deployments: ", num_deployments, "\nDate range: ", 
                           date_range_text)) +
    theme_classic() +
    theme(axis.text.y = element_blank(), 
          axis.ticks.y = element_blank(),
          axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
          axis.title = element_text(face = "bold"),
          legend.position = "none"))

# Save plot
parent_dir <- "/gpfs/gibbs/pi/jetz/projects/WildlifeInsights/Model outputs"
species_dir <- file.path(parent_dir, gsub(" ", "_", target_species))
if (!dir.exists(species_dir)) {dir.create(species_dir, recursive = TRUE)}
plot_file <- file.path(species_dir, paste0(gsub(" ", "_", target_species), "_deployment_plot_original.png"))
ggsave(filename = plot_file, plot = last_plot(), width = 10, height = 6, dpi = 300)


## Truncate data based on temporal consistency --------------------------------

# If large gaps (>= 2 years), trim data so model not having to predict over blank time 
# Trimming manually now -- come up with an automated rule to streamline for future? 

set_date <- "2010-01-01"
dat_filtered <- dat %>% filter(sensor_start_date >= as.Date(set_date))
dat_sp_filtered <- dat_sp %>% filter(sensor_start_date >= as.Date(set_date))

# Visualize as above 
dat_temp_filtered <- dat_filtered %>%
  mutate(proj_depl = paste(project_id, deployment_location_id, sep = "_"),
         record_date = as.Date(record_date)) %>%
  dplyr::select(project_id, deployment_location_id, proj_depl, record_date,
                sensor_start_date, sensor_end_date) %>%
  distinct()

#  Aggregate deployments to get one start/end per deployment
deploy_dates_filtered <- dat_temp_filtered %>%
  group_by(project_id, deployment_location_id, proj_depl) %>%
  summarise(start = min(sensor_start_date, na.rm = TRUE),
            end   = max(sensor_end_date, na.rm = TRUE),
            .groups = "drop") %>%
  arrange(start) %>%
  mutate(y_pos = row_number())  

# Merge y_pos back to dat_temp (for deployments)
dat_temp_filtered <- dat_temp_filtered %>%
  dplyr::select(project_id, deployment_location_id, proj_depl, record_date) %>%
  distinct() %>%
  left_join(deploy_dates %>% dplyr::select(proj_depl, y_pos, start, end), 
            by = "proj_depl")

# Prepare species detection points, aligned to deployments
dat_sp_temp_filtered <- dat_sp_filtered %>%
  mutate(proj_depl = paste(project_id, deployment_location_id, sep = "_"),
         record_date = as.Date(record_date)) %>%
  dplyr::select(project_id, deployment_location_id, proj_depl, record_date) %>%
  distinct() %>%
  semi_join(deploy_dates, by = "proj_depl") %>%
  left_join(deploy_dates %>% dplyr::select(proj_depl, y_pos), by = "proj_depl") %>%
  filter(!is.na(y_pos) & !is.na(record_date))

# Calculate summary statistics for subtitle
num_deployments <- n_distinct(dat_temp_filtered$proj_depl)
date_range <- range(dat_temp_filtered$record_date, na.rm = TRUE)
date_range_text <- paste0(format(date_range[1], "%Y-%m-%d"), " to ", 
                          format(date_range[2], "%Y-%m-%d"))

# Create Gantt-style plot
(deployPlotTRIM <- ggplot() +
    geom_linerange(data = deploy_dates_filtered, aes(y = y_pos, xmin = start, xmax = end,
                                                     color = as.factor(project_id)), 
                   linewidth = 2, alpha = 0.7) + #deployment timelines
    geom_point(data = dat_sp_temp_filtered, aes(x = record_date, y = y_pos), color = "black",
               size = 1.5, alpha = 0.8) + #species detections 
    scale_x_date(date_breaks = "6 months", date_labels = "%Y-%m") +
    scale_y_continuous(breaks = deploy_dates_filtered$y_pos,
                       labels = deploy_dates_filtered$proj_depl,
                       expand = expansion(add = c(0.5, 0.5))) +
    labs(y = "Deployments (chronological)", x = "Date",
         title = bquote("Deployments through time TRIMMED: " * italic(.(target_species))),
         subtitle = paste0("Deployments: ", num_deployments, "\nDate range: ", 
                           date_range_text)) +
    theme_classic() +
    theme(axis.text.y = element_blank(), 
          axis.ticks.y = element_blank(),
          axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
          axis.title = element_text(face = "bold"),
          legend.position = "none"))

library(gridExtra)
(deployPlotComp <- grid.arrange(deployPlot, deployPlotTRIM, ncol = 2))

file_name = paste0("Model outputs/", species_file, "/", species_file, "_deployPlotTRIM.jpg")
ggsave(filename = file_name, plot = deployPlotComp, width = 10, height = 4, units = "in")

# Update data 
dat <- dat_filtered
dat_sp <- dat_sp_filtered

# After doing checks, clean original data for outliers ------------------------
#dat <- dat %>% filter(latitude != min(latitude)) #remove weird location
#min(dat_sp$latitude) -> if also bad, remove


## Build camera operability matrix (effort) -----------------------------------

# Start and ending dates for each project_deployment
proj_depl <- dat %>% 
  mutate(proj_depl = paste(project_id, deployment_location_id, sep = "_")) %>% 
  dplyr::select(proj_depl, sensor_start_date, sensor_end_date) %>% 
  distinct() %>% 
  drop_na() %>% # to remove depl with NAs in start/end sampling
  filter(sensor_start_date >= min_date & sensor_end_date <= max_date)

# List of project_deployments
proj_depl_unique <- unique(proj_depl$proj_depl)

# Days from start to end project
sampling_dates <- seq.Date(periods[1,2], periods[nrow(periods),3], by = "1 days")

# Create empty matrix, add colummn and row names
cam_op <- matrix(NA, nrow = length(proj_depl_unique), 
                 ncol = length(sampling_dates))
colnames(cam_op) <- as.character(sampling_dates)
rownames(cam_op) <- proj_depl_unique

# Fill matrix with 1 when cameras were active
for(i in 1:nrow(proj_depl)){
  cam_op[proj_depl$proj_depl[i], 
         as.character(seq.Date(as.Date(proj_depl$sensor_start_date[i]),
                               as.Date(proj_depl$sensor_end_date[i]), 
                               by =  1))] <- 1
} 

sum(cam_op,na.rm=T) #check if working 


## Build Julian date matrix ---------------------------------------------------

juldat <- matrix(rep(yday(colnames(cam_op)), nrow(cam_op)), 
                 ncol = ncol(cam_op),
                 byrow = TRUE)
colnames(juldat) <- colnames(cam_op)
rownames(juldat) <- rownames(cam_op)


## Build detection/non-detection matrix ----------------------------------------

# Prepare matrix
det_mat <- cam_op

# Replace 1s with 0s (camera on; set baselien as non-detections) 
det_mat[det_mat == 1] <- 0   

# Remove records without effort information
dat_sp <- dat_sp %>% 
  filter(paste(project_id, deployment_location_id, sep = "_") %in% proj_depl_unique)

# Fill in detections 
dat_sp$proj_depl <- paste(dat_sp$project_id, dat_sp$deployment_location_id, sep = "_")
for(i in 1:nrow(dat_sp)){
  det_mat[dat_sp$proj_depl[i], as.character(dat_sp$record_date[i])] <- 1
} 

# Check 
table(det_mat) 

## Organize in periods -------------------------------------------------------- 

# Detection/non-detection -----------------------------------------------------

# Prepare list to store detection matrix by period
det_ls <- vector("list", length = nrow(periods))

# Split detection/non-detection matrix by period
for (i in 1:nrow(periods)){
  det_ls[[i]] <- det_mat[, as.character(seq.Date(as.Date(periods$start_period[i]), 
                                                 as.Date(periods$end_period[i]), 
                                                 by = "1 days"))]
}

# Make ncol even across year-season by adding NAs columns
# Set max length to 13 weeks x 7 days = 91 
max.length <- 91 

# Remove columns if more that max.length
for(i in 1:length(det_ls)){
  if(ncol(det_ls[[i]]) > max.length){
    det_ls[[i]] <- det_ls[[i]][, 1:max.length]
  }
}

# Add NA columns to list elements
det_ls <- lapply(det_ls, function(v) {cbind(v, matrix(NA, nrow = nrow(v), ncol = max.length-ncol(v)))})

# Check that all items in list have same dimensions
unlist(sapply(det_ls, dim))

# Add info about season, year
for(i in 1:length(det_ls)) {
  det_ls[[i]] <- cbind(det_ls[[i]], 
                       cbind(rep(periods$season[i], nrow(det_ls[[i]])), 
                             rep(year(periods$start_period[i]), nrow(det_ls[[i]]))))
  colnames(det_ls[[i]])[(max.length+1):(max.length+2)] <- c("season", "year")
}

# Unlist detection histories 
det_mat <- list.rbind(det_ls)  


## Effort ---------------------------------------------------------------------

# Prepare list to store cam_op (i.e. effort) by period
cam_op_ls <- vector("list", length = nrow(periods))

# split cam_op matrix by period
for (i in 1:nrow(periods)){
  cam_op_ls[[i]] <- cam_op[, as.character(seq.Date(as.Date(periods$start_period[i]), 
                                                   as.Date(periods$end_period[i]), 
                                                   by = "1 days"))]
}

# Make ncol even across year-season by adding NAs columns
# Set max length to 13 weeks x 7 days = 91 
max.length <- 91

# Remove columns if more that max.length
for(i in 1:length(cam_op_ls)){
  if(ncol(cam_op_ls[[i]]) > max.length){
    cam_op_ls[[i]] <- cam_op_ls[[i]][, 1:max.length]
  }
}

# Add NA columns to list elements
cam_op_ls <- lapply(cam_op_ls, function(v) { cbind(v, matrix(NA, nrow = nrow(v), ncol = max.length-ncol(v)))})

# Check that all items in list have same dimensions
unlist(sapply(cam_op_ls, dim))

# Add info about season, year
for(i in 1:length(cam_op_ls)) {
  cam_op_ls[[i]] <- cbind(cam_op_ls[[i]], 
                          cbind(rep(periods$season[i], nrow(cam_op_ls[[i]])), 
                                rep(year(periods$start_period[i]), nrow(cam_op_ls[[i]]))))
  colnames(cam_op_ls[[i]])[(max.length+1):(max.length+2)] <- c("season", "year")
}

# Unlist detection histories
cam_op <- list.rbind(cam_op_ls) 


## Julian date ----------------------------------------------------------------

# Prepare list to store juldat by period
juldat_ls <- vector("list", length = nrow(periods))

# Split juldat matrix by period
for (i in 1:nrow(periods)){
  juldat_ls[[i]] <- juldat[, as.character(seq.Date(as.Date(periods$start_period[i]), 
                                                   as.Date(periods$end_period[i]), 
                                                   by =  "1 days"))]
}

# Make ncol even across year-season by adding NAs columns
# Set max length to 13 weeks x 7 days = 91 
max.length <- 91

# Remove columns if more that max.length
for(i in 1:length(juldat_ls)){
  if(ncol(juldat_ls[[i]]) > max.length){
    juldat_ls[[i]] <- juldat_ls[[i]][, 1:max.length]
  }
}

# Add NA columns to list elements
juldat_ls <- lapply(juldat_ls, function(v) { cbind(v, matrix(NA, nrow = nrow(v), ncol = max.length-ncol(v)))})

# Check that all items in list have same dimensions
unlist(sapply(juldat_ls, dim))

# Unlist detection histories
juldat <- list.rbind(juldat_ls)  


## Remove deployment-season combinations with no effort -----------------------
ind <- apply(cam_op[,1:max.length], 1, function(x) all(is.na(x)))
cam_op <- cam_op[!ind, ]
det_mat <- det_mat[!ind, ]
juldat <- juldat[!ind, ]


## Aggregate in weekly occasions ----------------------------------------------

# Detection/non-detection -----------------------------------------------------
det_sub <- data.frame(apply(as.data.frame(det_mat[, 1:(ncol(det_mat)-2)]), 2, as.numeric))
indx <- seq(1, ceiling(ncol(det_sub)+1), 7)
ystack <- matrix(NA, nrow = nrow(det_sub), ncol = length(indx)-1)

for(i in 1:(length(indx)-1)) {
  temp <- det_sub[,indx[i]:(indx[i+1]-1)]
  ystack[,i] <- apply(temp, 1, function(x) ifelse(all(is.na(x)), NA, max(x, na.rm = TRUE)))
}

# Effort ------------------------------------------------------------------ 
cam_op_sub <- data.frame(apply(as.data.frame(cam_op[, 1:(ncol(cam_op)-2)]), 2, as.numeric))
indx <- seq(1, ceiling(ncol(cam_op_sub)+1), 7)
effort <- matrix(NA, nrow = nrow(cam_op_sub), ncol = length(indx)-1)

for(i in 1:(length(indx)-1)) {
  temp <- cam_op_sub[,indx[i]:(indx[i+1]-1)]
  effort[,i] <- apply(temp, 1, function(x) ifelse(all(is.na(x)), NA, sum(x, na.rm = TRUE)))
}

effort <- as.data.frame(effort)
colnames(effort) <- paste("effort_w", seq(1, ncol(effort), 1), sep = "")

# Minimum Julian date ----------------------------------------------------- 
juldat_sub <- data.frame(apply(as.data.frame(juldat[, 1:(ncol(juldat))]), 2, as.numeric))
indx <- seq(1, ceiling(ncol(juldat_sub)+1), 7)
minjul <- matrix(NA, nrow = nrow(juldat_sub), ncol = length(indx)-1)
for(i in 1:(length(indx)-1)) {
  temp <- juldat_sub[,indx[i]:(indx[i+1]-1)]
  minjul[,i] <- apply(temp, 1, function(x) min(x, na.rm = TRUE))
}
minjul <- as.data.frame(minjul)
colnames(minjul) <- paste("minjul_w", seq(1, ncol(minjul), 1), sep = "")

# Remove deployment-season with less than two weeks of sampling
ind <- apply(effort[,1:13], 1, function(x) sum(!is.na(x)))
effort <- effort[ind > 1, ]
ystack <- ystack[ind > 1, ]
det_mat <- det_mat[ind > 1, ]
minjul <- minjul[ind > 1, ]


## Prepare covariates related to sampling -------------------------------------

# Add season/time covariates 
covs <- cbind(effort, minjul) %>% 
  mutate(proj_depl = rownames(det_mat),
         season = det_mat[, ncol(det_mat)-1],
         year = det_mat[, ncol(det_mat)],
         proj = substr(proj_depl, 1, 7),
         depl = substr(proj_depl, 9, nchar(proj_depl)))

# Add period counter 
covs <- left_join(covs, periods %>% 
                    dplyr::select("season", "year", "period_counter") %>% 
                    mutate(year = as.character(year)))


## Spatially aggregate to cell ID ---------------------------------------------

coords <- read.csv("10x10 km spatial layers/WI_loc_matching_cell_ID.csv") %>% 
  mutate(proj_depl = paste(project_id, deployment_location_id, sep="_")) %>% 
  dplyr::select(-c(project_id, deployment_location_id)) %>% 
  distinct()

covs <- left_join(covs, coords, by = "proj_depl")

# Quick check 
sum(is.na(covs$cell_ID))

ystack2 <- as.data.frame(ystack) 
ystack2 <- cbind(ystack2, covs)

ystack2_group <- ystack2 %>% 
  drop_na(cell_ID) %>% 
  group_by(cell_ID, season, `year`) %>% 
  summarise(
    n_depl = n(),
    across(starts_with("V"), ~ ifelse(all(is.na(.x)), NA, max(.x, na.rm = TRUE))),
    across(starts_with("effort_w"), ~ ifelse(all(is.na(.x)), NA, sum(.x, na.rm = TRUE))),
    across(minjul_w1:minjul_w13, ~ min(.x, na.rm = TRUE)),
    proj = first(proj),
    period_counter = first(period_counter),
    .groups = "drop"
  )

# -> more than 7 days effort because combining effort from multiple project-deployments within same 
# cell_ID (e.g., if 6 projects each operating for full 7 days, effort is 42)

# Reorganize outputs for modeling
ystack <- ystack2_group %>% dplyr::select(V1:V13)
covs <- ystack2_group %>% dplyr::select(-(V1:V13))

filename <- paste("Data for modelling/", target_species, "_", Sys.Date(), ".RData", sep = "")
save(ystack, covs, target_species, file = filename)
